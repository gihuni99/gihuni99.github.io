---
title: X-Decoder 논문 리뷰
date: 2023-03-29 00:00:00 +09:00
categories: [Paper, Decoder]
use_math: true
tags:
  [
    Computer Vision,
    Paper,
    Decoder
  ]
pin: true
---
X Decoder(Generalized Decoding for Pixel, Image, and Language)

# Abstract

![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/8ac85cb7-f39d-4274-b84e-e91a3703ae3c)

![Untitled 1](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/192a13fa-52ae-4f8a-9163-18a8a100aa0b)

[Abstract 번역]

해당 논문에서는 X-Decoder라는 모델을 제안하며, 이 모델은 픽셀 수준의 세그멘테이션과 언어 토큰을 예측할 수 있는 일반화된 디코딩 모델입니다. X-Decoder는 두 가지 종류의 쿼리를 입력으로 받습니다: (i) 일반적인 비 의미론적 쿼리와 (ii) 텍스트 입력에서 유도된 의미론적 쿼리로, 동일한 의미 공간에서 다른 픽셀 수준과 토큰 수준 출력을 디코딩합니다. 이러한 혁신적인 디자인으로 X-Decoder는 모든 유형의 이미지 세그멘테이션을 지원하며, 다양한 비전-언어(VL) 작업을 효과적으로 수행할 수 있는 첫 번째 모델입니다. 또한, 이 디자인은 서로 다른 단위 간에 원활한 상호 작용을 가능하게 하며, 의사 라벨링 없이 공통적이고 풍부한 픽셀 수준의 시각-의미 이해 공간을 학습함으로써 상호 이익을 가져옵니다. 제한된 양의 세그멘테이션 데이터와 수백만 개의 이미지-텍스트 쌍으로 사전 훈련을 한 후, X-Decoder는 제로샷 및 파인튜닝 설정에서 다양한 다운스트림 작업에 대한 강력한 전이 가능성을 보여줍니다. 특히, 이 모델은 8개 데이터셋에서 오픈 보캐블러리 세그멘테이션 및 참조 세그멘테이션의 최첨단 결과를 달성하며, 세그멘테이션 및 VL 작업에 대해 다른 일반 모델 및 전문가 모델보다 더 나은 또는 경쟁력 있는 파인튜닝 성능을 보입니다. 또한, 효율적인 파인튜닝과 새로운 작업 구성의 유연성을 제공합니다.

- X-Decoder: pixel-level segmentation과 language tokens을 예측할 수 있는 일반화된 decoding model
- X-Decoder는 2가지 type의 query를 input으로 한다.
1. 일반적인 non-semantic queries
⇒ 자연어 처리에서는 단어의 의미를 파악하고 처리하기 위해 "semantic" 정보를 사용
⇒ 반면, **문장을 단어로 분할하는 "tokenization" 과정에서는 문장 내에서 단어를 식별하기 위한 "non-semantic" 정보인 공백, 구두점 등의 문자를 제거**("non-semantic" 정보를 제거함으로써 자연어 처리의 성능을 향상)

2. text input으로부터 만들어진 semantic queries
- ⇒동일한 semantic space에서 서로 다른 pixel-level과 token-level의 output을 decoding할 수 있다.
- 이러한 새로운 design으로, X-Decoder는 모든 유형의 image segmentation과 다양한 Vision-language(VL) task에 적용할 수 있는 첫번째 모델이다.
- 더 나아가 X-Decoder는 다양한 분야의 task사이의 매끄러운 interaction(상호작용)을 가능하게 하고, **pseudo-labeling(가상 라벨링)** 없이, 공통적이고 풍부한 pixel-level visual-semantic understanding space를 학습함으로써 상호간의 이익을 가져온다.
- 제한된 양의 segmentation data와 수백만개의 image-text pairs로 pre-training한 후, X-Decoder는 zero-shot과 finetuning setting에서 다양한 downstream task에서 대해 강력한 전이 가능성을 보여준다.
    - 특히, X-Decoder는
    1) **open-vocabulary segmentation**과 **referring segmentation**에서 state-of-the-art(8개의 dataset에 대하여)
    
    2) Segmentation과 VL task에서 다른 generalist and specialist model들보다 더 낫거나 경쟁력 있는 finetuning 성능을 보인다.
    
    3) 효율적인 finetuning과 새로운 task composition의 유연성 제공

# Introduction(요약)

- 다양한 단위의 level에서 Visual Understanding은 Vision분야에서 오래된 관심 분야이다.
- Task는 image-level task(image classification, image-text retrieval, image captioning, visual question answering[VQA]), region-level localization tasks(object detection, phrase grounding), pixel-level grouping tasks( image instance/semantic/panoptic segmentation)으로 이루어져 있다.
- 최근까지, 대부분의 이러한 task들은 특정화된 model design으로 처리되었기 때문에, 서로 다른 작업끼리의 synergy를 발휘하기 힘들었다.
- Transformers의 다용도성을 고려하여, 다양한 vision, vision-language task에서 학습하고 적용될 수 있는 일반적인 model을 구축하는 것에 관심이 높아지고 있다.
⇒ multi-task learning, **sequential decoding**, **unified learning strategy**를 이용하여
- 이러한 연구들은 cross-task generalization을 장려하는 결과를 보여주고 있지만, 대부분은 image-level과 region-level tasks의 통합을 목표로 하고, 중요한 pixel-level understanding은 연구가 되지 않았다.
- [7, 54]논문에서 coordinated sequence(좌표 평면) 또는 color map의 decoding으로 segmentation을 통합하려는 시도를 했지만, 최적이 아닌 결과와 open-world generalization(개방적인 일반화)에 대한 지원이 제한적이다.
- pixel-level에서 image를 이해하는 것은 가장 중요하면서 어려운 문제 중 하나이다.
(1) pixel-level의 annotations(주석)이 다른 종류의 annotation에 비해 비용이 많이 들고, 매우 드물다.
(2) 모든 pixel을 grouping하고, open-vocabulary방식으로 인식하는 것은 연구가 적은 분야이다.
(3) 더 중요한 것은, 2가지의 크게 다른 granularity(단위)의 data를 mutual(상호간의) 이득을 얻으면서 학습하는 것은 쉽지 않다.
- 최근에는 다양한 측면에서 이러한 문제들을 극복하려는 노력들이 있었다.
⇒[12]논문에서 Chen은 모든 Segmentation task를 다룰 수 있는 unified architecture ‘Mask2Former’를 제안했지만, closed set(폐 집합)에서만 동작한다.
⇒open-vocabulary recognition을 지원하기 위해, 많은 연구들이 어떻게 rich semantic knowledge를 image-level vision-language foundation models(CLIP 또는 ALIGN)를 통해 transfer(전이) 또는 distill(증류)할 수 있을지에 대해 연구하였다.
- 하지만, 이러한 초기의 연구들은 특정 segmentation task에만 초점을 두고 다른 granularity(단위)에서의 task에 일반화를 보여주지는 않는다.
- 이 논문에서는, pixel-level과 image-level vision-language understanding을 통합하기 위해 ‘X-Decoder’라고 불리우는 일반화된 decoder를 만들었다.(Figure 1)

![Untitled 2](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/a2c4aed5-7506-430b-b930-754af6f10de8)

## A generalized decoding framework

- 논문은 pixel-level image segmentation, image-level retrieval and vision-language task를 포함한 모든 task를 일반적인 decoding procedure(절차)로 정의한다.
- 구체적으로 X-Decoder는 Mask2Former Framework를 따르며, multi-scale image feature를 추출하기 위해 Vision backbone과 Transformer Encoder를 사용한다.
- 핵심적인 차이는 Decoder의 design에 있다.
1. 첫째, Decoder는 2개의 query를 input으로 받는다
1) Generic non-semantic queries
⇒  일반적인 Segmentation을 위한 Segmentation mask를 Decoding하기 위해(Mask2Former과 비슷하게)

2) 새롭게 도입된 textual query
⇒language-related vision task를 위한 decoder language-aware(디코더 언어 인식)을 만든다.
2. 둘째, Decoder는 2가지 type의 output을 예측한다.
⇒ pixel-level mask와 token-level semantics
(이 둘의 다른 조합은 모든 interest task에 균일하게 지원될 수 있다.)
3. 논문은 하나의 Text Encoder를 사용하여, Segmentation에서의 이해, Referring Segmentation에서의 구문, image captioning에서의 token, **VQA**에서의 질문과 같은 모든 Task에 포함된 Text corpus(말뭉치)를 Encoding한다.

- 결과적으로, X-Decoder는 Task사이의 synergy를 자연스럽게 촉진하고, shared visual-semantic space learning을 지지하는 동시에, 서로 다른 task간의 이질성을 고려한다.

## An end-to-end learning paradigm

- 논문에서 제안한 일반화된 Decoder design으로, 모든 supervision단계의 학습을 위한 **end-to-end** pretraining method를 제안한다.
- 논문은 3가지 type의 data를 통합한다.
⇒**panoptic segmentation**, referring segmentation, and image-text pairs.
- image-text pair에서 fine-grained(세세한) supervision을 extract하기 위해 **pseudo-labeling** 기술을 사용했던 이전 연구들과는 다르게, X-Decoder는 몇가지의 의미있는 Segmentation candidate(후보군)들을 직접 grouping하고 제안하여, 영상의 내용을 caption과 mapping하기 쉽게 만든다.
- 한편, referring segmentation task는 일반적인 Segmentation과 pixel-level decoding을, image captioning과 Semantic queries를 공유함으로써 일반적인 Segmentation과 image captioning을 연결할 수 있다.

## Strong zero-shot and task-specific transferability to a
wide range of segmentation and VL tasks.

- 한정된 양의 Segmentation data와 수백만개의 image-text pairs로 pre-train된 X-Decoder는 **zero-shot**과 open-vocabulary 방식으로 다양한 task를 지원한다.
- 구체적으로, X-Decoder는 넓은 범위의 domain에서 3가지 type의 segmentation task에 대해 모두 직접적으로 적용할 수 있고, 7가지의 dataset의 10가지 setting에서 state-of-the-art를 달성했다.
- 특정 task로 transfer될 때도, X-Decoder는 이전 연구에 비해 일관되게 우위를 보인다.
- 마지막으로, X-Decoder의 flexibility model design으로 일부 신규 task 조합과 효율적인 finetuning을 지원하는 특성을 발견했다.

# 3. X-Decoder

![Untitled 3](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5afa19f8-b390-45e1-a66b-865e86b0ecae)

## 3.1. Formulation

- X-Decoder는 일반적인 Encoder-Decoder architecture를 따른다.(Figure 2)

### [Image Encoder]

![Untitled 4](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f6d56a2c-bf3b-46cc-aaf4-074d9c3a91c4)

![Untitled 5](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7c1f03cb-5475-4fce-91a8-defaf394740d)

**Input Image**

![Untitled 6](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/a25be697-2767-48cc-91ba-341a34c2e6e5)

Image Encoder

![Untitled 7](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/8ecf5fdb-963e-407d-9338-2ea46ac5dca1)

Extract Features

- Image Encoder를 사용하여 image ‘I’를 feature ‘Z’로 encoding한다.

### [Text Encoder]

![Untitled 8](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/33176a63-d3aa-48cd-b0df-3b8b09bdfce7)

![Untitled 9](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/a01c8d6d-24ed-4f43-af47-36f312013d79)

Textual Queries

![Untitled 10](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/594286e9-3096-4a17-b073-9acbca0acc1c)

Text Encoder

![Untitled 11](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c7016e05-8994-4636-8447-70ee3f36bbe2)

n의 길이를 가진다.

- Text Encoder를 사용하여 Textual Queries를 Qt로 encoding한다.

### [X-Decoder]

![Untitled 12](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/464fb11e-9747-41e9-8abb-2ee647aafc17)

![Untitled 13](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/6e1c4392-eaf0-4d62-849e-55e50e9e79ff)

**m개의 non-semantic 또는 latent queries**

- Visual features, Textual queries, m개의 non-semantic 또는 latent queries를 Output을 예측하기 위해 X-Decoder에 feed한다.

![Untitled 14](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c2062ed4-7faf-4c95-8aa9-90d6b993a279)

- O^p와 O^s는 각각 pixel-level mask와 token-level semantics
- 위의 공식에서, 논문은 X-Decoder의 일반화 능력을 강화하기 위해 중요한 세 가지 디자인을 고려한다.
- 이러한 디자인은 다양한 Vision 및 Vision-Language Task에 대한 X-Decoder의 일반화 능력을 강화하는 데 중요하다.

### 논문은 X-Decoder에서 2가지 type의 query와 output을 정의한다.

- 앞서 말했듯이, Decoder를 위한 Query는 일반적인 vision 및 vision-language task에 쓰이는Latent query(Q^h)와 Text query(Q^t)로 분류된다.
- 그리고 2가지 Query의 조합은 더 나아가 referring segmentation, VQA와 같은 다양한 language-aware task를 지원할 수 있다.
- 또한 output은 pixel-level mask(O^p)와 Semantic embedding(O^s)로 분류된다.
- 서로 다른 조합을 간단하게 사용하기 위해서, X-Decoder를 같은 세트의 parameter들로 다양한 Task에 적용할 수 있다.

### We employ a single text encoder Enc(T) to encode the textual corpus from all tasks.

- 일반적인 Text encoder는 referring phrases, text description, referring segmentation을 위한 image captions, image-text retrieval, 그리고 **image captioning**을 encoding하는 것에 각각 사용된다.
- 뿐만 아니라, 논문은 Segmentation에서의 Mask classification을 [25,84]논문과 비슷하게, O^s(Semantic embedding)과 유도된 textual concepts(원문의 의미)의 textual embedding 사이의 Mask-Text Matching problem으로 재정의한다.
- 모든 Text Corpus(말뭉치)에 대해 Text encoder를 공유함으로써, 서로 다른 Task에서의 Knowledge(정보)를 충분히 교환하고, 풍부하고 더 일관된 Semantic space를 학습할 수 있다.
- 논문은 image와 text encoder를 완전히 분리한다.
- unified encoder-decoder models의 많은 이전 연구들에서, image와 text는 encoder에서 융합
⇒이러한 설계는 global image-text contrastive learning(전역 이미지-텍스트 대조 학습)뿐만 아니라, **Generative pretraining** 또한 어렵게 만든다.
- 대조적으로, image와 text encoder를 완전히 분리하고, 출력을 모두 query로 사용함으로써, X-Decoder는 intra-image supervision과 inter-image supervision에서 모두 학습할 수 있다.
⇒strong pixel-level의 representation을 학습하고, 다양한 Task에 지원하는 것에 필수적임

## 3.2. Unification of Tasks

- 위의 design에 기초하여, X-Decoder는 서로 다른 Query의 조합을 입력으로 사용하여, 서로 다른 vision과 vision-language task들을 매끄럽게 통합할 수 있다.

![Untitled 15](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/aa3f13e4-2d33-4895-b3eb-437872312863)

### [Generic Segmentation]

![Untitled 16](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/8ac650b2-a1cf-4cad-8c72-2f08f0845191)

- 이 Task를 위해서 Textual Query는 input으로 사용하지 않는다. 따라서 Eq(1)은 아래와 같은 식의 형태로 바뀐다.

![Untitled 17](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/dd270916-ba73-432e-a2cb-2eba880dcc4a)

⇒ O^p(pixel-level mask)와 O^s(token-level semantics)는 Q^h(latent queries**)**와 동일한 크기를 갖는다.

- Eq(2)를 Mask2former와 유사하지만,  mask classification을 위해 mask-text matching을 사용하기 때문에 open-vocabulary 기능을 갖추고 있다.

### [Referring Segmentation]

![Untitled 18](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c2695203-9237-4662-8315-f25ff603c027)

- Referring Segmentation은 latent와 text Query 둘 다 input으로 필요하기 때문에, Eq(1)과 동일한 식을 갖는다.

![Untitled 15](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/aa3f13e4-2d33-4895-b3eb-437872312863)

- Generic Segmentation과 마찬가지로, 논문은 latent query에 해당하는 첫 m개의 decoding된 output들만을 사용한다.
- Eq(2)와 비교해서, referring segmentation은 language-conditional generic segmentation으로 여겨질 수 있다.

### [**Image-Text Retrieval**]

![Untitled 19](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/3728c61a-43e9-4a16-9538-e617ac15c41e)

- X-Decoder에서 분리된 image와 text encoder는 inter-image retrieval task에 직관적이도록 한다.

**`interimage retrieval task`**: `**다른 이미지들 사이에서 유사성을 찾아내는 작업입니다. 즉, 주어진 쿼리 이미지와 가장 유사한 다른 이미지를 찾는 것입니다. 이 작업은 이미지 검색, 추천 시스템 등에서 유용하게 활용될 수 있습니다.**`

- 구체적으로 말하자면, 논문에서는 오직 latent query만을 decoder에 feed하고 image의 semantic representation을 얻는다.

![Untitled 20](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/ee559632-82a0-4ec4-91f8-5d4492c80e31)

⇒ O^s(token-level semantics)는 Q^h(latent queries)와 동일한 length

⇒ O^s의 마지막 Token(m-th Token)은 image와 text 사이의 유사성을 계산하는 것에 사용된다.

### [Image Captioning and VQA]

![Untitled 21](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/a0e92f14-9964-4ce1-84ac-cd348f5b46bd)

- 두 Task모두, X-Decoder는 latent와 text query모두 input으로 가져가고, output으로 decoding한다.

![Untitled 22](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/01612113-90ce-4235-aba4-a801acb185ce)

- O^s(token-level semantics)은 Q^t(Text query)와 같은 size를 갖고, masks는 predict되지 않는다.
- 두가지 Task의 조금 다른 점이 있다.

1) Caption prediction은 **causal masking strategy**를 사용하지만 VQA는 사용하지 않는다.
2) 논문에서 Captioning을 위해 모든 O^s(token-level semantics) output값을 사용하지만, VQA에서는 마지막 값만 사용한다.

### [마무리]

- 위에서 소개한 통합을 기반으로, 논문은 Query와 Loss의 적절한 조합을 사용하여 X-Decoder를 pretraining시키고, 더 나아가 추가적인 Head(각 task에 맞도록 설정하는 값)없이 각각의 Task에 대해 Finetuning 할 수 있다.
- 이전 연구들은, sequential decoding(순차적인 decoding) **interface**를 활용하여 통합을 시도했다.
- 하지만, 해당 논문은, **functionality**를 사용하여 통합한다.

**Interface는 두 개체가 서로 상호작용할 때 사용하는 매개체이며, 이는 기술적인 측면에서 두 개체간의 상호 작용을 중심으로 설계됩니다. 기술적 구현을 고려한 설계입니다.**

**반면에 functionality는 기능 측면에서 설계됩니다. 즉, 두 가지 작업의 목적이나 기능이 유사한 경우에는 공통된 부분을 최대한 공유하고, 개별 작업에 따라 차이점을 유지합니다. 이는 기술적인 구현보다는 더 넓은 의미의 설계입니다.**

**이 논문에서는 이전 연구들이 interface를 중심으로 설계한 반면에, X-Decoder는 functionality를 중심으로 설계하여, 서로 다른 작업에서 공통된 부분을 최대한 활용하는 것을 강조합니다.**