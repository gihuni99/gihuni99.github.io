---
title: Visual Speech-Aware Perceptual 3D Facial Expression Reconstruction from Videos 논문 리뷰
date: 2023-11-20 00:00:00 +09:00
categories: [Paper, 3D Face Reconstruction]
use_math: true
tags:
  [
    Paper,
    3D Face Reconstruction
  ]
pin: true
---


# Abstract

![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d10af4e0-dd37-435a-8938-4cfeef6bcf85)

![Untitled 1](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f06eabde-f992-44aa-9b24-1f99bd73003e)

- Deep Learning 덕분에, 최근 image data에서의 monocular 3D face reconstruction분야의 SOTA 모델은 많은 발전을 이루고 있다.
- 하지만, 대부분 single RGB image를 input으로 사용하는 것에 초점을 두고 있다.
- 다음 중요한 요소들은 간과하는 경향이 있다.
a) 관심있는 facial image data의 대다수가 single image로부터 온 것이 아니라, video에서부터 왔다.(video는 많은 dynamic information을 포함)
b) 게다가, 이 video들은 일반적으로 언어로 소통하는 형태의 individuals를 담고있다.(ex. public talks, teleconferences(화상회의), audiovisual human-computer interactions, interviews, monologues/dialogues in movies 등)
- 현재 존재하는 3D face reconstruction method가 이러한 video에 적용된다면, 입 영역의 shape과 motion reconstruction에서의 artifacts는 심해질 것이다.
⇒ speech audio와 잘 match되지 않기 때문

![Untitled 2](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/e7f2bf7d-723f-466b-b708-4b4e02d23c23)

![Untitled 3](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/e4bd1839-ed74-4487-8e36-4275f4a71ecc)

- 앞서 언급한 한계점을 극복하기 위해서, 논문은 3D moutho expression의 visual speech-aware perceptual reconstruction을 위한 첫번째 방법을 제시한다.
- 논문은 **“lipread” loss**를 통해 이를 해결하고자 하였다.
⇒ fitting process를 guide하여, 3D reconstructed talking head로부터 이끌어낸 perception이 원래의 video 장면과 유사할 수 있게 해준다.
- 흥미롭게도, 논문은 **lipread loss가 traditional landmark loss들과 비교하여 mouth movements의 3D reconstruction에 더 알맞다는 것을 입증**하였고, 심지어 3D supervision에 비교될만 하다.
- 게다가, 논문이 고안한 방법은 text transcription 또는 corresponding audio에 의존하지 않는다.
⇒ unlabeled dataset에서 학습시켜 이상적으로 rendering하기 위함
- 논문은 3개의 large-scale dataset에서 철저하고 실증적인 평가를 통해 논문 방식의 효과성을 입증하고, 거기에 더하여 web-based studies(웹기반 사용자 연구, 온라인 설문조사, 사용자 행동 분석 등)로 주관적인 평가 또한 입증하였다.

# 1. Introduction

![Untitled 4](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5c0ca57d-3d1a-4226-9123-c4b0658abc74)

- 지난 몇년간, Deep Learning frameworks는 monocular 3D face reconstruction의 정확도를 유의미하게 상승시켰다. (어떠한 image data에서도)
- 최근 SOTA model은 3D facial geometry의 fine detail을 robust하게 reconstruction할 수 있고, 게다가 captured subject의 facial anatomy(몸,구조)의 신뢰할 수 있을 만한 estimation도 해낸다.
⇒ 이는 augmented reality, performance capture, visual effects, photo-realistic video synthesis, human computer interaction, personalized avatars 등 많은 분야에 이득이 된다.

![Untitled 5](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c15d80f5-283a-406e-95ce-01d90815cfd4)

- 반면, 대부분의 현존하는 method들은 single RGB image로부터의 3D face reconstruction에 주목한다. (사람의 얼굴에 내제되어 있는 많은 dynamic information(ex. speech를 하는 동안)을 활용하지 않는)
- 하지만, facial video를 reconstruction하는 dynamic modelling을 포함하는 몇몇의 method들도, mouth motion과 articulated(표현되는) speech 사이의 강한 correlation을 명확하게 modeling하지 않는다.
- 동시에 대부분의 흥미로운 facial video들은 verbal communication(언어적 소통)에 참여하는 individuals를 capture한다.
- 현존하는 3D face reconstruction method들이 이러한 video에 적용된다면, mouth area의 shape과 motion의 reconstruction에 존재하는 artifacts는 더 심해지고, human perception(대인지각)의 관점에서 상당한 문제가 생기고, speech와 관련 있는 입의 perceptual movements가 잘 capture되지 않는다.

![Untitled 6](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d9a10448-f4e2-41cc-8432-9bc3046d6e7f)

![Untitled 7](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/76c402dc-eb11-4d17-a76c-fe5b7f672e37)

- 틀림없이, 기존 method들의 한계의 중대한 요소는 대부분의 method들이 guidance의 형태로 face alignment method에 의해 예측된 landmark들을 통해 weak 2D supervision을 사용한다는 것이다.
- 이러한 landmark들은 facial shape의 coarse etimation을 할 수는 있지만, 크게 변화되는 mouth reion의 expression detail에 대한 정확한 representation을 제공하지 못한다.
- human mouth의 shapes가 인지적으로 speech와 연관되어있고, 3D talking head의 realism이 말해지는 문장과 완전하게 연관되어있다는 것에 주목하는 것 또한 매우 중요하다.
- 결과적으로, bi-labial consonants(양순음, 입술이 붙는 소리)를 말할 때 입술을 붙이거나 rounded-vowel(원순모음, 입술을 동그랗게 하여 내는 소리)을 말할 때 입술을 동그랗게 하지 않고 말하는 3D model은 인지적인 자연스러움이 없다.
- “EMOCA”에서는 3D reconstructed head의 표현력 향상에 사용되는 중요한 과정이 있지만, **perceptual emotional consistency loss**는 emotions와 일치하는 순간에만 영향을 미친다.
⇒ 게다가, 위 method는 jaw parameter를 잘 예측하지 못해, articualtion(표현)이 좋지 못하다.

![Untitled 8](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/44a5e3d2-aeb5-4ced-9fcb-76a0d62990af)

- 논문은 다음과 같이 결론을 내렸다.
⇒ reconstructed 3D face로부터의 speech perception(음성 인식)이 다양한 분야(augmented and virtual reality, gaming, affective avatars)들에 중요하지만, 기존 연구들에서는 보통 간과되었다.
- 기존 method 대부분에서 사용되었던 주된 evaluation metric이 model이 예측한 vertex들과 ground truth의 distance라는 것은 언급할 가치가 있다.
- 하지만 facial/mouth expression의 geometric error는 human perception 필수적으로 연관된 것은 아니다.

![Untitled 9](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/ac793465-c361-49b4-9ec7-4138636e1df4)

- 기존 연구들의 한계점을 극복하기 위해서, 논문은 video를 통한 monocular 3D face reconstruction의 문제를 해결하였다.
⇒ **mouth area와 speech articulation(음성 표현)과 관련된 mouth area의 expression과 movements에 중점을 둠**
- 논문은 video에서 human talking의 정확한 3D reconstruction에는 사람들이 speech와 일치한다고 여기는 mouth expression과 movements가 포함되어야 한다는 사실을 강조하고 언급한다.
- 논문의 method는 lip reading의 SOTA model을 활용한다.
⇒ **rendered video와 original input video 사이의 “speech perceptual” distance를 최소화하기 위함**
- 논문의 기여도는 아래와 같다.

![Untitled 10](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/320f78d7-f814-4fb8-b25b-00386a7b9b63)

- 논문은 **audio에 해당하는 text transcriptions이 필요하지 않은**, speech에 초점을 둔 human faces의 perceptual 3D reconstruction을 위한 첫번째 method를 design, implement하였다.
- 논문은 “lipread” loss를 고안하였다.
⇒ fitting process를 guid하여, reconstructed face와 특히 mouth area가, 관련 audio와 함께 결합될 때, viewer에게 유사한 perception을 이끌어내고 현실감을 느낄 수 있게 한다.
- 논문은 광범위한 객관적/주관적 evaluation을 진행하였다.
⇒reconstructed talking head의 perception이 상당히 증가했다는 것을 입증함.
- 또한 논문은 reconstructed 3D heads에서 human speech의 perception을 객관적으로 평가하는 것에 다양한 lip-read metric의 사용을 제안한다.

# 2. Related Work

#### 3D Models

![Untitled 11](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/bde7474a-ccc2-4807-9b0c-fccc79ed849e)

![Untitled 12](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b87b8bc9-c48d-4854-b695-e4e0202f08bd)

- 다양한 input source(RGB, Depth)을 통한 3D face reconstruction model을 위해 computer vision, graphic분야에서 많은 연구들이 있다.
- “3D Morphable Models”는 훨씬 가장 널리 사용되는 선택지이다.
⇒ 빈틈없는 representation을 제공할 뿐만 아니라, expression과 identity variation을 편리하게 분리한다.→ 더 잘 manipulation(조작)할 수 있다.
- 전통적인 3DMM(3D Morphable Models)는 linear하고 3D shape variation의 PCA-based model이었지만, 몇몇의 non-linear, deep learning-based 방법이 최근 몇년동안 제안되었다.

**PCA (Principal Component Analysis): 다양한 분야에서 데이터의 차원을 축소하거나 핵심적인 정보를 추출하는 데 사용되는 통계적인 기법**

- 가장 유명한 3D face model들은 “Basel Face Model”, “FaceWare-house model”, “FLAME”이 있고, 더 최근에는 “FaceScape”, “FaceVerse” model이 있다.
- 일반적으로, 이 모델들은 human face의 large 3D scan dataset을 통해 만들어진다.

#### Monocular 3D Face Reconstruction

![Untitled 13](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c1c7f942-eeca-45ac-b5c9-d587ebc0d7f9)

- 3DMMs의 일반적인 적용 방법은 RGB image에 가장 잘 맞는 model parameters의 예측을 포함한다.
- 이것은 analysis-by-synthesis framework에서 직접적인 optimization 절차를 통해 일어날 수 있다.
- 하지만, 매번 새로운 image에서 실행하기에 계산적인 비용이 큰 절차이다.
Ex) “FaceVerse” method는 detailed refinement를 위해 10분정도 소요된다.
- 위와 같은 이유로, 해당 문제를 image data를 통한 regression으로 표현하는 다양한 방법들이 제시되었다.
⇒ Deep Learning을 사용
- 신뢰할 수 있는 facial landmarker와 결합되어, 3D supervision이 없이도 정확한 결과를 도출할 수 있다.

![Untitled 14](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c66e4946-6434-4ffc-9035-17cde6b71408)

- 예를 들어, “RingNet”은 FLAME model을 사용하여 3D reconstruction을 수행했다.
⇒ identity와 expression을 구분하기 위해 shape subject의 image들 사이에서 shape consistency loss를 사용함으로써
- “DECA”는 RingNet을 기반으로 발전시켰고, 3D ground truth의 부족함을 해결하는 다양한 loss coefficients를 사용하여 CNN을 통해 FLAME model의 parameter를 jointly(jointly loss)하게 예측한다.
- “EMOCA는 reconstructed model의 expressiveness에 중점을 둔다.
⇒ emotional perceptual loss를 추가하고, 3DMM의 expression parameter를 예측하는 특정 CNN을 large emotional dataset을 통해 학습시킴으로써
- 반면, “ExpNet”은 SOTA method를 사용하여 주어진 image의 정확한 3D reconstruction을 통해 optimization 문제를 해결함으로써, pseudo-3DMM parameter들을 생성했고, landmark없이 CNN이 해당 parameter들을 예측하도록 하였다.
- 3DDFA에서는, Cascaded CNNs을 사용하여 face alignment와 3D reconstruction을 동시에 수행한다.

**3DDFA(3D Dense Face Alignment): face image의 특징점을 감지하고, 이를 통해 얼굴의 3D model을 alignment하는 기술**

**Cascaded CNN: 여러 단계들을 나누어, 각 단계에서 특정한 task를 수행하는 CNN**

- 최신 기술인 “MICA” method는 3DMM의 identity parameter를 정확하게 예측하는 것에 중점을 두었다.
⇒ large-scale 2D raw image data와 pair된 medium-scale 3D annotated dataset을 사용
- 마지막으로, “DAD3DHeads”는 3D reconstruction의 direct supervision에 사용될 수 있는 large-scale 3D head dataset을 제공한다.

![Untitled 15](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f522d102-ab59-44bd-9f64-78b206bbd9fc)

- 대부분의 methode들이 single face image를 reconstruct하거나 video의 frame-by-frame방식으로 동작한다고 하더라도, subject의 facial shape를 강제하거나 face reconstruction의 시간적 일관성을 도입하기 위해 monocular face video의 dynamic information을 이용하는 method는 거의 없다.

![Untitled 16](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/fad0495d-f53a-45bb-b6d5-038368e5dcc8)

![Untitled 17](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/086f783b-62f4-4229-bd69-78f6f62dec41)

- 논문은 “EMOCA”의 연구와 거의 유사하다
⇒perceptual reconstruction과 연관있다는 점에서
- 하지만 비교해보자면, EMOCA는 image로부터 affective(정서적인) information을 유지하는 것에 중점을 둔 반면, 논문은 speech production과 관련된 입과 입술 위치의 정확한 reconstruction에 중점을 둔다.
- 게다가, EMOCA는 입의 벌어진 정도와 rotation을 포함하는 jaw pose parameter를 정확하게 예측하는 것에 실패했다.(jaw pose가 고정됨)
⇒ convergence(융합)에 어려움을 겪음

#### Mouth/Lip Reconstruction

![Untitled 18](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/02c175f7-5354-48e3-8a7e-0e01a664dce7)

- 3D reconstruction을 위한 입과 입술의 dynamics에 중점을 둔 이전의 몇몇 연구들은 다음과 같다.
⇒ “Basu”(combined-statistical model을 사용, “Gregor”(lip motion을 따라가기 위해 markers를 사용), “Cheng”(2D images를 통한 mouth tracking을 Adaboost와 Kalman filter를 사용하여 수행)
- 가장 최근 연구는 video로부터 lip tracking을 하는 “Garrido”의 연구이다. (3D lips reconstruction에서 주목할만한 결과를 얻음)
⇒ lip tracking을 위해 high quality 3D stereo database를 사용하고, 2D images의 reconstruction 결과를 수정하기 위해 ground truth shapes와 함께 radial basis function(RBF)를 사용

---

##### **”Radial Basis Function”**

![Untitled 19](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/4f0b5a80-dc12-4d8e-b1ef-c7ef1e1cdbdc)

- RBF는 다음 2단계로 학습을 진행
1. 각 Basis function의 Center 와 Width 를 추정(input data만을 사용하여 unsupervised)
2. input data와 target data를 모두 사용하여 Weight를 구함(supervised)

---

# 3. Method

## 3.1. Preliminaries

![Untitled 20](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/3e99c9d0-ec0f-4519-9e57-0fd22256c981)

- 논문의 연구는 static RGB image를 통해 monocular 3D reconstruction을 하는 SOTA모델 “DECA”를 기반으로 한다.
⇒ 따라서 논문은 DECA 논문의 표기법을 채택
- original DECA에서, input image($I$)가 주어지면, coarse encoder(ResNet50 CNN)가 다음 parameter들을 jointly하게 예측한다.
⇒ **identity parameters($β ∈ R^{100}$), neck pose와 jaw($θ ∈ R^6$
), expression parameters($ψ ∈ R^{50}$), albedo($α ∈ R^{50}$), lighting($l ∈ R^{27}$), camera(scale and translation)($c ∈ R^3$)**
- 위 parameter들은 FLAME 3D face model의 parameters중 일부임을 주의
- 그 후, 위 parameter들은 predicted 3D face의 rendering에 사용한다.
- 또한 DECA는 **UV-displacement map과 연관된 latent vector를 예측하는 detail encoder**를 포함한다.
⇒ **high-frequency person-specific details(ex. wrinkles)를 modeling**
- 더 최근에는, “EMOCA”가 DECA를 기반으로 더 발전시켰다.
⇒ expression vector($ψ$)를 예측하기 위해 extra expression encoder(ResNet50)을 추가⇒ reconstructed face의 perceived emotion을 original image와 유사하게 만든다.
- 논문은 위 2개의 연구를 출발점으로 삼고, input video의 perceived expression을 증가시키는 architecture를 deisgn하는 것에 중점을 둔다.
⇒ mouth area에 집중하고, realistic articulation(발화) movements를 이끌어냄

![Untitled 21](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/493167d8-296b-4bc5-98cb-d294789abda9)

**<Figure 2에 대한 설명>**

- Figure 2는 perceptual 3D reconstruction을 위한 논문의 architecture를 전체적으로 보여준다.
- input video는 먼저 **3D reconstruction block**으로 들어간다.
⇒ **fixed encoder가 scene parameters(camera, lighting), identity parameters(albedo/identity), jaw parameter와 expression parameter의 initial estimation을 detect한다.**
- 그 후, **Mouth/Expression encoder가 refined facial expression parameters와 jaw pose를 예측**하고, **differentiable renderer가 prediced 3D shape을 rendering**한다.
- 마지막으로, **mouth area가 구별 가능하도록 input과 rendered image에서 모두 crop**된다. 그리고 **lip reader가 그들(input과 rendered image)사이의 perceptual lip reading loss를 측정**한다.
- **perceptual expression loss를 측정하기 위해 facial expression recognize에도 위와 같은 과정을 진행**한다.

## 3.2. Architecture

![Untitled 22](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/81ec1c11-90bd-4396-b477-ce6718114ea0)

![Untitled 23](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/1789ea5b-1dfe-4b9a-ab08-8fd4a6e3b5e0)

- architecture의 high-level overview는 Figure 2에 있다.
- **input video($V$)로부터 sampling된 K개의 연속적인 RGB frames이 주어졌을 때**, 논문의 method는 **FLAME topology에서 각 frame $I$마다 얼굴의 3D mesh를 reconstruct**한다.
⇒ **mouth movements와 general facial expressions가 perceptual하게 보존**된다.
- FLAME 3D face model 명명법에 따라, 논문은 estimated parameters들을 2개씩 구별된 sets로 분리한다.

#### Rigid & Identity parameters

![Untitled 24](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/fcc4ed76-3b25-41bc-88e5-9dc2d62d3037)

- 논문은 **input sequence의 각 image $I$에서 identity($β$), neck pose($θ_{neck}$), albedo($α ∈ R^{50}$), lighting($l ∈ R^{27}$), camera($c$)를 독립적으로 예측하기 위해 DECA의 coarse encoder를 사용**했다.
- “EMOCA”와 같이, 논문은 **해당 network(coarse encoder)를 training시에 fix**했다.

#### Expression & Jaw parameters

![Untitled 25](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/4c96b256-912a-43d3-8b16-15c64cb09e00)

- **input sequence와 연관되어 있는 expression parameters($ψ$)와 jaw pose parameters($θ_{jaw}$)는 추가적인 “perceptual” CNN encoder에 의해 예측**된다.
- 위 parameter들은 FLAME framework 아래에서 mouth expression과 movement를 control한다.
⇒ 따라서 논문의 방식에 의해 적절하게 예측되어야 한다.
- 논문은 **“lightweight MobileNet v2” architecture를 선택**하였고, **output에 “temporal convolution kernel”을 사용**한다.(**input sequence의 mouth movements와 facial expressions의 temporal dynamics를 modeling하기 위해**서)
- 논문은 system상의 연산적인 간접비를 줄이기 위해, 앞서 언급한 MobileNet의 lightweight option을 선택하였다.(이미 DECA backbone이 rescource가 필요한 ResNet50모델을 사용하기 때문)

![Untitled 26](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/29589737-6964-4e58-96c5-1dee0653ba64)

- 요약하자면, 논문은 “EMOCA”에서 제시된 것과 유사한 architecture를 계승한다.
⇒앞서 언급한 parameter들의 2개의 parallel path를 가지고 있음
- 그럼에도 불구하고, 논문의 중점은 전혀 다른 문제이고, 따라서 적절한 “directions”와 “constraints”가 제시된 Loss들의 사용을 통해 학습되어야 한다.

### 3.2.1 Training Loss

![Untitled 27](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/67afc31c-1df2-4ec2-a99e-df6ce567f145)

- **perceptual encoder를 training하기 위해**서, 논문은 **reconstruction guiding을 위한 2개의 perceptual loss function을 사용**한다.(geometric constraints와 함께)

#### Perceptual Expression Loss

![Untitled 28](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7f45a365-55d5-4e32-ba5b-4de267c7da87)

![Untitled 29](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d9b68e8a-eb6d-4028-9069-582e7843f368)

- **“perceptual encoder”의 output은 identity, albedo, camer, lighting의 predictions와 함께 사용**된다.
⇒**original input video와 관련있는 textured 3D mesh sequence를 구별 가능하도록 rendering**하기 위함
- 그 후, **input video와 reconstructed 3D mesh는 “emotion recognition network”(EMOCA에서 가져온)에 feed되고, 2개의 feature vector sequence를 얻게 된다.**
- 그 후, **2개의 feature vector sequence 사이의 distance를 minimize하는 시도를 함으로써 “perceptual expression loss($L_{em}$)을 적용**한다.
- 흥미롭게도, **“emotion recognition network”가 emotion을 예측하기 위해 학습되지만, 도움이 되는 facial characteristics를 잘 유지**한다.
- 따라서, 위와 같은 loss는 일**반적인 facial expression을 학습할 수 있게 하고, reconstruction의 현실성을 증대시킬 수 있는 emotion을 만들어낼 수 있다.**
- 특히, **perceptual expression loss($L_{em}$)은 눈 부분에 긍정적인 영향**을 미친다.
⇒ **눈을 감거나 얼굴을 찌푸리는 등의 동작의 estimation을 더 사실적으로 표현**한다.

#### Perceptual Lip Movements Loss

![Untitled 30](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/8c5dd9eb-9f88-41d4-87d0-188b5c876153)

- **“perceptual expression loss”는 mouth에 대한 detailed information을 충분히 유지시키지 못한다.**
⇒ 따라서, **추가적인 mouth-related loss가 필요**하다.
- **2D landmarks를 사용한 weak supervision의 geometric loss에만 의존하는 것이 아니라, 논문은 추가적인 perceptual loss를 사용**한다.
⇒**mout movements의 복잡함을 capture하는 “output jaw, expression coefficients”를 guide**함
- 이러한 perceptual mouth-oriented loss의 필요성은 extracted 2D landmarks를 통한 부정확성에 의해 더욱 주목받는다. (이러한 현상의 예시는 “Suppl. Material”을 참고)

![Untitled 31](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/30092ccd-e6f4-48e8-adf2-7acf3aa0947f)

![Untitled 32](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b72fc0d0-21d5-4d53-b222-46f5c2fc7237)

![Untitled 33](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/76c390a9-5403-468c-a9c9-e20cd0cb0cc8)

- 이러한 목적으로, 논문은 LRS3(Lip Reading in the Wild3) dataset으로 학습된 network를 사용한다.
- “rip-reading network”는 “Ma”에 의해 제공되는 pretrained model이다.
⇒input으로 mouth주변이 crop된 grayscale image의 sequence를 사용하고, predicted character sequence를 출력
- 위 network는 attention과 결합된 Connectionist Temporal Classification(CTC) loss를 통해 학습되었다.

**Connectionist Temporal Classification(CTC): audio와 transcription pair가 주어졌을 때, alignment를 위한 확률을 계산하는 방법(자세한 내용은 추후에 알아보기)**

- **model architecture는 3D convolutional kernel, 2D ResNet18, 12-layer conformer, transformer decoder layer의 순서로 구성**되어 있다.
⇒**predicted sequence를 출력**
- 여기에서의 논문의 목표는 **original sequence와 output image sequence사이의 speech-aware movements의 perceptual distance를 최소화**하는 것이다.
- 마지막까지, 논문은 **구별 가능하게 rendering된 image sequence를 얻고, 그 후에 landmarks prediction에 사용되는 mouth area 부분을 crop**한다.
- 마지막으로 **“lip-reading network”의 2D ResNet-18의 output으로부터 corresponding feature vector($ε_I$와 $ε_R$)를 계산**한다.
- 논문은 CNN output으로부터 나온 features가 mouth의 spatial structure를 더 잘 modeling하지만, “conformer”의 output을 통한 feature들은 sequence context에 의해 크게 영향을 받고, 반드시 필요한 spatial structure는 보존하지 못한다는 것을 실증적으로 밝혀냈다.
- feature vector를 산출한 뒤, 논문은 **input image sequence와 output rendered sequence($L_{lr} =(1/K)Σ^kd(ε_I,ε_R)$)사이의 “perceptual lip reading loss”를 최소화**한다.
(cosine distance($d$), input sequence length($K$))
- 추가적으로, 초기의 실험들은 문장의 original transcription을 고려하여, 기존 lip reading network의 예측된 output에 대한 CTC loss를 기반으로 두는 명시적인 lip reading loss를 포함한다.
- 이것의 간단한 직관에도 불구하고, 위와 같은 접근법은 video transcription이 필요하다는 점 외에도 주된 단점들이 있다.
- 먼저, 전체 문장이 한번에 처리되어야 하기 때문에, 상당한 연산적 간접비가 발생한다.
- 반면에, 제안된 접근법은 단순히 consecutive(연속적인) frame의 부분집합을 sampling하고, extract된 mouth-related features를 minimize하는 것이다.
- 게다가, 해당 approach는 “conformer”의 output을 통해 산출된 feature와 동일한 동작을 하여 실제로 효과적이지 않다는 것이 밝혀졌다.

#### Geometric Constraints

![Untitled 34](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/ebf4b7c2-6ec5-403c-9c9d-831d57c4171b)

![Untitled 35](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/6fc4e927-bca2-48e9-8925-d2c53b502ec8)

- **rendered images와 original images의 domain mismatch로 인해, “perceptual losses”가 perception의 high level information을 유지하는 것에 도움이 된다고 할지라도, 몇몇 상황에서 artifacts를 유발**하는 경향이 있다.
- 이것은 예상된 결과이다; **perceptual losses가, input manifold(데이터의 집합)가 realistic images에 일치한다는 것을 보장하지 않는 pretrained task-specific CNNs에 의존**한다.
- 예를 들어, **lip reading 성능이 좋은 왜곡된 facial reconstruction의 unrealistic images를 만들 수도 있다**.(adversarial model에서 전형적으로 나타나는 문제)
- 따라서 논문은, **training process가 geometric constraints를 따를 수 있도록 강제**함으로써 guide한다.
⇒ **initial predicted DECA parameters에 관한 L2 normalization($||ψ − ψ^{DECA}||^2$, $||θ_{jaw} − θ^{DECA}_{jaw}||^2$)을 규제하여 expression, jaw parameters를 regularization**한다.
- 앞서 언급된 regularization terms는 **DECA의 estimation을 “good” starting point로 사용**한다.
⇒ 이는 **논문의 method가, artifact가 없는 결과를 만들어내는 것이 입증된 DECA parameters로부터 크게 벗어나면 안된다는 것을 의미**
- 다시 말해서, 위와 같은 regularization 전략을 사용하여, 논문은 **DECA와 DECA의 training과정에서 hardcoding(직접 설정)된 일부 제약조건들을 간접적으로 사용**하는 것이다.
- 또한 논문은 **“3D model의 nose,face outline, eyes의 landmarks”와 “face alighnment method를 통한 predicted landmarks” 사이의 $L_1$ loss도 적용**하였다.
- **“mouth area”를 위해서는, “mouth landmarks의 intra-distances”사이의 더 완화된 $L_2$ relative loss를 사용**하였다.
- 앞서 언급된 **landmark losses는 “reconstructed face의 predicted 2D landmarks”와 “original image의 2D landmarks”사이의 distance를 기반으로 둔 geometric loss를 명확한 적용을 위한 대안으로써 구성**한다.
- 간단한 loss는 잘못된 reconstruction을 유발한다.
⇒ “perceptual losses”와 “2D landmark loss”는 주로 서로 상충되기 때문
- 제안된 relative landmark losses를 사용하는 것은 perceptual losses를 제한하는 지나치게 엄격한 제한 없이, 필수적인 face geometric strucuture를 유지 가능하게 한다.

![Untitled 36](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/a89d653d-777c-45dc-b0a0-689f12c4d414)

- 최종적으로, training에 사용되는 전체 Loss는 다음과 같다.
⇒ $L = λ_{lr}L_{lr} + λ_{em}L_{em} + L_c$ ($L_c$는 이전에 정의된 geometric constraints를 포함)

## 3.3. Training Details

![Untitled 37](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/439f8bee-f1b4-4c51-b7d3-86e3aa97b943)

- 논문은 Lip Reading Sentences3(LRS3) dataset으로 network를 학습시켰다.(lip reading in the wild를 위한 large public dataset)
- 나머지는 parameter들의 조건

## 4.3. Ablation study

⇒ 논문에서 제시한 아이디어를 제거하여, 성능을 확인해보는 방법

![Untitled 38](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/80b29e4a-8164-4d08-af83-f3980f410a9a)

- Fig6에서 landmarks를 통해 geometric constraints를 포함하거나 포함하지 않는 network의 training결과를 보여준다.
⇒ 논문의 오타인 것 같다. Fig 4에 제시된다.
- 논문은 geometric constraints없이 perceptual losses만을 사용하여 training했을 때, eyes, nose, mouth shape에 artifacts가 발생하는 것을 확인하였다.

---

**Figure 4**

![Untitled 39](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/bc0a2d2a-3ccb-4bc2-9806-8895ff2ee4d4)

- perceptual encoder를 여러 조건으로 Training시킨 결과
(left: 2D landmarks를 기반으로 geometric constraints를 적용하지 않은 결과)
(middle: 2D landmarks를 기반으로 geometric constraints를 적용한 결과)
- geometric constraints를 적용하지 않으면 eyes, nose, mouth 등에 artifacts가 발생하는 것을 확인할 수 있다.

---

![Untitled 40](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/702021be-83ba-431e-a85a-3df02e63c563)

- Fig3에서는 논문의 method와 다른 method들의 결과를 비교

![Untitled 41](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/560b22b6-2ed1-46cd-afc6-4292054cadcd)

왼쪽부터, original footage, 3DDFAv2의 결과, DAD의 결과, DECA의 결과, EMOCA의 결과, 논문의 결과 순이다.

# 5. Discussion

![Untitled 42](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c98954cb-2476-43d3-ac25-4a8d39ad80fb)

![Untitled 43](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/53b8e883-dcba-433b-9c1a-51eb400e748f)

- 논문의 method는 realistic 3D talking heads를 생성하기 위한 의미있는 절차들을 소개한다.
⇒ 다른 SoTA 모델들과의 다양한 평가를 통해 검증
- 주목할 점은 **large-scale 3D annotated dataset을 사용한 DAD보다 좋은 성능**을 보였다는 점이다.
- **“lipread loss”는 입의 motions와 shape을 유지시킬 뿐만 아니라, 더 확실한 rendered mesh를 만들어 낸다.**
- **speech의 관점에서 realism을 얻기 위해서 “perceptual losses”를 사용해야 한다는 것은 분명**해졌다.
⇒ 3D shape만큼이나 emotional expression관련 이전 연구들에서도 밝힌 바 있다.
- 논문의 **lipread loss를 통한 training은 audio에 해당하는 어떠한 text transcription도 필요하지 않다는 것에 주목**해야 한다.
- 게다가, 논문의 method는 “lipread loss”에 따라 speech videos를 통해 학습되긴 하지만, **speech와 관련 없는 임의적인 mouth movements를 modeling하는 것에도 사용될 수 있다.**
- 이러한 generalization 특성은 **mouth movements와 인지적으로 유사하게 만들도록 training하고, 그 결과 encoded mouth features가 speech-related movements일 필요가 없다**는 사실을 통해 나올 수 있다.

---

![Untitled 44](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/091858ec-0d47-4c89-8a0e-8e551c7875a4)

---

#### Limitaions

![Untitled 45](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5763daa5-7de5-4455-bab1-81f5117035e9)

![Untitled 46](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/34df9755-e533-4d7e-830d-ab9ec3cad57b)

- 논문은 objective evalutation(Table 1)에서 original footage에 비해 “CER”과 “WER”이 다소 높다는 것을 발견했다.
- 이것은 당연히, ground truth와 rendered images 사이의 domain 차이 문제이다.
- teeth와 tongue이 없는 것도 중요하다.
⇒ 특정 소리를 감지할 때, 중요한 역할을 하기 때문
- 논문의 방식은 실제로도 잘 동작하기 때문에, domain adaptation문제에 대해 따로 언급하지 않았다.
⇒ 하지만, 여전히 described losses의 잠재력이 촉발되는 것을 방해하는 요소이다.
- 이 domain problem은 perceptual losses도 포함한다.
- perceptual losses는 original image와 rendered image의 같은 visual domain에 포함된다는 가정을 만든다.
- 그러나, 위 2개의 feature space사이에는 realism/domain gap이 존재한다.
⇒relative landmarks를 사용해야 하는 이유
- 결과적으로, landmarks loss와 lipread loss는 때때로, 서로 반대되어 대립한다.
⇒ lip reading은 talking head의 perception을 향상시키기 위해 시도하는 반면, 정확하게 detected되지 않은 landmark는 realism을 감소시키는 경향이 있다.
- 다른 한편, 논문은 threshold 이하에서는 lip read loss의 감소가 artifacts를 유발한다는 것을 발견했다.
⇒ facial shape의 realism을 위해 landmarks의 constrains이 필요한 이유
- 추가적으로, 논문의 method가 “EMOCA”의 loss를 포함하고 있지만(입 외의 facial expression을 유지하기 위함), LRS3 dataset(emotional sample이 없는 dataset)만을 사용하여 training되었기 때문에, 결과에 EMOCA에 있는 emotion intensity가 포함되지 않는 경향이 있다.
- 게다가, “DECA”와 “EMOCA”는 detailed UV displacement map(wrinkles와 같은 person specific details이 modeling된 것)을 추정함으로써 detail refinement를 하는 반면, 논문의 method는 이러한 절차가 포함되어 있지 않다.
- 최종적으로, 이미 text transcription 또는 audio가 필요하지 않다고 말했지만, 전체적인 perception의 향상을 위해 위 2개의 modality가 사용될 수 있다고 본다.

#### Societal Impact

![Untitled 47](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/e9d1da21-ccfa-487d-a275-15f5cb23d8d0)

- 논문에서 소개한 직접적으로 부정적 요소로 사용될 것 같지는 않다고 믿지만, 이 method의 최종 목표는 부정적으로 사용될 수도 있는 매우 사실적인 human talking heads 3D reconstructiondlek.(ex deep fake)
- 결과적으로, 3D face reconstruction과 synthesis 분야를 연구하는 연구진들은 fake 3D reconstruction을 정확하게 찾아내는 method도 동시에 연구를 해야 한다고 생각한다.

# 6. Conclusion

![Untitled 48](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/454af540-4fc5-4a89-b331-08712a701dd4)

- 논문은 visual speech aware perceptual 3D talking head reconstruction을 위한 첫번째 method를 제시한다.
- 논문의 method는 text transcription이나 audio에 의존하지 않는다.
⇒ 대신 “lipread” loss를 사용하여 mouth의 perception을 증가시키기 위해 training process를 guide한다.
- 논문의 광범위한 subjective/objective evaluation을 통해3D supervision을 직접적으로 사용하는 method뿐만 아니라, mouth movements를 위해 geometric loss에 의존하는 다른 method들보다 훨씬 선호된다는 것을 입증하였다.

# Supplementary Material

(추가적인 설명과 실험들에 대한 설명)

## A. Inaccuracies of 2D landmarks

![Untitled 49](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5e204855-8e7f-40a0-b8f4-123b414842bb)

- 위 논문에서 언급했듯이, “perceptual visual-speech aware mouth loss”는 ‘face alignment methods의 부정확함에서 유래되었다.
- **전통적인 in-the-wild 3D reconstruction에서, face alignment method를 통해 구한 “2D landmarks”는 weak supervision을 위해 사용**된다.
⇒ **3D ground truth의 부족을 보충**하기 위함
- 하지만, **2D landmarks(특히  입 주변의)는 정확한 reconstruction과 인지적으로 사실적인 mouth movements를 위해서는 부족**하다는 것이 드러났다.
⇒ Figure 5에 2D landmarks의 부정확함이 들어난다.

---

![Untitled 50](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/002bd3b3-d6d0-4ef2-bc2e-b9fe867b8817)

- Fig 5는 2D landmark detection의 부정확함을 보여준다.
- face alignment가 mouth closure을 정확하게 예측하지 못하는 것에 주목(특히 right)
⇒**”mouth closure”은 양순음 자음(p, m b)의 realistic perception을 위해 필수적**이다.

---

![Untitled 51](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b426caf4-0849-4e14-906a-24cccbf2e573)

- 그렇지만, 몇몇 경우의 **2D landmarks의 부정확한 prediction을 제외하더라도, dense modeling을 위한 weak 2D supervision은 부적절**하다.
(특히 **극도로 다양한 formation을 갖는 “lip area”에 대해**서는)
- 해당 주제에 대한 마지막 의견은, lip corners와 같은 몇몇의 lip landmarks(보통 68개의 facial landmark사용)는 semantic meaning을 갖고, 중간의 lip landmarks는 그들의 정의에서 본질적인 모호성을 갖으며, annotators에 따라 상당한 다양성을 보여준다.
⇒ 즉 lip landmarks를 취득하는 방법이 annotator에 따라 다르다.

## B. Ablation study on lipreading features and CTC loss

![Untitled 52](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b88a74ad-1560-4a86-9842-a374515460f0)

### B.1. ResNet18 vs Conformer features

![Untitled 53](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/867c52b9-a5f2-46c4-9da3-b74fa151eb25)

![Untitled 54](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/62efc426-305b-4127-8bbb-bc348ab5a48b)

- 논문은 **conformer의 output feature대신 lipreading network의 ResNet18 output feature를 선택**하였다. ⇒ 논문은 이 2개의 feature 사이의 ablation study를 진행
- 두 feature의 직접적인 효과를 연구하기 위해, lipread loss와 regularization loss에 사용되는 “DECA” expression parameter($ψ$)와 jaw pose parameter($θ_{jaw}$)의 initail estimation을 직접적으로 optimization하였다.
- $L = λ_{lr}L_{lr} + λ_ψL_ψ + λ_{θ_{jaw}} L_{θ_{jaw}}$ ($λ_{lr} = 4, λ_ψ = 1e − 3,   λ_{θ_{jaw}} = 200$)
(위 2개의 feature의 효과를 확실하게 보기 위해 landmarks의 relaxed geometric loss는 사용 안함)
- 위 ablation study의 결과는 Figure 6에서 볼 수 있다.

---

***Figure 6***

![Untitled 55](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/9f90d69a-cced-424a-b470-2b4b771d5b7d)

![Untitled 56](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c7378e17-1c0c-47c8-bedb-c67e9a89dc58)

- Figure 6는 lipread의 ResNet18 network의 feature와 conformer의 feature를 비교한다.
- **top row: original footage, 2nd row: prediction of DECA, 3rd row: conformer feature를 활용하여 lipread loss를 optimizing한 결과, last row: ResNet feature를 활용**
- conformer feature는 mouth area를 향상시켰지만, original images와 정밀한 correspondence가 없다.
- 반면, ResNet feature는 mouth structure의 spatial information을 더 잘 유지한다.

---

![Untitled 57](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c8bb9860-1f9a-4ce8-abe9-e00ed5ba219c)

- **conformer feature는 mouth area를 향상시켰지만, original images와 정밀하게 맞지 않는다.**
⇒ **feature가 sequence context에 크게 영향 받기 때문** (image의 흐름에 영향을 받는다)
- 반면, **ResNet feature는 mouth structure의 spatial information와 정밀한 correspondence를 유지하고, perceptual lipread loss에 더 적합하다.**

---

***Figure 7***

![Untitled 58](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/df231444-fea9-4373-b022-f75780003123)

- Figure 7은 CTC loss를 사용한 adversarial attaks 예시를 보여준다.
- middle row: CER(character error rate)가 0.9인, DECA에 의한 predicted sequence
- last row: 완전히 왜곡되어 보이는 결과(CER은 거의 완벽하다)

---

### B.2. CTC loss and adversarial examples

![Untitled 59](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5f0c29b9-0f8a-431f-a724-97683eb994d2)

- 논문은 **text transcriptions를 활용하는 것을 고려해보았고, lipreader의 text prediction에 Connectionist Temporal Classification(CTC) loss를 적용**하였다.
- **전체 문장을 한번에 processing하여 computational overhead가 발생하고, text transcriptions가 필요하다는 간단한 단점 외에도, CTC loss는 spatial structure를 유지하지 못할 뿐만 아니라, 완벽한 lip reading recognition을 갖는 왜곡된 facial reconstruction을 발생**시킨다.
(**Adversarial attacks**(GAN에서 만들어지는 결과)에서 흔히 발생하는 현상)

## C. Details on geometric constraints and loss function for the perceptual encoder

![Untitled 60](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/87d1e79c-5abd-4e81-97c7-e77c4a4dbbfd)

- Section 3.2.1에서 preceptual expression과 lip movement loss외에도, 논문은 **geometric constraints loss($L_c$)**에 대해서 간단하게 언급하였다.
⇒ **input image와 rendered images 사이의 domain gap으로 인해 발생하는 문제들을 완화하기 위해 optimization process를 guide하는 것에 사용**
- **geometric constraints loss($L_c$)는 DECA의 initail estimation에 대한 expression parameters($ψ$)와 jaw pose parameter($θ_{jaw}$)의 $L^2$ normalization을 포함**한다.
⇒ $L_ψ = ||ψ − ψ^{DECA}||^2, L_{θ_{jaw}} = ||θ_{jaw} − θ^{DECA}_{jaw} ||^2$

![Untitled 61](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/72846b03-2063-45b2-b436-c968b3e9c243)

![Untitled 62](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/df68264f-947c-4cd7-bbb8-db87edd1eada)

![Untitled 63](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/8715eae8-19d3-48e3-b5be-be027432dd01)

- 추가적으로, 논문은 **predicted landmarks와 original landmarks(nose, eyes, face outline의 face alignment을 통해 획득) 사이의 $L_1$ loss를 적용**하였다.
**⇒ $L_n = ||E_r −E_{gt}||$ ($E_r$: prediected landmarks, $E_{gt}$: original landmarks)**
- 반면 mouth area에는 직접적인 값을 쓰는 것 대신, mouth landmarks의 intra-dstances를 사용함으로써 더 완화된 constraints를 사용했다.
⇒ $L_m=||D^m_r−D^m_{gt}||^2$  ($D^m_r$: predicted mouth landmarks의 pair사이의 distance, $D^m_{gt}$: original mouth landmarks의 pair사이의 distance)

---

***Figure 8***

![Untitled 64](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b389b580-9b91-48f1-b100-8783ea2db13e)

- Figure 8은 **“mouth landmarks의 정확한  위치”를 사용하는 것과 “relative intra-mouth distance”를 사용하는 것 사이의 ablation study** 결과를 보인다.
- **first column: DECA의 initial estimation**
- **second column: mouth landmarks에 $L_1$ loss를 사용하여 training된 model의 결과**
- **third column: intra-mouth distances를 사용하여 mouth에 대해 더 완화된 loss를 사용하여 training된 model의 결과**
- **“Strict mouth landmark losses”는 DECA의 출력과 유사하도록 잘못 guide**한다.
⇒ 반면 **relaxed constraints는 perceptual loss가 정확하게 lips formation을 capture할 수 있도록 충분한 자유도를 제공**한다.

---

![Untitled 65](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b6cc72a1-4999-410d-a733-6a2ff2c65670)

- 논문에서 조금 더 완화된 version을 사용하는 이유는 **predicted landmarks와 original landmarks 사이의 간단한 loss는 더 엄격하고 잘못된 reconstruction을 유발**할 수 있다.
⇒ **perceptual losses와 2D landmark loss가 상충될 수 있기 때문**
- Figure 8의 결과를 통해 **relative loss를 사용하는 것이 더 정확한 결과를 만들어내는 것**을 볼 수 있고, 결과적으로 **“perceptual encoder”**는 다음 criterion에 따라 학습된다.

![Untitled 66](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/ccd95eaa-17c8-41a0-836c-3f9fddba6d47)

- $L_{pc} = λ_{lr}L_{lr}+λ_{em}L_{em}+λ_ψL_ψ+λ_{θ_{jaw}}L_{θ_{jaw}} +λ_nL_n+λ_mL_m$
($λ_{lr} = 2, λ_{em} = 0.5, λ_{θ_{jaw}} = 200, λ_n = 50, λ_m = 50$)
- 특히 $λ_ψ$에 대해서는 **nonlinear weighting을 사용**한다. (위 식(1)과 같다)
(고정된 weight보다 더 좋은 성능을 보인다는 것을 실험적으로 발견)

![Untitled 67](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f6001e15-4e2e-469c-b240-66fb0d5b0e3a)

![Untitled 68](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/fbe6fdfc-085f-463e-9dab-fca7c371b1a8)

- 위 regularization term의 nonlinear방법의 motivation은 실험적으로 측정한 threshold 이후 더 엄격한 constriants를 적용하기 위함이다.
⇒ **perceptual losses를 지속적으로 minimize해야 하는 것이 artifacts를 유발**할 수 있다는 것을 발견
- 비록 위의 방법이 주요하게 영향을 미치지 못할지라도, 특정 artifacts를 효과적으로 줄일 수 있다는 것을 발견

## D. Failure Cases

![Untitled 69](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/07a93e77-9a1a-49ab-a76f-bdd6a1b6be01)

- 마지막으로, Figure 9에 논문의 model에서 mouth reconstructions에 오류가 발생한 2가지 예시를 보여준다.
- 첫번째 예시는 **mouth area에 artifact가 발생한 경우**이고, 두번째 예시는 **잘못된 open mouth를 갖는 3D reconstruction shape을 만든 경우**이다.
- 논문은 **잘못된 영향을 미칠 수 있는 2개의 주요한 원인**들을 제시한다.
**1) 논문의 “geometric relative constraints”가 perceptual losses에서의 domain gap문제를 효과적으로 완화하지만, 여전히 domain gap문제로 인한 소수의 artifacts들이 발생하는 사례가 있다.
2) perceptual loss가 그 자체로 neural network에서 비롯되었기 때문에, lipread loss의 실패 case들이 3D reconstruction model에 영향을 끼친다.**

---

***Figure 9***

![Untitled 70](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c1dc6dc8-dbe0-447a-a342-dda3e0274a42)

- Figure 9은 논문 model의 실패한 사례들이다.
- **geometric constraints의 guide를 받을 때에도, domain gap문제를 mouth artifacts를 발생**시킨다.
- **lipread network의 실패한 결과들이 3D reconstruction method에 영향을 미친다**는 사실에도 주목해야 한다.

---

## E. Analysis of Second User Study

![Untitled 71](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/688524ec-0a13-40a7-b14e-88504615814e)

![Untitled 72](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f4e98009-dc26-4c6b-8acf-a16dc4bbf7ee)

- 논문의 method가 mouth motion관점에서 다른 method들 보다 상당히 더 realistic할지라도, “EMOCA”와 “DAD”보다 조금 더 좋은 성능을 얻었을 뿐이다.
- Lip Reading study에서 결과(Table 4)를 보면 논문의 method가 다른 method에 비해 더 안좋은 성능을 보인 case와 더 좋은 성능을 보인 case를 나누어 보여준다.
- 추가적으로, 논문의 user study(lip reading)결과가 Figure 10에 나와있다.

---

***Table 4***

![Untitled 73](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c932ad86-bb86-4dec-93d8-18e89b7593cc)

- Table 4는 Per-word(낱말 단위의) recognition결과를 보여준다.

---

Figure 10

![Untitled 74](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d001fdf5-a8a9-4cc2-adc5-42e14fc583e2)

- Figure 10은 3개의 example 단어에 대한 second user study(lip reading)결과를 보여준다.
(”DAD”와 “EMOCA”의 결과와 비교)
- 단어 “PERFUME”과 “NARROW”에서, 논문의 method는 입술이 둥글게 되는 것을 정확하게 예측한다.
- 세번째 예시 단어인 “PEOPLE”에서는, 논문의 method가 실패한 것을 볼 수 있다.
⇒ 입술이 닫히는 소리(/p/와 같은 bilabial consonant)에서는 정확하게 예측하지 못함
- “PERFUME”단어의 3번째 frame에서 /f/ 는 어떻게 정확하게 나타냈는지 주목해야 한다.

---

![Untitled 75](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/303e55eb-ca0a-4525-abd8-8ac37c3fcabc)

- 주요 method들이 “BALEFUL”이나 “UMBRELLA”와 같은 발음의 특징이 확실한 경우에는 성능이 좋았지만, 수준 이하의 reconstruction이나 ‘까다로운’ 대체 가능한 단어들로 인해 수준 이하의 결과를 얻는다.(”SURRENDER”는 “SURROUNDED”와 혼동되는 경우가 많다)

## F. Extra Visual Comparisons

![Untitled 76](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/71b7f03a-86f7-48e1-af63-6d43d9b036df)

- 마지막으로 Figure 11에서 3DDFAv2, DAD, DECA, EMOCA의 시각적인 비교를 보여준다.

![Untitled 77](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/0ece0a9f-245a-4d68-9d6b-30f0a646fba6)

### 정리 노트

![image](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/859fb110-e43c-406f-a4f1-62838105db51)
