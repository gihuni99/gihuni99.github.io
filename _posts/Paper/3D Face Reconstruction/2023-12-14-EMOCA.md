---
title: EMOCA:Emotion Driven Monocular Face Capture and Animation 논문 리뷰
date: 2023-12-14 00:00:00 +09:00
categories: [Paper, 3D Face Reconstruction]
use_math: true
tags:
  [
    Paper,
    3D Face Reconstruction
  ]
pin: true
---

# Abstract

![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/2793c71f-8c5b-4d9a-905a-8abb3cd9c7ba)

- 기존 연구들은 ‘facial expression’을 전부 capture하지 못한다.
- 논문은 training에 사용되는 ‘standard reconstruction metrics’(landmark reprojection error, photometric error, face recognition loss)가 높은 정확도의 expressions를 capture하기에 불충분하다는 것을 발견했다.
⇒ input image의 expression과 맞지 않는 facial geometries
- 논문은 이를 “EMOCA(EMOtion Capture and Animation)으로 해결하였다.
⇒ 새로운 ‘deep perceptual emotion consistency loss”
    - input image에 묘사된 expression과 reconstructed 3D expression이 match되도록 도와준다.
- ‘valence’와 ‘arousal’의 단계를 직접적으로 regression하고, estimated 3D face parameters를 통해 ‘basic expressions’를 분류한다.
    - valence(감정 가치): 감정의 긍정적, 또는 부정적인 정도를 나타낸다.
    - arousal(감정의 활성화): 감정이 강렬한지, 차분한지 정도를 나타낸다.

# 1. Introduction

![Untitled 1](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/dbfd1f74-a14d-4631-9cf2-060770a8164d)

- 논문은 “EMOCA(EMOtion Capture and Animation)”를 설계하였다.
⇒ 3D supervision없는 in-the-wild images를 통해 animatable face model을 학습
- 논문은 SOTA ‘emotion recognition’ model을 training하고, EMOCA을 training할 때 supervision으로 사용하였다.
- 또한 EMOCA는 새로운 ‘perceptual emotion consistency loss’를 사용한다.
⇒ input과 rendered reconstruction 사이의 emotional content를 유사하게 만들어 준다.

![Untitled 2](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/622a0950-152c-401e-944d-39a107e2ed64)

- 새로운 ‘emotion consistency loss’는 더 나은 emotion을 reconstruction하지만, 이것 하나만으로는 충분하지 않다.
- 기존 3D reconstruction model에 사용된 large image dataset은 다양한 인종을 갖는 많은 subjects들이 있었지만, 감정 표현이 부족하다.
- 반면, facial expression, valence, arousar이 있는 large dataset은 emotion이 풍부하지만, 하나의 subject 당 다양한 conditions의 multiple images를 제공하지 않는다.
- 같은 사람의 multiple images는 SOTA 3D face reconstruction method를 사용하기 위해서 필수적이다.
- 이러한 문제를 해결하기 위해, EMOCA는 ‘identity shape reconstruction accuracy’에서 SOTA의 성능을 얻은 DECA를 기반으로 한다.
    - 구체적으로, DECA의 architecture에 ‘facial expression’을 위한 추가적인 ‘trainable prediction branch’를 추가하였다. (다른 구조들은 그대로)
- 위는 emtion-rich image data를 통해 EMOCA의 expression part를 training시킬 수 있다.
⇒ emotion reconstruction performance를 향상시킨다.
(DECA의 identity face shape quality를 retraining시키면서)

![Untitled 3](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/772f57fb-9cb2-4af7-b650-bf541b3fd57f)

- training한 후, EMOCA는 single image로부터 3D face를 reconstruction한다.(Fig 1)
- EMOCA는 ‘reconstructed expression quality ‘관점에서 이전 sota model보다 상당한 결과를 냈다.
- EMOCA는 SOTA identity shape reconstruction accuracy를 보존한다.
- EMOCA를 통해 regression된 ‘expression parameter’들은 in-the-wild emotion recognition에 충분한 정보를 전달한다.

![Untitled 4](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/10d28381-23a7-47ba-acf3-a6c8ce014b23)

![Untitled 5](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/0c143e66-f1b5-4e77-9b77-725906e8099b)

---

***Figure 2***

![Untitled 6](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/fbb8fa25-679a-4d8f-918b-fa9d46777839)

- “Coarse training stage(green box)”
    - input image가 ‘coarse shape encoder(DECA로 initialized, fixed)’, EMOCA의 ‘trainable expression shape encoder’에 feed된다.
    - regressed ‘identity shape’, ‘expression shape’, ‘pose’, ‘albedo’ parameters로 “FLAME’s geomtry models”와 “albedo models”를 fixed decoders로 사용하여 ‘textured 3D mesh’를 reconstruction한다.
    ⇒ regressed ‘camera’, ‘spherical harmonics lighting’으로 “differentiable renderer”를 통해 만들어진다.
    - 논문의 새로운 ‘emotion consistency loss(식 8)’는 ‘input image’와 ‘rendered coarse shape’의 ‘emotion features’사이의 차이에 대해 penalize한다.
    (위 2개의 image 모두 ‘fixed emotion recognition network’를 통과한 후에)
- “Detail training stage(yellow box)”
    - EMOCA’s expression encoder는 fixed된 상태이고, regressed ‘expression(and jaw-pose)’ parameters는 ‘detail decoder’의 condition으로 사용된다.

---

# 3. Preliminaries

### Face model

![Untitled 7](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/47784ead-1be1-4609-9b3c-18509de49a73)

- “FLAME”은 여러 parameter들이 존재하는 ‘statistical 3D head model’이다.
    - identity shape $\beta \in \R^{|beta|}$
    - facial expression $\psi \in \R^{|\psi|}$
    - pose parameters $\theta \in \R^{3k+3}$
        - $k$=4 joints(neck, jaw, eyballs)의 rotation과 global rotation을 위한 parameters(3*(k+1))
- 모든 parameter들이 주어졌을 때, FLAME은 $n_v=5023$ vertex를 갖는 mesh를 출력한다.
- “FLAEM”:  $M(\beta,\theta,\psi)$→$(V,F)$
    - vertex $V \in \R^{n_v \times 3}$
    - $n_f$=9976 faces $F \in \R^{n_f \times 3}$
- FLAME은 ‘Basel Face Model의 albedo space’에서 ‘FLAME의 UV layout’으로 변환된 ‘appearance model’과 함께 사용된다.
- $\alpha \in \R^{|\alpha|}$ parameters가 주어졌을 때, ‘appearance model’은 ‘FLAME texture map $A(\alpha) \in \R^{d \times d \times 3}$’을 출력한다.

### Face reconstruction

- **Face reconstruction은 DECA와 거의 동일하다, 따라서 간단하게 언급만 하겠다. 자세한 내용은 DECA논문 리뷰에 정리되어 있다.**

![Untitled 8](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c14a7169-e5a5-4ef5-84b9-1f17320bcdc7)

- image($I$)를 ‘Coars Encoder($E_c$)’에 input으로 넣으면,   $\beta, \theta, \psi, \alpha, l, c$ 의 parameter들이 나온다.

![Untitled 9](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/3fca8d00-29c9-432f-9063-15332d6ec902)

- image($I$)를 ‘Detail Encoder($E_d$)’에 input으로 넣으면 ‘detail code($\delta$)’가 나온다.

![Untitled 10](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/dd752640-7279-4e57-83d2-aa6d2518d418)

- 위 ‘Coarse Encoder’의 출력, $\psi, \theta_{jaw}$와 ‘Detail Encoder’의 출력, $\delta$를 ‘Detail Decoder($F_d$)’의 input으로 넣으면 ‘expression-dependent dependent detail UV displacement map($D$)’이 생성된다.

![Untitled 11](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/9aa84a5d-cd7b-4243-bd27-44e88ce70f86)

![Untitled 12](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/da9edb57-9b92-43b1-82f2-f900f3ee98a3)

- Coarse shape으로 ‘Rendering function($R$)’을 이용해서 rendering하면 $I_{Rd}$가 나오고, ‘expression-dependent details’를 추가된 FLAME mesh를 image로 rendering하기 위해서는, D$D$$N_d$’로 변환하여 $R$의 input으로 넣어주면 된다.

![Untitled 13](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/3e8ef581-f82c-4059-a93a-c8b9eeddf62b)

![Untitled 14](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/2ea6cdd1-7df4-4a58-a3ce-7fd93c0f540b)

- Relative keypoint loss는 DECA와 동일하다.

![Untitled 15](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/84acbec1-190c-470e-a912-c09bdc467547)

- 해당 부분이 “DECA” 모델과 주요하게 다른 부분이다.
- 논문은 **‘emotion network**’로 FC layer가 있는 **ResNet-50** backbone을 사용하였다.
⇒ output은 **‘expression classification’, ‘valence’, ‘arousal’**
- ‘emotion network’는 large scale annotated emotion dataset ‘AffectNet’을 통해 training되었다.
- loss function은
    - ‘expression classification’⇒ categorical cross entropy
    - ‘valence’, ‘arousal’⇒ mean squared error, correlation coefficient loss
- training후에는 prediction head(FC layer)는 쓰지 않는다.
- training된 ‘emotion network’의 output은 ‘emotion feature($\epsilon \in \R^{|\epsilon|}$)’이다.
- ‘emotion network’는 $A(I)$→$\epsilon$ 으로 표기된다.

![Untitled 16](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/13f99dc4-74a8-41e9-88ed-c4415ec8afd2)

- “EMOCA”의 기여
    - 새로운 ‘emotion consistency loss’를 제안한다.
    ⇒ input과 rendered image사이에 ‘emotion similarity’를 증가시킨다.(supervision)
    - DECA의 identity shape reconstruction 성능을 유지하면서, ‘emotion-rich image data’를 통해 EMOCA의 expression part만을 training한다.(나머지는 DECA의 trained model 사용)

![Untitled 17](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/130e9fbc-5349-4e16-80d6-b84d9c6d2939)

- EMOCA는 DECA의 architecture를 기반으로 한다. EMOCA는 여기서 ‘expression’을 잘 표현할 수 있도록 새로운 방법을 시도한다.(input과 동일한 rendered image가 나올 수 있도록)

![Untitled 18](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/07325bbd-3030-4412-91ae-6b1e49fe2091)

- ‘DECA’와 같은 모델을 ‘emotion-rich image data’로 학습시키는 것은 불가능하다.
⇒ $E_c$의 ‘identity shape reconstruction’을 training할 때, regularization을 위해서는 같은 subject의 multiple training image가 필요하기 때문
- 따라서 “EMOCA”는 추가적인 ‘expression encoder’를 통해 DECA를 변형시켰다.
⇒ expression encoder $E_e(I)\rightarrow \psi_e$

![Untitled 19](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d1afccba-0f19-422b-832a-68cbbf23ac6d)

- training시에 $E_c$의 weight는 고정되어, $\beta, \theta, \alpha, l, c$에 대한 prediction은 유지된다.
- 하지만, DECA의 $\psi$는 사용하지 않는다.
- $R(M(\beta, \theta, \psi_e),\alpha,l,c) \rightarrow I_{Re}$는 ‘input image의 expression($E_e(I)$)’와 $E_c$의 output을 rendering에 사용한 결과이다.

![Untitled 20](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/dda307e8-ad61-4443-bdfd-4a3729158194)

- $E_e$만을 학습시키는 것은 많은 장점이 있다.
    - subject에 대한 multiple image가 필요하지 않다.
    - identity prediction을 학습하지 않아, face recognition loss를 사용하지 않는다.
    - parameter들이 고정되어 landmark reprojection loss를 사용하지 않는다.
    - 적은 parameter들을 학습하기 때문에 training에 자원이 감소한다.

![Untitled 21](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/dd633944-7e37-4b65-91d2-d0680c946ddf)

- 위 Loss Function은 DECA와 거의 유사하다. 따라서 다른 부분만 자세히 살펴보겠다.

![Untitled 22](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/993c7228-167e-42f5-8526-282fe832320c)

- DECA와 가장 다른 부분이다. 자세히 살펴보자.
- ‘Emotion consistency loss’는 ‘input image의 emotion features $\epsilon_I=A(I)$’과 ‘rendered image의 emotion feature $\epsilon_{Re}=A(I_{Re})$’사이의 difference를 계산한다.
    - $L_{emo}=d(\epsilon_I,\epsilon_{Re})$, ($d(\epsilon_1,\epsilon_2)=||\epsilon_1-\epsilon2||_2$)
- $L_{emo}$는 geometry error를 구하는 것 대신, input image와 rendered image 사이의 ‘perceptual difference’를 계산한다.

![Untitled 23](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/ecc8aec1-9b0f-431c-b242-38ea8b85770c)

- 위 Loss는 DECA와 동일하다.

![Untitled 24](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/1e4e8e86-97c2-4ce1-b843-6d98785328d5)

- 위 Loss는 DECA와 동일하다.
- DECA의 training된 model을 사용하기 때문에 landmark keypoint loss를 사용하지는 않지만, 해당 loss는 사용하였다.

![Untitled 25](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/06429cea-b677-4c1d-9498-14ae20ed764a)

- 해당 Loss는 DECA에는 나와있지 않았지만, ‘Eye closure loss’와 방식이 동일하고, lip keypoint pair에 대한 loss이다.

![Untitled 26](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/86b32f0a-9d6b-4290-8ec0-c2984f0342d3)

- landmark keypoint loss를 사용하지 않는 대신, expression에 중요한 landmark에 대해 loss를 계산한다고 생각된다.

![Untitled 27](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/22f234ee-54c7-4f98-8b91-5d39331236df)

- Expression regularization은 위와 같다.

![Untitled 28](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f9ce0928-78ed-4307-af95-145287cbe65d)

- Detailed stage는 DECA와 동일하다.

### 정리 노트

![image](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c806d384-be71-40fa-aad5-c19eefa6f71a)