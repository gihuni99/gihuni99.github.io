---
title: ViT(Vision Transformer) 논문 리뷰
date: 2022-09-26 22:11:00 +09:00
categories: [Paper, Transformer]
tags:
  [
    Computer Vision,
    Paper
  ]
pin: true
---

# 1. Introduction

- NLP에서 Transformer의 성공에 영향을 받아 Computer Vision분야에서 Self-Attention을 적용한 CNN architecture를 개발하였지만 하드웨어의 가속장치와 맞지 않아 ResNet에서 가장 성능이 잘나왔다.
- ViT는 standard Transformer를 image에 바로 적용하여 약간의 수정을 통한 실험을 진행하였다. 즉 ViT는 Transformer를 Computer Vision분야에 적용한 것이다.

## 1.1. 간단한 동작 원리

1) Image를 patch단위로 나누어 Transformer에 Linear한 Embedding Sequence제공

(Image patch는 NLP와 동일하게 token처럼 다루어진다.)

2) Image Classification model을 Supervised Learning으로 학습

Transformer은 CNN과 다르게 Translation Equivariance나 Locality와 같은 귀납적(개별적인 특수한 사실이나 원리로부터 일반적이고 보편적인 명제 및 법칙을 유도해 내는 일)인 bias가 부족하여 불충분한 양의 데이터로 학습되면 generalize가 잘 되지 않는다.

하지만 model이 큰 Dataset으로 학습되면 large scale training이 inductive bias의 효과를 뛰어넘을 수 있는 결과를 찾았다. 즉, ViT가 충분한 양으로 Pre-Train되고, 더 적은 datapoint만을 task에 전이시킨다면 훌륭한 결과를 낼 수 있다.

# 2. Related Work

-Image에 Self-Attention을 단순하게 적용하는 것은 각 pixel이 다른 모든 pixel에 attention해야 한다는 의미이다. 따라서 Transformer를 Computer Vision에 적용하기 위한 많은 시도가 있었다.

## 2.1. 종류

- Self-Attention을 Global하게 적용하지 않고, local neighborhood에서 각 query pixel에만 적용

⇒ Local Multi-Head Dot-Product Self-Attention Block으로 Convolution을 완벽하게 대체 가능

- Sparse Transformer는 Global Self-Attention에 Scalable Approximation을 적용

⇒ Computer Vision분야에 유의미한 결과를 주지만, Hardware Accelerator에 효과적으로 적용될 수 있도록 복잡한 공학기술이 필요

- Cordonnier은 Input Image에서 2x2 size의 patch들을 추출하고 가장 마지막에 Full Self-Attention을 적용

⇒ ViT와 매우 유사, But 2x2 size patch는 작은 해상도의 image에만 적용 가능

# 3. Method

Model을 설계할 때, 최대한 original Transformer를 적용하기 위해 노력

⇒ Scalable NLP Transformer Architecture의 효과적인 성능을 따라가기 위해

![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/ed497e2b-c3dc-476d-9450-1bcf5647d713)

## 1) Patch+Position Embedding

**Standard Transformer를 사용하기 위해서는 1차원 Input이 필요하다.**

(NLP에서 사용했던 문장이나 단어에 비해 Image는 매우 큰 차원)

![Untitled 1](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d566cb77-cb9b-4d7e-9087-1ee1837afa62)

- Image를 Patch단위로 나누어서 token처럼 사용한다. (논문은 Patch Size를 16x16으로 설정)

⇒ Ex) 48x48x3 Image ⇒ 9x(16x16x3) : 9개의 16x16x3 Patch를 Input으로 갖는다.

- 각 Patch를 Flatting(1차원 Vector로)한 후 D차원으로 Embedding한다.

⇒  9x(16x16x3) Patch → 9xD Token

- 9xD Token에 Class Token을 concatenate해준다. Class Token은 Learnable Embedding Vector, 즉 학습 가능한 parameter이다. Transformer Encoder의 output에서 Class Token에 해당하는 Vector만을 Classification Header의 input으로 활용한다.

⇒ 9xD + Class Token → 10xD Input

- Class Token이 concatenate된 10xD Vector에 Positional Embedding을 더해준다.(이는 Transformer에서 적용했던 Positional encoding을 통해 local information을 따로 추가했던 방식과 유사하다)

⇒ 여기서 Positional Embedding을 concat하지 않고 add하였는데, 논문에서는 concat이 더 좋은 성능을 낼지는 모르나 추가적인 memory와 computation cost가 발생한다는 점에서 add하는 방식을 채택했다고 말한다.

## 2) Transformer Encoder

![Untitled 2](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c667eadf-fe80-464f-8e3c-957178b6228b)

- 10xD Vector를 Transformer구조에 넣어준다.  Transformer의 동작 원리에 대해서는 따로 정리해두었으니 생략하겠다.

## 3)MLP header & Classification

![Untitled 3](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/85da967d-3665-45f9-943d-3345d5e1c1bf)

- Transformer로 구성된 Encoder L개를 통과한 output에서 Class Token에 해당하는 Vector만 MLP header의 Input으로 넣어준다. 그 결과 1xD Class Token Vector를 1xN(N은 class의 개수) Vector로 바꾸어주는 것이다.

⇒ D → n-class

