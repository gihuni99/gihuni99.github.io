---
title: FFCANet 논문 리뷰
date: 2023-01-13 00:00:00 +09:00
categories: [Paper, RGB-D Segmentation]
tags:
  [
    Computer Vision,
    Paper,
    RGB-D,
    Segmentation
  ]
pin: true
---

[Application of Multi-modal Fusion Attention Mechanism in Semantic Segmentation]

![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/cad620e3-26db-4d86-84c7-325db065e6b0)

- Deep learning algorithm의 발전으로 Semantic Segmentation의 어려움은 연구자들에게 흥미로운 주제로 다시 다가왔다.
- 논문의 연구는 RGB-D image를 input으로 받는 RGB와 Depth 2개의 다른 modality으로 images에 대한 Multi-modal Semantic Segmentation의 Logic이 목표이다.
- Cross-modal(2개의 다른 modal을 사용) 보정과 융합을 위해, 논문은 새로운 FFCA Module을 제안
⇒여러 modality의 상호 보완적인 information을 획득함으로써 Segmentation 결과를 향상시키는 목표를 달성할 수 있다.
⇒FFCA Module은 Plug-and-Play가 호환되고 기존의 NN과 함께 쓰일 수 있다.
- FFCANet이라는 Multi-Modal Semantic Segmentation Network는 고전적인 조합인 ResNet과 DeepLabV3+ backbone의 조합을 사용하여 개발된 dual-branch encoder와 global context module을 사용하여 유효성을 테스트

# Introduction

- Neural Network의 발전은 Semantic Segmentation 성능을 매우 향상시킴
⇒ 특히 Encoder-Decoder structure기반의 CNN
- 하지만 Indoor Semantic Segmentation은 여전히 어려운 문제(outdoor보다 복잡)
⇒ Multi-Modal Input이 Indoor에 적합
- RGB-D method가 Multi-Modal Sementation System중 점점 인기
⇒ Spatial Informantion와 Scene Structure Coding을 얻을 수 있다
- RGB-D method의 가장 큰 문제점은 Depth image를 얻기 힘들다는 것이었는데, Depth Sensor가 발전하면서 해결
⇒RGB-D image를 기반으로 한 Semantic Segmentation dataset이 순차적으로 등장
⇒ RGB-D-based Semantic Segmentation이 더 보편화됨
- 추가적인 Modality는 추가적인 Spatial Information을 제공하지만, 2개의 modality를 융합하는 것은 새로운 과제
⇒ HHA 계산은 Feature Extraction을 더 잘하기 위해 Preprocessing(전처리) 단계에서 Depth를 향상시키는 합리적인 방법이다.
⇒HHA는 3개의 Channel을 포함
1) Horizontal Disparity: 수평차
2) Height Above Ground: 지면으로부터의 높이
3) Angle the pixel’s local surface normal makes with the inferred gravity direction:
   Pixel의 local surface 법선이 추론된 중력방향과 만드는 각도
- RGB-D Segmentation method기반의 Deep-learning에서는 RGB와 Depth modality에서 각각 Feature를 추출하기 위해서, 2개의 branch가 있는 Network구조로 많이 설계된다.
- Depth Feature는 Network가 deep해짐에 따라 RGB의 Feature Map에 점차적으로 융합됨
⇒Depth information이 있는 Feature를 RGB에 결합하면 Segmentation의 정확도가 매우 향상될 수 있다.
⇒특히  HHA를 쓰는 경우, HHA image가 3개의 Channel을 가지고 있기 때문에, 같은 구조가 RGB와 Depth information 추출을 위해 사용될 수 있다.

![Untitled 1](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d866bf96-6496-4af2-9304-f7318cf34ef2)

- 논문에서는 Cross-Modal(다중 modal) 보정을 위한 Multi-Modal Feature Fusion Attention Mechanism을 혁신적으로 소개
⇒ 2개의 다른 Attention Mechanism을 통해 Feature Channel과 Spatial Dimension 모두에서 서로 다른 Modality의 정보를 보정(개선)
- 논문의 Unique Feature Fusion Module은 일반적인 Feature를 기반으로 서로 다른 modality간의 상호 보완적인 information을 더 잘 Capture할 수 있다
⇒ Segmentation 결과 향상
- 이러한 특성에 따라, 이 모듈은  Feature Fusion Cascade Attention Mechanism Module(FFCA module)로 이름 붙여졌다.
- 논문의 method 챕터에서는 이 module에 대한 소개(multi-modal attention과 Feature Fusion을 포함한)와 FFCA module을 기반으로 한 Semantic Segmentation model인 FFCANet에 대해 소개한다.

# Main

![Untitled 2](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/01d1ccf4-3911-451a-bc73-fccf8de50aa2)

- Attention mechanism을 기반으로 한 Cross-modal Calibration(보정)이후 2개의 다른 Modality의 Feature들이 Fusion Feature를 얻기 위해 RGB와 HHA branch를 통해 element-by-element summation으로 수렴된다.
- HHA는 CNN Algorithm을 RGB-D data에 더 쉽게 만드는 Depth image의 generalization으로, Depth channel만 존재할 때보다 유의미하게 개선되었다.

# [3.1. FFCA Module: Feature Fused Cascade Attention Module]

![Untitled 3](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/4181ddfb-fc62-4f0a-98f3-8c958ff8dd34)

- RGB-D에 포함되어 있는 추가적인 공간정보는 순수 RGB의 기존 2D image semantic Segmentation에 비해 RGB의 부족함을 보완할 수 있다.
- 하지만, 단순히 2개의 coding branch의 output을 더하는 것은 원하는 결과를 얻기 힘들다.
⇒ Depth information을 RGB와 정렬하기 어려움, Depth information의 noise양
- 따라서 Feature Fusion의 핵심은 2개의 다른 image signal의 차이를 적절하게 처리하는 것

**⇒ 논문은 위 문제 해결을 위해 cross-modal Cascade Attention을 제안**

- Fig. 2.를 보면 2개의 다른 Attention Mechanism을 포함
1) Channel Attention
2) Spatial Attention
- Module은  Feature map의 Channel과 Spatial Dimension에서 Cross-modal Calibration(보정)을 하기 위해 2개의 Attention구조를 Concat하였다.
- 두 modality의 Feature Map은 같은 Size
- Calibration(보정)은 2개의 modality에서 같은 위치에 있는 elements에 한 쌍의 가중치(RGB, HHA)를 할당한다.
⇒ 그 다음 element-by-element(요소별) addition을 통해 Feature Fusion을 가능하게 하기 위해

![Untitled 4](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/eec931ee-8bfd-49a0-b27c-3096ace4ef48)

먼저 Channel Attention의 구조를 보시겠습니다. 먼저 각 modality의 Feature를 Global average pooling을 통해 1차원 벡터로 만들고, Feature Fusion을 위해 FC layer에 각 벡터를 통과시킵니다. FC layer의 수식은 2번째 식과 같은데, batch normalization을 하고, ReLU함수를 통과시키는 등의 연산과정이 있습니다. 두번째의 FC layer는 channel차원에서 각 modality의 Weight를 얻기 위해 사용된다고 언급되어 있습니다. 이후 softmax 함수를 통과시켜 각 weight를 더 smooth하게 만든 후 각 modality에 적용해줍니다.

![Untitled 5](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/14ba6d96-0044-47c3-ac89-26e8ca7bd6a0)

다음은 Spatial Attension입니다. Spatial Attention은 Channel Average pooling을 통해 channel dimension은 1차원으로 만들고, 각 Feature channel과 각 Feature channel을 pixel단위로 summation한 Feature channel 3개를 concat합니다. 그 후 convolution layer를 통해 spatial weight를 생성하고 softmax함수를 통해 normalization을 진행합니다. 이를 각 Feature의 weight로 적용합니다.

- 위는 Channel Attention과 Spatial Attention의 구조이고, 각 구조에서 실행되는 자세한 연산에 대해서는 우선 생략한다.(나중에 자세히 읽어보기)

![Untitled 6](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/38a3ba47-0eef-443b-be71-670d6fc0c92b)

![Untitled 7](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/839fd4ef-0133-4bb6-a533-e722655affb5)

- FFCA Module은 Attentio Mechanism을 기반으로 한 plug-and-play cross-modal calibration and feature fusion module이다.
⇒ Semantic Segmentation task를 위한 module을 진행할 Network구조가 필요
- 논문은 기존의 연구를 기반으로 기존의 Network 구성을 수정함으로써 Semantic Segmentation을 위한 Network Structure을 구축⇒ 이 Network는 FFCA Module과 효과적으로 결합
⇒ Feature Fused Cascade Attention Network(FFCANet)
- Network구조는 Fig. 5.와 같다.(Encoder, Context Module, Decoder)
- Encoder는 Semantic Segmentation task에서 가장 흔하게 쓰이는 ResNet에서 선택
⇒ FFCA Module에 의해 연결된 2개의 Branch로 확장
- Context Module은 PSPNet의 pyramid pooling과 유사하며, ESANet에서 사용된 체계를 참고하여 수정
- Encoder의 Two-branch structure는 Network parameter를 2배로 늘리기 때문에, 적은 수의 Parameter를 가진 DeepLabV3+가 Decoder로 채택되었다.
⇒ Memory Overhead에 따른 Network의 정확도의 균형을 맞추기 위해

![Untitled 8](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7dee802a-6c11-4a26-a0a6-cd60262fd162)

- 논문에서는 RGB-D Semantic Segmentation task를 정확하게 수행하기 위해서 FFCANet이라는 Neural Network를 제안
- 논문은 Semantic Segmentatio task를 위한 Network Structure를 기존의 ResNet을 수정하여 구축
- 이 Module(FFCANet)은 depth information을 통해 RGB information의 Cross-modal calibration을 얻을 수 있고 상호보완적인 정보를 융합할 수 있다.
- 새로운 Structure로써 FFCA Module의 역할은 2개의 다른 modality를 통합하는 것
- 이 Attention module은 다른 어떤 RGB-D Semantic Segmentation Network와 결합될 수 있도록 plug-and-play로 설계되었고, 계산 부담의 증가 없이 Double-Branch Encoder 구조를 갖는다.

![Untitled 9](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/38e998f4-260e-4df3-af65-7b71e5186e1d)