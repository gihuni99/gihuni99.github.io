---
title: SA-Gate 논문 리뷰
date: 2023-01-11 00:00:00 +09:00
categories: [Paper, RGB-D Segmentation]
use_math: true
tags:
  [
    Computer Vision,
    Paper,
    RGB-D,
    Segmentation
  ]
pin: true
---


[Bi-directional Cross-Modality Feature Propagation with Separation-and-Aggregation Gate for RGB-D Semantic Segmentation]

![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/0b5cdccb-be99-49c6-a483-3733ffde9145)

- Depth information은 RGB-D Semantic Segmentation에서 RGB representation에 대한 기하학적 counterpart(대응되는 정보)를 제공하기 위한 유용한 정보라는 것이 입증됨
- 대부분의 기존 연구들은 단순히 depth 측정이 정확하고 RGB pixel들과 잘 정렬되어 있고,
- Most existing works simply assume that depth measurements are accurate and well-aligned with the RGB pixels and models the problem as a cross-modal feature fusion to obtain better feature representations to achieve more accurate segmentation.(해석안됨)
(대부분의 기존 작업은 단순히 깊이 측정이 정확하고 RGB 픽셀과 잘 정렬되어 있다고 가정하고 보다 정확한 세분화를 달성하기 위해 더 나은 기능 표현을 얻기 위해 교차 모달 기능 융합으로 문제를 모델링합니다.)
- 하지만 이것(위의 문장)은 실제 Depth data는 일반적으로 노이즈가 있어, Network가 깊어질수록 정확도를 떨어뜨릴 수 있기 때문에 만족할만한 결과를 얻지 못할 수도 있다.
- 논문에서는 통합되고 효과적인 Cross-modality Guided Encoder를 제안한다.
⇒ RGB Feature responses를 효과적으로 재보정할 뿐만 아니라, multi stages를 통해 정확한 Depth information을 추출하고, 2개의 재보정된 representations를 교대로 통합한다.
- 제안된 Architecture의 핵심은 새로운 Separation-and-Aggregation Gateing operation이다. 
⇒Cross-Modality Aggregation 전에 2개의 representations 모두 공통적으로 filtering하고, 재보정한다.
- Bi-direction Multi-step Propagation strategy는 한편으로 2개의 modility 사이의 정보를 전파하고 융합하는 것을 돕고, 한편으로 Long-Term Propagation process를 따라 그들의 특이성을 보존하게 위해 도입되었다.
- 논문에서 제안된 Encoder는 RGB-D Semantic Segmentation에서 성능을 향상시키기 위해 이전 Encoder-Decoder Structure에 쉽게 삽입될 수 있다.

# [Introduction]

### [2가지 주요 과제]

1. RGB와 Depth modality 사이의 차이(두 information을 효과적으로 통합하는 것은 매우 중요한 문제) ⇒ 지금까지 대부분의 연구들은 1번 과제를 해결하는 것에 중점
- 두개의 modality를 효과적으로 융합하여 사용하는 방식은 그럴듯한 해결책을 제공하지만, Depth information이 정확하고, RGB signal과 잘 정렬된다는 가정은 사실이 아닐 수도 있다⇒야외의 sample(indoor가 아닌)에서는 노이즈가 많이 생길 수 있다.

1. Depth 측정의 불확실성(Sensor로 하는 Depth measurement는 노이즈가 많다)
⇒Depth 측정의 품질에 대한 Network의 민감도를 약화시켜 2번 과제를 해결하려는 연구들이 있었다. 
- Depth data를 추가적인 input으로 받는 것이 아니라, Multi-Task learning을 통해 Depth Feature를 추출하고, Depth Data를 Training을 위한 추가적인 Supervision으로 간주하는 것

## [논문에서 제안하는 2가지 과제에 대한 해결책]

새로운 Cross-Modality Guided Encoder를 FCN(Fully Convolutional Network)과 같은 RGB-D Semantic Segmentation backbone에 도입하여 간단하면서도 효과적인 Framework 제안

- 논문 Framework의 핵심 아이디어는 2개의 modality의 channel-wise(채널 별), spatial-wise(공간 별) correlation(상관 관계)를 활용하여 Depth의 우수한 Feature response를 뽑아냄으로써 낮은 품질의 Depth측정치들의 Feature response를 효과적으로 억제하고, 억제된 Depth representation을 RGB feature를 개선하는 것에 사용한다.
(The key idea of the proposed framework is to leverage both channel-wise and spatial-wise correlation of the two modalities to firstly squeeze the exceptional feature responses of depth, which effectively suppresses feature responses from the low-quality depth measurements, and then use the suppressed depth representations to refine RGB features.)
- Indoor RGB source에도 noisy feature가 포함되어 있기 때문에, 단계를 양방향으로 고안
- Depth data와 달리, RGB noisy feature는 주로 이웃한 서로 다른 object의 유사성에 의해 발생
⇒논문에서는 위 과정을 각각 Depth-Feature Recalibration(재측정), RGB-Feature Recalibration이라고 나타낸다.
- 논문은 새로운 gate-unit인 Separation-and Aggregation Gate(SA-Gate)를 도입
[SA-Gate의 역할]
⇒Network가 먼저 각 modality의 modality-specific feature를 재보정하고 주목하도록 장려함으로써 Multi-Modality representation의 quality를 향상시킴
⇒최종 Segmentation을 위해 2개의 modality의 information feature를 선택적으로 종합
- 두가지 modality Feature 차이의 이점을 효과적으로 가져오기 위해, 논문은 Bi-direction Multi-step Propagation(BMP)를 도입
[BMP의 역할]
⇒Encoder stage에서 information intercation과정동안 두개의 stream이 그들의 specificity(특성)을 더 잘 보존하도록 장려

## [논문의 기여]

![Untitled 1](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/4808afc5-e351-4901-9ccc-c78cf076ddfc)

![Untitled 2](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/3b16cc6e-92ae-4fc3-850f-ba41ce5952fc)

- RGB-D Semantic Segmentation을 위한 새로운 Bi-directional Cross-Modality Guided Encoder를 제안한다. SA-Gate와 BMP module을 이용하면, Noisy Depth Measurement의 영향력을 효과적으로 억제하고, Segmentation을 위해 식별가능한 representations을 형성하기 위한 충분히 상호보완적인 information을 통합할 수 있다.

# Main

![Untitled 3](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f17db705-811f-43d7-9232-ed4857dfcae0)

![Untitled 4](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/3f4b609c-036e-444a-8180-f98ebe3ff48e)

- RGB-D semantic segmentation에서는 RGB와 Depth modality의 Feature를 모두 통합해야한다
⇒두 modality는 필연적으로 노이즈를 가지고 있다.
- Depth 측정은 depth sensor의 특성 때문에 부정확
- RGB feature는 objects사이의 높은 유사성 때문에 정확하지 않은 Feature 만들어낼 수 있다
- Effective cross-modality aggregation 체계는 각 Feature의 강점을 식별할 수 있을 뿐만 아니라 가장 유용한 정보를 주는 Cross-Modality Feature를 효과적인 representation(표현)으로 통합할 수 있어야 한다.
⇒ 새로운 Cross-Modality Guided Encoder를 제안
- Framework는 Cross-Modality Guided Encoder와 Segmentation decoder로 구성
- RGB-D data가 input으로 주어지면, Encoder는 SA-Gate unit을 통해 두 modality의 상호보완적인 정보를 재보정하고 융합, 그리고 Bi-direction Multi-step Propagation(BMP)를 통해 modality-specific feature와 함께 융합된 multi-modal feature를 전파
⇒ 위 정보들은 Segmentation Map을 만들기 위해 Segmentation decoder를 통해 decoding된다.

# [3.1. Bi-direction Guided Encoder]

## Separation-and-Aggregation (SA) Gate

![Untitled 5](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/06463634-71c8-44fc-b113-fd956297c72e)

- 두 modality간의 유익한 Feature Propagation을 보장하기 위해 SA-Gate는 2가지 연산으로 설계
- 1) Feature Separation(FS): 각 modality의 Feature 재보정
⇒ Bi-directional Cross-Modality Feature Propagation을 통해 각 Feature를 효과적으로 개선
    
![Untitled 6](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/03f9c06a-46e9-4b18-9e1a-ef8b7293d5d6)
    

Separation Part의 구조는 다음과 같은데, 각 modality의 Feature를 개선하는 것에 목적을 두고 있습니다. Fgp는 concat한 RGB와 HHA modality를 global average pooling을 하는 연산이고, Fmlp는 MLP network를 의미합니다. 이 결과를 sigmoid 함수에 넣어 noise를 억제합니다. 이를 channel-wise multiplication(연산을 줄이기 위한 것)하고 원래의 서로 다른 modality의 input과 합칩니다. 즉 RGB input에는 HHA filtered를, HHA input에는 RGB filtered를 summation해줍니다. 즉 separation연산은 서로 다른 modality를 통해 Feature를 개선하는 의미를 가지고 있는 것입니다.

- 2) Feature Aggregation(FA): Cross-modality Feature Aggregation
⇒각 Cross-Modality Feature를 상호보완적으로 결합한다.
(마찬가지로 자세한 수식과 과정 생략)

![Untitled 7](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b6920fd9-3f71-4175-9391-899b6f3b4b44)

다음은 Aggregation part입니다. 구조는 다음과 같고, 목적은 서로 다른 modality를 효과적으로 융합하는 것에 있습니다. 연산을 간단하게 살펴보자면 Frgb와 Fhha는 각 modality를 위한 spatial-wise gate인데, 정확한 연산에 대해서는 언급되지 않았습니다. 이렇게 생성된 Gate에 Softmax연산을 적용하고 이 결과를 각 modality의 Weight로 사용합니다. Weight를 적용한 각 modality의 feature를 상호 보완적으로 융합합니다.

![Untitled 8](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/85f60aca-5380-4fbd-989a-835c33adcd32)

![Untitled 9](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/8fda2b1c-05fa-49f1-96b3-03f5c053e410)

![Untitled 10](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/e942a888-6c63-44a0-ba38-0e1244220977)

![Untitled 11](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/ba089184-25f9-4b4f-ac71-fc4fe61ad32a)

- SA-Gate와 BMP module과 함께 Cross-Modality Guided Encoder 소개
⇒서로 다른 modality의 representation을 효과적으로 통합
⇒낮은 품질의 Depth 정보를 보완
- 논문의 Encoder는 Plug-and-Play(연결만 하면 바로 사용 가능한) module이다.
⇒현재 RGB Semantic Segmentation Framework에 적용만 하면 바로 사용 가능
