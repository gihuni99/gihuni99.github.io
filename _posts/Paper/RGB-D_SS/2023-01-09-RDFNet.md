---
title: RDFNet 논문 리뷰
date: 2023-01-09 00:00:00 +09:00
categories: [Paper, RGB-D Segmentation]
use_math: true
tags:
  [
    Computer Vision,
    Paper,
    RGB-D,
    Segmentation
  ]
pin: true
---

[RDFNet: RGB-D Multi-level Residual Feature Fusion for Indoor Semantic Segmentation]

![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/9a8f2519-fb75-4008-8e8e-02fd225a3b4f)

- **RGB-D data를 쓰는 multi-class indoor semantic segmentation에서 RGB feature에 depth feature를 포함시키는 것은 Segmentation Accuracy를 향상시키는 것으로 나타났다.**
- **하지만 이전의 연구들은 multi-modal feature fusion의 잠재력을 충분히 이용하지 못했다.
(단순히 RGB와 Depth feature를 concat하거나RGB와 Depth score map을 평균하는 등의)**
- **multi-modal feature들의 최선의 fusion을 학습시키기 위해서, 논문은 residual learning의 핵심 아이디어를 RGB-D semantic segmentation으로 확장하는 새로운 network를 제시한다.
⇒ 논문의 network는 multi-modal feature fusion block들과 multi-level feature refinement block들을 포함함으로써 multi-level RGB-D CNN features를 효과적으로 capture한다.**
- **Feature fusion block들은 RGB와 depth data의 상호 보완적인 특성을 충분히 이용하기 위해 residual RGB and depth features와 이들의 결합을 학습한다.**
- **Feature refinement blocks는 high-resolution prediction(고해상도 예측)을 위해 multiple levels에서의 융합된 feature들의 결합을 학습한다.**
- **논문의 network는 skip-connection의 이점을 최대한 가져감으로써 각 modality의 끝과 끝에서의 차별적인 multi-level feature들을 효과적으로 학습할 수 있다.**

![Untitled 1](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b45e0957-4467-42db-9703-a1f9c2d67053)

[논문의 기여]

1. Residual learning의 핵심 아이디어를 RGB-D semantic segmentation으로 확장함으로써 very deep network에서 multi-level RGB-D features를 효과적으로 extract하고 fuse하는 network를 제안
2. 논문의 multi-modal feature fusion block은 Skip-connection을 통해 Residual learning의 장점을 최대한 활용함으로써 단일 GPU에서 차별적인 RGB-D features의 효과적인 end-to-end training을 가능하게 한다.

![Untitled 2](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/4ea6aa16-3bdb-47c9-a0ed-64c54d941089)

- RDFNet의 구조

# [Main]

![Untitled 3](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/2b93ed52-026e-4574-ada2-883c5223c843)

![Untitled 4](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c6224af2-3c24-4549-9151-e4c43d7da575)

- Multi-level features의 활용은 high resolution dense prediction(고해상도 밀도 예측)에 중요
- 기존의 RGB-D semantic segmentation은 효과적으로 2개의 modality의 feature를 융합하지 못함
- 논문은 multi-level RGB-D features를 이용하고 Skip-connection을 사용하는 Residual learning을 통해 서로 다른 modality의 feature들을 효과적으로 융합하는 Network를 제안
- 먼저 최근 제안된 RefineNet Architecture를 연구
⇒Residual Connection을 적용하여 RGB semantic segmentation에서 성공을 거둠
- 그 후 Multi-level RGB와 Depth features를 extract, fuse하는 방법을 효과적으로 학습하도록 확장시킨 RefineNet인 논문의 Network를 소개(Indoor Semantic Segmentation을 위한 Network)

# [3.1. RefineNet Review]

![Untitled 5](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5bb192f3-719b-41e9-bf7a-a4164c4e85b1)

- 단일 label prediction layer를 dense prediction layer로 바꿈
- 하지만 output prediction이 각 spatial dimension(공간차원)에서 original image보다 32배 작다
⇒ Sub-building blocks(RefineNet)을 통해 low-level feature들을 통합함으로써 higher-level features를 반복적으로 개선
- RefineNet은 skip connection을 통해 각 단계(each multi-level) ResNet feature와 이전에 Refine된 Feature를 Input으로 받는다.(2개의 input)
⇒ Input으로 받은 Feature들은 sub-components(Residual Convolutional Unit(RCU), Multi-Resolution Fusion, Chained Residual Pooling)의 연속된 구조에 의해 refine되고 fuse된다.
- Residual Convolutional Unit(RCU)
⇒Semantic Segmentation을 위해 pretrain된 ResNet Weights를 미세 조정하는 adaptive convolution set
- Multi-Resolution Funsion Block
⇒Input들을 high-resolution map에 융합시킴
- Chained Residual Pooling(CRP)
⇒Large Region에서 Contextual(맥락과 관련된) information을 encoding하는 것이 목적
(각각 하나의 max-pooling layer와 convolution layer로 구성됨)
* pooling 연산은 주변 location에서 접근할 수 있는 Large Activation Value를 contextual features로 확산시키는 효과를 가지고 있다.

![Untitled 6](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/6d23b2fa-1e0f-4fc7-ac31-1e38a21d21dc)

# 3.2. Our RDFNet with Multi-Modal Feature Fusion

![Untitled 7](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/17ad403e-3a25-448d-9dc5-d42be6e8e59a)

- RefineNet은 단순히 Feature들을 concat하는 것보다 효과적인, 서로 다른 level feature들을 융합하기 위한 일반적인 방법을 제시
⇒논문은 Skip Connection의 이점은 유지하면서 Multi-Modal CNN Feature Fusion을 위한 비슷한 Architecture를 사용
- RDFNet은 Multi-Modal Feature Fusion을 다루기 위해 RefineNet을 확장하였고, fused feature refinement를 위해 RefineNet block들을 포함한다.

![Untitled 8](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7d9472a1-42c3-4172-aae3-e5751ac85b0d)

- 논문의 Feature Fusion Block(MMFNet)은RefineNet과 동일한 구성으로 되어있지만, 요구하는 작업이 조금은 다른, 서로 다른 input들을 받는다는 점에서 다5르다.

[순서]

1. RGB와 Depth ResNet features가 주어지면, MMFNet은 먼저 parameter들의 증폭을 완화하면서 효과적인 training을 가능하게 하기 위해 하나의 convolution으로 각 feature의 dimension을 줄인다.
2. 각 Feature는 RefineNet과 같이 2개의 RCU(Residual Convolutional Unit)와 1개의 Convolution을 통과한다.
⇒ MMFNet과 RefinNet에서 RCU의 의도(목적) 사이에는 분명한 차이점이 존재
⇒ MMFNet의 RCU는 Modality Fusion을 위한 nonlinear transformation을 수행하기를 요구된다.
- 서로 다른 modality의 Feature들은 operation으로 서로를 향상시키기 위해 상호보완적으로 결합된다.
- 반면, RefineNet의 RCU는 higher-resolution의 lower level feature를 이용하여 coarse(조잡한) higher level feature 개선한다.
3. MMFNet의 다음 Convolution은 서로 다른 modality의 Feature를 적응적으로 융합하는 것에 중요할 뿐만 아니라, summation을 위해 Feature Value를 적절하게 re-scaling(재조정)에도 중요하다.
- Color Feature Value(RGB Value)가 일반적으로 Depth Feature보다 Semantic Segmentation을 위한 더 좋은 Discrimination(식별) power를 가지고 있기 때문에, Summantion Fusion은 주로 supplementary(부가적인) 또는 residual(잔여의) Depth feature들을 학습하기 위해 동작
(복잡한 pattern들을 식별할 수 있는 RGB Feature를 개선할 수도 있다)
- 각 modality feature의 중요성(가중치)은 RCU block 다음에 있는 convolution 안의 학습 가능한 parameter들을 통해 조절될 수 있다.
4. Fused Feature에서 확실한 Contextual Information을 포함하기 위해 추가적인 Residual Pooling operation을 수행한다.
- 각 level의 MMFNet에서 Residual Pooling 하나만 있으면 충분하다는 것을 발견했다.
- 더 강력한 Contextual information은 RefineNet Blocks를 통해 다음과 같은 Multi-level fusion에 그 이상으로 통합될 수 있다.
- MMFNet의 output은 RefineNet의 RCU를 곧바로 통과하기 때문에, MMFNet의 마지막 부분에 들어가는 RCU는 생략하였다.

![Untitled 9](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/88eef5f7-74a7-4c81-89e6-78296b46362d)

- 논문은 Semantic Segmentation에서 효과적인 Multi-Modal CNN Feature를 extract하기 위해 skip-connection을 활용하여 Residual Learning의 최대 이점을 가져가는 새로운 Network를 제안
- Residual Architecture는 단일 GPU에서 very deep RGB-D CNN Feature의 효과적이고 end-to-end training을 가능하게 한다.
- MMFNet은 최근의 Multi-level Feature Refinement Architecture가 Skip-connection의 장점을 유지하면서, 서로 다른 modality의 Features를 활용하도록 효과적으로 확장 가능하다는 것을 보여준다.
