---
title: MGCNet 논문 리뷰
date: 2023-02-06 00:00:00 +09:00
categories: [Paper, RGB-D Segmentation]
tags:
  [
    Computer Vision,
    Paper,
    RGB-D,
    Segmentation
  ]
pin: true
---

[Multilevel Gated Collaborative Network for RGB-D Semantic Segmentation of Indoor Scene]

![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/706074dc-f9ae-4c93-9bf0-e5d28bb2129a)

- 실내 RGB-D semantic segmentation은 오래 동안 연구주제로 이어져오고 있다
- 하지만 modal 정보의 본질적인 차이와 multi-level feature cue의 큰 차이로 인해, 차선의 indoor scene segmentation을 제공하는 기존의 U-Net framework를 채택하였다.
- 논문에서는 정확한 segmentation을 얻기 위해 효과적인 Feature 탐지 접근법을 생각했다.
- 구체적으로, 3가지 step이 존재한다
1) Encoder에서, 본질적으로 일치하는 Feature Fusion을 얻기 위해, fusion을 위한 guide로 2개의 modality의 ‘difference weight’를 추출하는 ‘difference-exploration fusion module’을 구상했다.

Gated Decoder module은 나머지 2), 3)단계와 관련이 있다.
2) layers사이의 차이를 줄이기 위해, fusion 정보의 각 level을 위한 gating unit을 사용
⇒layer사이의 정보 차단을 피하면서 특정 layer의 고유한 차이를 증가시킨다.
3) 직렬 병렬을 번갈아 사용하는 전략을 사용한다
⇒ contextual knowledge를 추출하는 능력을 증가시키기 위해서
- 위 3가지 step을 고려하여 MGCNet(Multilevel Gated Collaborative Network)를 구상

# Introduce

- 기존 연구에 따르면 modal과 multilevel 정보를 효과적으로 사용하면 Segmentation성능을 상당히 향상시킬 수 있다.
⇒ 논문은 modality funsion의 효과, 효과적인 multilevel information 결합, 그리고 context extraction을 중점으로 둔다.
- 새로운 Multilevel Gated Collaborative Network(MGCNet) 제안

### MGCNet

- DEFM(difference-exploration fusion module)을 통해 modality fusion에서 modal difference의 영향을 완화시킨다.
⇒상보성을 높이고, modality사이의 충돌을 줄인다.
- GDM(Gate Decoder Module)은 다른 level끼리의 Feature 차이를 줄이고, 특정 level들의 특성(distinction)을 향상시키고, level끼리의 어떠한 정보의 차단도 피한다.
- Serial-Parallel Alternation Strategy(직병렬 교차 전략)을 통해 Decoding하는 동안 풍부한 Contextual Information이 extract된다.

# Method

![Untitled 1](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7be22c26-480e-48f1-94d2-58fe4ba971f4)

- 위는 DEFM으로 구성된 Encoder와 GDM으로 구성된 Decoder를 갖는 MGCNet의 구조이다.
- 4-level에 대한 modal encoding information을 생성한 후, Top-Down(하향식) 방향으로 직접 upsampling하고 decoding하는 것을 포기한다.
⇒서로 다른 level끼리의 information 차이의 본질 때문에, 큰 Feature의 상반성이 존재하고, 직접 Feature 통합은 정보의 차단을 이끌 수도 있다.
- 위 문제를 해결하기 위해 Gate Unit을 사용
⇒각 Gate Unit에서  현재 level은 main branch의 input으로 사용되고, 나머지 모든 layer들은 auxiliary(보조의) branch의 input으로 사용된다.(정보 간의 상호작용을 위해)
- 또한 convolution의 receptive field의 크기는 Context extraction과 밀접한 관련이 있다.
⇒ Gating 이후, 각 level별로 contextual information을 extract하기 위해 새로운 Convolution전략을 사용( 이 과정까지 GDM이라고 말한다)

# DEFM(Difference-Exploration Fusion Module)

![Untitled 2](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b4e99a22-a011-41f1-aea8-17d3a798f968)

- RGB에는 풍부한 visual color와 texture 단서가 있다.
Depth에는 풍부한 기하학 정보가 있다.
- 2개의 modality에 담긴 정보는 본질적으로 다르다.
⇒이 차이는 modality가 합쳐질 때, 정보 충돌을 줄이는데 사용할 수 있다.

(모든 연산은 element-wise(요소 별))

1. 요소 별 뺄셈과, Convolution 연산을 통해 차이를 나타내는 Feature정보를 직관적으로 얻는다.
2. 각 modal마다 가중치를 부여하고, 최종 modality fusion을 얻는다. 식은 아래와 같다.

![Untitled 3](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d56570c6-ac34-4a41-ad85-469d178574ac)

# GDM(Gate Decoder Module)

- DEFM의 결과를 얻은 후, 정확한 Segmentation을 위해, 각 layer의 정보들을 점진적으로 합하고, 해상도를 복원해야한다.
⇒ 하지만 level의 차이가 클 수록 비호환성/비유사성도 증가한다.
- 또한 contextual information extraction도 중요하다.
- 위 두가지 문제를 위해 2가지 단계의 GDM이 개발되었다.

![Untitled 4](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/784a1f90-ff95-4348-b97f-7afd6cc29c49)

- 3개의 보조 branch의 Weight를 생성하고, main branch에서 추출된 정보와 함께 가중치를 부여
⇒ 이를 A라고 명칭
- 추출된 정보를 더 포괄적으로 만들기 위해 확장성이 다른 Serial(직렬) convolution 사용
    
![Untitled 5](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/6d114d8b-68a6-45a3-92fb-9c82cc49a862)
    

(CBR은 BN(Batch Normaliztion)과 ReLU가 뒤따르는 Convolutional layer를 의미한다)

(Conv3는 3x3 kernel을 가진 convolution)

- Ii에 Channel Attention을 사용하고, 그 결과로 부터 spatial weight인 Si를 얻는다.
- Ai에 (1-Si)의 가중치를 부여한다.
⇒ 이 layer에서는 무시되지만, 다른 layer에서는 관련이 있는 정보를 extract하기 위해
⇒ 정보의 격차를 더 채울 수 있다.
- 마지막으로 정보의 손실을 막기 위해 원래의 정보와 융합한다.

![Untitled 6](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/afdfc5e9-43d3-4673-b4da-b5d0d3901acb)

(CA는 Channel Attention)

- 두번째 단계에서는, 해상도의 복구와 Contextual 정보를 추출하는 것이 목표

![Untitled 7](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f96ae1d9-5447-4c3f-a0a9-c6b683c4ef7d)

- 이전 layer의 통합된 정보가 input된 이후, 3개의 branch를 contexts를 추출하는 것에 따로 사용한다.
- 동일한 branch의 Feature map은 하나의 convolution kernel을 사용하여 생성된 Feature보다 더 나은 Feature를 만들기 위해, 다른 scale의 Feature들을 추출하고 합치기 위한 Multiple Convolution을 이용한다.
- 각 branch에 대해, 서로 다른 receptive field로부터 정보를 extract하기 위해 1, 3, 5 kernel을 가진 convolution을 먼저 사용하고, 추가 extraction을 위해 이전보다 큰 receptive field를 가진 convolution을 선택한다.
- 연산량을 고려하여, 팽창률이 2, 4, 8인 convolution을 각각 선택한다.
- 마지막으로 3개의 branch를 concat한다.
⇒위의 receptive field가 확장되고 branch들이 합쳐지는 연속된 convolution의 serial-parallel strategy를 통해, 모든 level에서의 contextual 정보를 성공적으로 extract 할 수 있다.

![Untitled 8](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/938fd026-4e25-4724-a58e-53c17f8a9568)

(Tconv는 Transposed convolution layer(with BN, ReLU))

(SP연산은 아래와 같다)

![Untitled 9](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/8c3de485-54c8-4a7c-bdf6-9d487c40fc88)

(b m,i는 3개의 병렬 branch의 output)

(in m,i는 SP()의 input)

![Untitled 10](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/13870873-22f0-40ee-9183-d71b5c9b68ce)

![Untitled 11](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/6150e3c4-93aa-4dc4-8eb7-cd2e2015b2d7)

우리는 장면 분할 분야에서 기존 무대 기능 협업 전략을 효과적으로 보완
하기 위해 MGCNet이라는 새로운 프레임워크를 제시했습니다. 첫째, 제안
된 DEFM은 협력 충돌을 줄이기 위해 차등 탐색을 통해 양식의 융합을 안내
합니다. 디코더에서는 레이어 간의 정보 배제를 피하고 상황 정보를 적절하게
캡처하기 위해 게이팅 기능을 제안합니다. 광범위한 실험은 당사의 MGCNet
이 다른 최첨단 방법과 비교하여 경쟁력 있는 분할 결과를 달성한다는 것을
나타냅니다. 그러나 우리의 방법은 객체 경계의 원활한 분할을 제공하지 않으며 복잡한 작
업은 중복 정보의 도입을 피할 수 없으며 실시간으로 요구 사항을 충족합니
다. 앞으로도 서로 다른 클래스 간의 분할 경계를 세분화하는 방법을 계속 탐
색하고 고효율 추론을 사용하여 어려운 분할 작업을 해결하기 위해 노력할 것
입니다.