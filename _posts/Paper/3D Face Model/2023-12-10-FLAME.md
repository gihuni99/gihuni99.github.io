---
title: Learning a model of facial shape and expression from 4D scans(FLAME) 논문 리뷰
date: 2023-12-10 00:00:00 +09:00
categories: [Paper, 3D Face Model]
use_math: true
tags:
  [
    Paper,
    3D Face Model
  ]
pin: true
---

![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5f27d19d-2249-4d09-915d-b276fe8a1bc7)

![Untitled 1](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/ef679e66-fe5c-4b5a-89bd-b55a550b8372)

![Untitled 2](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/596ae1f0-a94f-40f1-8a9c-91c72dfa3f2f)

- 3D face modeling 분야는 ‘high-end method’와 ‘low-end method’ 간의 큰 차이가 있다.
⇒ ’high-end method’에서는 실제 사람과 best facial animation이 구별되지 않지만, cost가 크다.(수동으로 조작해야 하는 것이 많다)
⇒ ‘low-end method’는 일반적인 depth sensor를 통한 face capture는 실제 facial shape과 expression의 다양성을 충분하게 표현하지 못하는 3D face model에 의존한다.
- 논문의 **“FLAME(Faces Learned with an Articulated Model and Expressions)”** model은 기존의 graphics software를 통해 동작하고, data에 쉽게 fit될 수 있도록 design되었다.
- FLAME은 3800개의 human heads scans를 통해 학습된 linear shape space를 사용한다.
- FLAME은 **‘articualted(표현된) jaw, neck, eyeballs를 포함하는 linear shape space’, ‘pose-dependent corrective blendshapes’, ‘additional global expression blendshapes’**를 결합한다.
- ‘pose와 expression에 depedent한 articulations(얼굴의 표현)’는 추가적인 4D sequences와 함께 ‘D3DFACS’ dataset의 4D face sequences를 통해 학습된다.
- 논문은 정확하게 **‘template mesh’를 scan sequences에 ‘registeration’**하고, 연구목적으로 사용할 수 있는 **D3DFAC3 registration**을 만들었다.
⇒ **template mesh는 기본적인 facial mesh**를 의미하고, **register한다는 것은 template mesh를 scan sequence에 matching되는 mesh를 만든다는 뜻**이라고 이해했다.
⇒ scan에 맞는 tamplate mesh를 만든다.
- 최종적으로 model은 33000개의 scans를 통해 학습된다.
- FLAME은 ‘low-dimensional’이지만, ‘FaceWarehouse’나 ‘Basel Face Model’보다 더 잘 표현한다.
- 논문은 FLAME과 기존 model들을 같은 optimization method를 사용한 static 3D scans와 4D sequences에 fitting함으로써 성능을 비교했다.

# 1. Introduction

![Untitled 3](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7bbb7120-88e3-4a0b-9939-ff7a54259ceb)

![Untitled 4](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/4c8e7561-5faa-497c-ac9f-796d2de3d7a7)

- 이 논문은 3D face modeling분야의 상당한 차이에 대해 다룬다.
- Spectrum의 한쪽 끝에는 individual의 scans 또는 images를 통해 학습되고, 3D artist의 상당한 input을 포함하여 매우 정확하고 사실적인 3D models
- 다른 한쪽 끝에는, images, video, RGB-D 등의 데이터에 fit할 수 있지만, 사실성이 부족한 generic face models가 있다.
⇒ 두 Spectrum의 타협점을 찾아야 함.
- 위 spectrum에 없는 것은, **compact**하고 **data에 fit**할 수 있고, **realistic 3D face detail을 capture**할 수 있고, **animation**하도록 할 수 있는 **‘generic 3D face models’**이다.
- 논문의 목표는 **“low end” model을 “high end” model 쪽으로 이동**시키는 것
⇒ **model이 4D scans(3D scans sequence)의 facial shape과 expression에 대해 학습**

![Untitled 5](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/18ef3eef-a429-4b42-8b68-80eedb5bdf6c)

- 이전의 generic face model들은 대부분 neutral expression(무표정)의 어린 Europeans의 한정된 3D face scans를 이용하여 만들어졌다.→ “Blanz and Vetter”
- 더 최근에는, “FaceWarehouse” model은 20개의 다른 facial pose와 함께, 다양한 나이와 인종을 갖는 150명의 scans를 사용하였다.
- 널리 사용되었지만, data의 한정된 양은 위 model들이 표현할 수 있는 facial shapes의 범위를 제한했다.

![Untitled 6](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/1636db80-1648-4c48-82e3-c57019bd9a40)

- 기존 모델들의 한계점을 해결하기 위해서, 논문은 3개의 heterogeneous(다양한, 다차원의) dataset을 사용⇒ 33000개 이상의 3D scans를 사용
- **FLAME** model은 **identity, pose, facial expression의 representation으로 구별되어 factorization**되었다. ⇒ human body model과 유사하다.
- model simple, computationally efficient, 기존 game과 rendering engine에 호환되도록 하기 위해, 논문은 **relatively low polygon count(상대적으로 낮은 다각형 수), articulation, blend skinning을 갖는 ‘vertex-based model’**을 정의한다.
- 구체적으로, FLAME은 **‘identity variations, articulated jaw and neck, rotate되는 eyeballs의 학습된 shape space’를 포함**한다.
- 추가적으로, 논문은 **‘pose-dependent blendshapes’**를 학습한다.
⇒ sample들의 **jaw, neck**을 위해
- 마지막으로, **“expression” blendshapes를 학습**한다.
⇒ **non-rigid(뻣뻣하지 않은, 자연스러운) 얼굴의 변화를 capture**하기 위함
*blendshape: mesh의 vertex를 이동하는 것에 사용하며, 이를 통해 facial expression을 조절*

![Untitled 7](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/0f616dfc-2302-4c91-a5e6-98055ffcdb02)

- 논문은 대략 4000개의 다양한 나이, 인종, 성별을 갖는 ‘CAESAR’ body scans의 heads를 통해 **“identity shape space”**를 학습한다.
- pose와 expression의 다양성을 modeling하기 위해, 논문은 400개가 넘는 ‘D3DFACS dataset’의 4D face capture sequences와 직접 취득한 추가적인 4D sequences를 사용
⇒ 더 많은 expression variation을 포함
- **모든 model parameters는 ‘3D reconstruction error’를 최소화하기 위해 data를 통해 학습**된다.
⇒ 이를 가능하게 하기 위해, **모든 scans(CAESAR and 4D)에 대해 ‘template mesh’의 “detailed temporal registration”을 수행 (즉, 각각의 scan에 맞는 template mesh를 만든다)**

![Untitled 8](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/ad20b7e8-4218-4169-b5d4-3a0512999331)

- CAESAR dataset은 3D body shape modeling을 위해 광범위하게 사용되었지만, face modeling에서는 명시적으로 사용되지 않았고, CAESAR을 통해 만들어진 기존 body models는 ‘facial articulation, expression’을 capture하지 않는다.
- 논문(FLAME)은 **“SMPL body model”과 유사한 접근법을 사용했지만, 이를 face, neck, head에 적용**하였다.
- **“SMPL”은 ‘parameterized blend-skinned body model’**이다.
⇒ **‘identity shape space’, ‘articulated pose’, ‘pose-dependent corrective blendshapes’를 결합**한 model
- SMPL은 ‘facial motion’을 modeling하지 않고, 논문에서 SMPL에 더하여 ‘expression blendshapes’를 학습하도록 하였다.

![Untitled 9](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/4f7bc1e9-a745-41c9-bb90-a72f92de3db1)

![Untitled 10](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/3334f736-d4e3-4dc0-a020-bea11eee88a1)

- faces가 full body scans에서 상대적으로 low-resolution인 것을 고려할 때, scans을 정확하게 registering하는 task는 중요하면서도 어렵다.
- 정확한 registration을 취득하기 위해서, ‘co-registration’의 형식은 논문에서 face model을 build하고 동시에 raw data alignment하기 위해 사용한다.
- Registration이 주어졌을 때, 논문은 facial shape model을 만들고, “BFM”과 “FaceWarehouse”보다identity shape space의 결과가 더 풍부하다는 것을 보인다.

![Untitled 11](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/3b2246aa-874e-458c-aa7f-f9ba174a24bf)

- “FaceWarehouse”는 ‘raw scan data(depth sensor로 측정됨)’에 align된 template meshes와 함께 많은 수의 facial expression을 갖는 유일한 공개적으로 이용 가능한 ‘3D face database’이다.
- “D3DFACS dataset”은 훨씬 quality 좋은 scan을 갖지만, align된 mesh를 포함하지 않는다.
- 4D data를 Registering하는 것은 여전히 challenge하다.
- 논문은 texture를 갖는 3D scans sequence(4D data)로부터 high quality alignment를 취득하기 위해 ‘co-registration’과 ‘image texture’을 사용한다. (full bodies에 대한 연구와 유사)
- model에 eyeballs를 포함하는 것은 eye region(특히 eyelid)에 대한 alignment를 향상시킨다.
- registration과 model learning process는 fully automatic하다.

![Untitled 12](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/cdb2aa70-7573-4c81-ac0e-b19215fda806)

- 이전 연구에서 벗어나, 논문은 ‘expression’과 ‘blendshapes’를 “facial action units(FACS)”으로 묶지 않는다.
- 대신 ‘global linear model’을 통해 ‘blendshapes’를 학습한다.
⇒ 얼굴의 correlation을 capture
- “FACS models”는 어떤 settings을 해도 똑같은 shape을 가질 수 있다는 점에서 overcomplete하다.(overcomplete는 basis보다 function의 수가 더 많은 것)
⇒ data를 통해 parameter 설정하는 것을 어렵게 한다.
- 반면, “FLAME model”은 **orthonormal expression space**사용한다.
⇒ **identity와 pose로 factorization**된다.
- 이는 noisy, partial, sparse data에 대해 fitting하는 것에 장점이 있다고 논문은 주장한다.
⇒ representation을 통해 다른 sparse rigs를 만들 수 있다.

(rigs는 움직임을 위한 뼈대라고 생각하면 된다)

![Untitled 13](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/bec4b180-7ea9-42a3-a452-e9d3a265440a)

- 이전 연구들과 다르게, 논문은 **head와 neck을 같이 modeling**한다.
- 이는 **head가 neck과 연관되게 rotate**되도록 하고, **neck이 rotation 중 어떻게 변화하는지 capture하기 위해 ‘pose-dependent blendshapes’를 학습**한다.
- 이는 rotation동안 ‘neck tendons(힘줄)의 돌출’와 같은 영향들을 capture한다.
⇒ realism 증가

![Untitled 14](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/43d84b4b-80af-4fff-93d9-f2626f500c00)

- 논문의 주요 기여는 “기존 head, face models보다 상당히 정확하고 expressive한 statistical head model”이다. (기존 graphics software와 호환 가능)
- 기존 model들과 다르게, “FLAME”은 ‘head pose’와 ‘eyeball rotation’을 명확하게 modeling한다.
- 게다가, 서로 다른 model들 간의 상세한 정량적 비교, 분석을 제공한다.
- 논문은 학습된 모델을 연구 목적으로 이용 가능하도록 공개한다.
(release에는 모델을 animate하고 사용할 수 있는 software에 여성, 남성 모델로 구성되어 있음)
- 게다가, “D3DFACS dataset”의 ‘temporal registration’을 연구 목적으로 공개 이용 가능하도록 하였다. → 다른 모델들을 학습시키는 것에도 사용할 수 있다.

# 3. Model Formulation

![Untitled 15](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/4d3847a5-769b-489a-9408-3f1c2169627e)

- FLAME은 “SMPL” body model formulation을 head로 적용하였다.
- SMPL body model은 ‘facial pose(jaw 또는 eyes의 articulation)’ 과 ‘facial expression’이 없다.
- SMPL을 확장하여 FLAME은 연산적으로 효율적이고, 기존 game engine과 호환 가능하다.
⇒ SMPL과 동일한 notation을 사용한다.

---

***Figure 2***

![Untitled 16](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7179152a-b544-427f-856e-274ec3c71e39)

- FLAME model의 Parameterization
    - **Shape**: 3가지 shape에 대한 변화를 -3~+3 standard deviation으로 조절
    - **Pose**: 6개의 neck, jaw joints 중 4개를 Rotation방식으로 조절하여 동작시키는 parameter
    - **Expression**: 3가지 expression에 대한 변화를 -3~+3 standard deviation으로 조절

---

![Untitled 17](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/43252884-e254-4aa3-9014-3890363f5c86)

![Untitled 18](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/226c17a7-c6d4-44e8-b2a1-b2de1648b935)

- “SMPL”에서, ‘geometric deformations’는 subject의 고유한 shape 또는 kinematic tree의 pose 변화에 의한 deformations로 인해 발생한다.
- 하지만, **“face”**에서는, 많은 deformation이 **‘muscle activation’**에 의해 발생한다.
⇒ articulated(관절로 연결된) pose 변화와 관련이 없다.
- 따라서 논문은 추가적인 **‘expression blendshapes’를 추가**하여 SMPL을 확장하였다.(Fig 2)
- 여러 experiments에서 다른 method와의 비교를 위해 face region만을 보여주지만, “FLAME”은 face, full head, neck을 모두 modeling한다.

---

***Figure 3***

![Untitled 19](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d5100c21-32e1-4654-94bd-73e4a4ae5b75)

- 남성과 여성의 Joint 위치를 나타낸다.
    - Pink/Yellow: right/left eyes
    - Red: neck joint
    - Blue: jaw joint

---

![Untitled 20](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/12d552ce-d0b9-43a6-afcb-305e206de219)

- “FLAME”은 **LBS(Linear Blend Skinning)을 기반으로 하는 standard vertex를 ‘corrective blendshapes’를 통해 사용**
⇒**N=5023: vertex 수/ K=4: joints 수(neck, jaw, eyeballs) / Blendshapes**(data를 통해 학습)
    - Corrective Blendshapes: rest pose shape에서 deformation이 있을 때, 피부나 형태 등을 자연스럽게 변화할 수 있게 해주는 방법
- ** “FLAME”은 function $M(\vecβ,\vecθ,\vecψ) : \R^{|\vecβ|\times|\vecθ|\times|\vecψ|} \rightarrow \R^{3N}$으로 정의된다.**
    - $**\vecβ \in R^{|\vecβ|}$: Shape**
    - $**\vecθ \in R^{|\vecθ|}$: Pose**
        - 각 pose vector $\vecθ \in \R^{3K+3}$은 K+1개의 ‘axis-angle representation’ rotation vector($\in \R^3$)들을 포함한다.
        ⇒**’axis-angle representation’**이란 ?
            
            ![Untitled 21](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c03db31f-685c-4488-9eb8-5d61a8a096b5)
            
            ![Untitled 22](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/ec8c5b9c-209e-4c84-a546-bb738c752b52)
            
            axis가 축을 의미하고, analge만큼 축을 기준으로 rogtation하는 것을 vector로 나타낸 것
            (위 예시는 z축의 -방향 기준으로 -pi/2만큼 rotation하는 것)
            
            K개의 joint마다 1x3 rotation vector와 1x3 global rotation vector 1개
            
    - $**\vecψ \in \R^{|\vecψ|}$: Expression**
    
    ⇒ **N개의 vertex를 return**한다.
    

![Untitled 23](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/a9f3a4fe-6ca6-4924-9253-66824189998d)

![Untitled 24](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/edcf3b0f-db0a-471a-97a1-19f1cfdbed47)

- Model은 다음과 같이 구성된다.
    - **Template Mesh**($\overline{T}\in \R^{3N}$): zero pose($\vecθ^*$)”에서의 Mesh
    ⇒ 여기서 zero pose라는 것은 pose($\vec\theta$)가 없는, 즉 **shape($\vec\beta$)만 존재하는 mesh**인 것
    주의) shape과 pose가 모두 없는 것은 ‘generic face template mesh’이다.

    - **Shape blendshape function**($B_S(\vec{\beta};\mathcal{S})$ : $\R^{|\vec{\beta}|} \rightarrow \R^{3N}$): identity와 관련된 shape 변화를 나타내기 위함(몸매, 키, 체형 등 indentity마다 다른 특성을 위한 것이라고 이해)

    - **Corrective pose blendshapes**($B_P(\vec{\theta};\mathcal{P}): \R^{|\vec{\theta}|}\rightarrow \R^{3N}$): LBS(linear blend skinning) 하나만으로 표현되지 않는 ‘pose deformation’을 수정

    - **Expression blendshapes**($B_E(\vec\psi;\mathcal{E}): \R^{|\vec\psi|}\rightarrow \R^{3N}$): facial expressions를 caption
- **Standard skinning function**($W(\overline{T},J,\vec\theta,\mathcal{W})$은 $\overline{T}$(template mesh)중 joints($J\in \R^{3K}$) 주변의 vertex들을 rotation할 때 적용.( **‘blendweight**($\mathcal{W}\in \R^{K\times N}$)’에 의해 linear하게 smooth된다.)
⇒ 즉 **joint에 가까운 vertex일수록 rotation이 강하게 적용**된다.

![Untitled 25](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/69993a3c-b6c4-43f6-a78f-c3357f905b3f)

- Model에 대한 공식은 위 식(1)과 같다.
⇒ 위 $T_P(\vec\beta,\vec\theta,\vec\psi)$는 **template mesh에 shape, pose, expression offset을 더한 것, 식(2)**

![Untitled 26](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/1f7f9a37-dbd1-4bfc-9e3e-bc663c6f003c)

- **‘face shapes’가 다르다는 것은 ‘joint locations’가 다르다는 것을 의미**하므로, joint는 face shape function으로 정의된다. ⇒ $J(\vec\beta;\mathcal{J},\overline{T},\mathcal{S})=\mathcal{J}(\overline{T}+B_S(\vec\beta;\mathcal{S}))$ 
($\mathcal{J}$는 **‘Mesh vertices’로부터 ‘joint location’을 계산하는 방법**을 정의하는 “sparse matrix”)
- 위와 같은 ‘joint regression matrix($\mathcal{J}$)’는 training examples를 통해 학습된다.
⇒ Figure 3를 보면 학습된 joint location을 볼 수 있다.(head shape에 따라 자동으로 계산됨)

### *Shape blendshapes*

![Untitled 27](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/60c200d0-d15a-4044-957b-67a1786aad18)

- 각각의 다른 **subjects(identity)의 shape variations**는 위 식(3)과 같은 ‘linear blendshapes’에 의해 modeling된다.

![Untitled 28](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/bd2922ec-9e6f-4963-82f7-3df19f780537)

- $\vec\beta=[\beta_1, ... ,\beta_{|\vec\beta|}]^T$: shape coefficients
- $\mathcal{S}=[\mathcal{S}_1,... ,\mathcal{S}_{|\vec\beta|}]\in \R^{3N\times|\vec\beta|}$: orthonormal shape basis
⇒ PCA를 통해 아래에서 학습된다.(shape space의 training은 6.3에서 설명됨)

### *Pose blendshapes*

![Untitled 29](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/e8d554d4-a54f-4a79-aba2-02baf43a57af)

- $R(\vec\theta) : R^{|\vec\theta|}\rightarrow R^{9K}$를 **‘face/head/eye pose vector($\vec\theta$)’**에서 **‘각 vector에 대응되는 rotation matrix를 모두 concat한 vector’**라고 정의하자.
⇒ **“pose blendshape function”**은 위 식(4)와 같다.

![Untitled 30](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/2c0ab659-530f-45bd-985a-3ef8c33ca179)

- $R_n(\vec\theta)$, $R_n(\vec\theta^*)$은 각각 $R(\vec\theta),R(\vec\theta^*)$의 n번째 element이다.
- **‘vector $P_n \in \R^3$’**는 $R_n$에 의해 활성된 ‘rest pose’로 부터의 vertex offsets
(즉, **rest pose에서부터 vertex의 변화 정도**를 의미하는 것 같다)
- ‘**Pose space**( $\mathcal{P}=[P_1, ... , P_{9K}]\in \R^{3N\times 9K}$ )’는 **모든 “pose blendshapes”를 포함하는 matrix**이다.
- ‘pose blendshapes’는 R에 대해서는 linear하지만, $\vec\theta$에 대해서는 non-linear하다.
⇒ $\vec\theta$에서 ‘rotation matrix elements’로 “non-linear mapping”되기 때문
(pose parameters를 data로부터 계산하는 자세한 내용은 section 6.1)

### *Expression blendshapes*

![Untitled 31](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/1771c840-c987-43b5-96ad-9893a2003352)

- shape blendshapes와 유사하게, **‘expression blendshapes’는 linear blendshapes**에 의해 modeling된다. 식(5)와 같다.

![Untitled 32](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/8cb4e958-8541-4aae-9252-5775eb8ec9d4)

- $\vec\psi=[\psi_1, ... , \psi_{|\vec\psi|}]^T$ : expression coefficients
- $\mathcal{E}=[E_1,... ,E_{|\vec\psi|}]\in \R^{3N\times|\vec\psi|}$: orthonormal expression basis
- SMPL model에는 위와 같은 ‘expression blendshapes(pose에 영향을 받지 않음)’와 같은 요소가 없다.(자세한 내용은 Section 6.2)

### *Template shape*

![Untitled 33](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7a0ad179-b623-4a85-bfb2-f2ebed7a42e5)

- **‘shape blendshapes’, ‘pose blendshapes’, ‘expression blendshapes’는 모두 ‘template mesh($\overline{T}$)’의“displacements(변위)”이다.**
- 논문에서는, 먼저 1) ‘generic face template mesh’에서 시작하여, 2) scans를 통해 $\overline{T}$를 포한한 나머지 model들($B_S, B_P, B_E$)도 학습한다.
(또한 ‘blend weights($\mathcal{W}$)’도 학습한다.)

---

Figure 4

![Untitled 34](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c4ca30d5-e445-4fd6-af5e-722a2d4b56ac)

- face registration, model training, application

---

# 4. Temporal Registration

![Untitled 35](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c2ad8a68-40d0-4fc8-889e-c552c0aeec99)

- 통계적으로, facial shape modeling은 모든 training shape이 full vertex correspondence이어야 한다.
- 3D scans sequences가 주어졌을 때, 각 scan $i$마다, ‘registration process’에서 “aligned template($T_i\in\R^{3N}$)” 를 계산한다.
- “Registration Pipeline”에서는 다음 과정을 번갈아 수행한다. (Figure 4에서 볼 수 있다.)
    - **1) FLAME model을 regularization하면서 “registering mesh(registration)”**
    - **2) registration을 통해 FLAME model “training”**

## 4.1. Initial Model

![Untitled 36](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/4708ae98-ee46-4b40-9740-c5926be1d807)

- “alternating registration process”에는 초기 FLAME model이 필요하다.
- FLAME은 ‘shape{$\overline{T},\mathcal{S}$}’, ‘pose{$\mathcal{P,W,J}$}’, ‘expression $\mathcal{E}$’의 parameter들로 구성되어 있다.
⇒ initialization이 필요하고, 이후 registered scan data를 통해 refine한다.

### *Shape*

![Untitled 37](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/8404b56b-f385-4a1f-ab68-e68ce865655d)

- **‘initial head shape space’**를 취득하기 위해, 논문에서는 **“SMPL”의 ‘full-body registrations’**를 통해 “CAESAR” dataset에서 **‘head region’을 extract**한다.
- full-body SMPL template의 ‘mesh structure’를 refine하고, mouth와 eyes를 위해 topology(위상)이 holes를 포함하도록 수정한다.
- 그 후, ‘SMPL full-body shape registrations’와 ‘논문의 refined template’사이의 **“deformation transfer”**를 사용한다.
⇒ ‘refined head template’이 있는 full-body registrations’를 취득하기 위해
- 위 ‘registered head templates’를 사용하여, vertex에 PCA를 적용함으로써, ‘identity’를 나타내는 ‘initial shape blend shapes’를 계산한다.

![Untitled 38](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f2c95ca3-e10e-48f2-a3c9-15aff581386b)

![Untitled 39](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7629fdb0-e667-4969-a7c8-9cd1061e8046)

- ‘Registration’과정을 더 안정되게 하고, model의 visual quality를 증가시키기 위해, 논문은 ‘shape model’에 **eyeballs**를 추가했다.
- eyes를 initialize하기 위해, **“Woods”의 eye region model**을 사용하여 left eyeball을 배치하고, left eye 주변의 vertex들을 고려하여 geometric center를 regression한다.
- 마지막으로 right eye에도 같은 regressor를 적용한다.

### *Pose*

![Untitled 40](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/873ba78b-80d8-4cc2-9fa1-f285562dfac3)

- ‘blendweights($\mathcal{W}$)’과 ‘joint regressor($\mathcal{J}$)’는 사용자가 수동으로 정의한 weights에 의해 initialize된다.
- ‘eyeball joint regressors’의 initial vertex는 eyeball geometric center에 가까운 joints를 만들어내도록 수동으로 선택된다.

### *Expression*

![Untitled 41](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/0e941b42-c9c6-4303-b92d-5ba0a5905829)

- expression parameters ($\mathcal{E}$)를 initailize하기 위해, ‘correspondence(연관성)’를 설정한다.
⇒ 논문의 ‘head template’과 “Li”의 ‘artist generated FACS-based blendshape model’ 사이의 mesh registration을 통해
    - *FACS(Facial Action Coding System)는 얼굴 표정의 움직임을 분석하고 분류하는 체계*
    - 즉 **다양한 표정이 있는 blendshape와 head template 사이의 mesh registration**을 통해 ‘correspondence’를 구하고, 이를 이용한다.
- 이후 ‘expression blendshapes’를 model에 적용하기 위해 ‘deformation transfer’를 사용한다.
- 위와 같은 initial expression basis가 논문의 orthogonality와 expression realism 조건에 일치하지는 않지만, registration process의 bootstrapping에 유용하다.

## 4.2. Single-frame Registration

![Untitled 42](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c5f84f32-c2ec-47d6-98ea-633e7f138bc6)

- 논문의 mesh alignment에 사용하는 data는 **‘3D scan vertex’, ‘multi-view images**(2개는 D3DFACS, 3개는 논문의 sequences)’, **‘camera calibration’**을 포함한다.
    - carmera calibration: focal length, camera pose 등을 추정할 수 있는 정보
- 각 individual의 sequence를 align하기 위해, **‘personalized template’**과 2048 x 2048 pixels resolution을 갖는 **‘texture map’**을 계산한다. (Section 4.3에 자세히 나와있음)

“***FLAME model-based Face Scan Registration은 다음과 같은 3 Step으로 구성된다.”***

### Model-only

![Untitled 43](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/640fcefc-aade-4ffb-aec3-182db2cadb77)

- 먼저, Scan을 가장 잘 표현할 수 있는 **model coefficients {$\vec\beta,\vec\theta,\vec\psi$}를 예측**한다.
⇒ 위 **식(6)을 optimization**하여 예측
- 위 식(7)은 ‘data term’으로, ‘scan vertex($v_s$)’와 model surface의 ‘closest point’ 사이의 “scan-to-mesh distance”를 측정
(scan에서 mesh로 변환하는 과정에서 가장 비슷한 mesh를 찾기 위함)
- ‘weight($\lambda_D$)’는 data term의 영향력을 조절
- ‘Geman-McClure robust penalty function($\rho$)’는 scan내의 outlier에 대한 robustness를 위해 사용

![Untitled 44](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/fb9da0d1-3932-4d80-acd5-16e773bfb3b8)

---

***Figure 5***

![Untitled 45](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/53857653-6214-4cad-8503-9cf5be85debd)

- CMU Intraface landmark tracker를 통해 예측된 49개의 landmarks(left)와 같은 landmarks를 topology에 적용한 결과(right)

---

![Untitled 46](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f3c66d7a-4008-4eec-a54c-effc00dafefa)

![Untitled 47](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/1ee789fc-f3a1-4fcd-b93a-a365b8472483)

- $E_L$은 **‘landmark term’**이다.
⇒ ‘**image landmarks**(camera calibration을 통해 projected됨)’과 model template의 **‘corresponding(대응되는) vertex’** 사이의 **L2-norm** distance를 의미
- 논문은 “CMU Intraface”를 사용하여, 모든 multi-view camera images에서 49개의 landmarks를 자동적으로 예측한다.(Fig 5)
- 논문은 ‘template’에서 대응되는 49 landmarks는 수동으로 적용한다.
- ‘weight($\lambda_L$)’은 landmark term의 영향력

![Untitled 48](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/027e395d-7e05-42e4-84f4-611953a2c786)

- ‘prior term($E_P$)’은 위 식(8)과 같고, ‘pose coefficients($\vec\theta$)’, ‘shape coefficients($\vec\beta$)’, ‘expression coefficients($\vec\psi$)’를 regularization한다.
⇒ 각 coefficients의 제곱 값을 penalize하여 0에 가깝도록 만든다.

### Coupled

![Untitled 49](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7b0485b9-80d9-4fca-8a5e-ca3aa961baa7)

- 2번째로, model space를 벗어나서 optimization을 할 수 있도록 하고, 위 식(9)와 같다.
- ‘model parameters {$\vec\beta,\vec\theta,\vec\psi$}’와 ‘tamplate mesh의 vertex $T$’ 대하여 optimization한다.
⇒deformation이 가능해 진다.
- “Model-only Registration”과 다르게, $E_D$는 ‘scan’과 ‘aligned mesh($T$)’ 사이의 distance이다.
- ‘coupling term($E_C$)’는 $T$가 current statistical model과 유사하도록 한다.
⇒ $T$와 model $M(\vec\beta,\vec\theta,\vec\psi)$사이의 edge differences를 최소화하는 방향으로 학습
(아래 식(10)에서 볼 수 있다.

![Untitled 50](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c3552d6b-5a23-4d9c-904d-a6767a23f3d1)

- $T_e$와 $M(\vec\beta,\vec\theta,\vec\psi)_e$는 $T$와 $M(\vec\beta,\vec\theta,\vec\psi)$의 ‘edge(vertex와 vertex 사이의 선)’를 의미
- $\lambda_e$는 각 edge에 할당된 ‘individual weight’
- coupling은 ‘edge differences’를 사용하여 single points에 대한 neighbor points의 coupling influence를 분산시킨다.
- optimization은 $T$와 model parameter에 대해 동시에 수행된다.
⇒ 첫번째 단계에서 발생할 수 있는 model error를 recover하기 위해

![Untitled 51](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/225e80a1-2273-42ca-a0d8-d6558476370c)

- $T$의 각 vertex $v_k\in\R^3$에 대한 ‘Regularization term’은 “discrete Laplacian approximation”이다. 위 식(11)과 같다.

![Untitled 52](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/47db9fa3-efed-4ffe-b399-67d21c72bca8)

- U(v)는 위에 나와있는 값과 같고, $\mathcal{N}(v)$는 $v$의 ‘one-ring neighborhood’이다.
    - one-ring neighborhood: vertex를 기준으로 edge 하나로 이어져 있는 vertex들
- ‘Regularization term’은 registration에서의 ‘foldover’를 방지하여, noise와 occlustion에 robust하게 해준다.
    - foldover: mesh의 surface에서 발생하는 겹침이나 뒤틀림 등의 문제들
- 각 vertex에 대한 ‘weight $\lambda_k$’는 noisy scan region에 대한 regularization을 수행

### Texture-based

![Untitled 53](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/098bf4eb-fa2a-4537-9964-c683695336ec)

- 3번째로, ‘texture term($E_T$)’을 포함시킨다.
- $E_T$는 $T$의 모든 $V$시점에 대한 ‘real image($I$)’와 ‘rendered textured image($\hat{I}$)’사이의 “potomeric error”이다.(식은 아래 식(13)과 같다)

![Untitled 54](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/16db6702-40c8-45d5-b4be-091541bc9860)

- $||X||_F$는 $X$에 대한 ‘Fobenius norm’을 의미
    - Fobenius norm은 mxn matrix에 대해 아래 식과 같은 공식을 갖는다.
    
    ![Untitled 55](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/42430640-26ea-4d0c-aa54-bcea57317255)
    
- ‘Gaussian filters Ratio($Γ$)’는 real images와 rendered images 사이의 lighting changes에 대한 영향을 최소화하도록 도와준다. (real, rendered image 사이의 광도 변화)
- 또한, ‘photometric error’는 작은 displacements에 대해서만 의미를 갖기 때문에, 4개의 해상도에 대한 **multi-level pyramid**가 optimization동안 사용된다.
⇒ photometric error의 spatial extent를 증가 (small displacements도 잘 capture할 수 있을 것이라고 이해)
- $I_l^{(v)}$: ‘view($v$)’의 $l$ resolution level을 갖는 ‘image($I$)’를 의미

## 4.3. Sequential Registration

![Untitled 56](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/af2952f4-e0a8-4361-af58-beb14e9f455b)

- 논문의 temporal registration 방법은 database에서 각 subject마다 personalized template을 만드는 ‘personalization phase’를 사용한다.
⇒ facial performance를 tracking하는 동안 일정하게 유지된다.

### Personalization

![Untitled 57](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/16e24c63-923a-4c5f-a80c-a329d7ff7c7f)

![Untitled 58](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d5213d80-125d-4c30-a8a2-abc63296ac0f)

- 논문에서는 각 sequence가 neutral pose, neutral expression에서 시작된다고 가정한다.
- ‘personalization’동안, ‘coupled registration(식(9))’을 사용하고, 각 subject마다 ‘personalized template’을 취득하기 위해 여러 sequence에 걸쳐 ‘result $T_i$’들을 평균 낸다.
- 논문은 ‘personalized texture map’을 생성하기 위해 각 subject마다 $T$ 중 하나를 random하게 선택하였다.
⇒ ‘personalized texture map’은 “texture-based registration”을 위해 사용된다.
- 위와 같은 ‘personalization’은 registration의 안정성을 증가시키고, optimization의 성능을 향상시킨다(각 step에서 optimize되는 parameter 수를 크게 줄임).

### Sequence fitting

![Untitled 59](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/94c4df33-4031-445a-b16c-89e7fac97010)

- ‘Sequence fitting’ 동안, 논문은 $M$(식(1))의 ‘generic model template($\overline{T}$)’를 ‘personalized template’으로 바꾸고, $\vec\beta$를 0으로 고정한다.
- 각 frame마다, 이전 frame의 parameter로 model parameter를 initialize하고, ‘single-frame registration(Section 4.2.)’을 사용한다.
- register된 sequences가 주어졌을 때, 논문은 이전에 설명된 ‘new FLAME model’을 training하고, registration 절차를 반복한다.
- 3번의 iteration 후 registrations에 비해 improvement가 미미하기 때문에, 4번만 iteration을 하고 중단한다.

# 5. Data

![Untitled 60](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/322925a3-76f9-493d-a5e2-6665fc96c6a3)

- FLAME은 2개의 large dataset과 self-captured sequences로 학습되었다.

### Our capture setup

![Untitled 61](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/fd892440-dc77-477d-b1cc-ccfe06dc8119)

- ‘self-captured sequences’를 위해, multi-camera active stereo system을 사용
- Capture system은 ‘3쌍의 stereo camera’, ‘3개의 color camera’, ‘3개의 speckle projectors’, ‘3개의 백색광 LED panel’로 구성된다.
- ‘Capture system’은 60fps로 평균 45000개의 vertex가 있는 3D mesh를 생성한다.
- ‘color image’는 각 frame의 ‘UV texture map(3D→2D)’을 생성하기 위해 사용된다.
⇒ ‘UV texture map’은 “image-based facial landmarks”를 찾는 것에 사용된다.

### Training data

![Untitled 62](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c7660f65-7cb6-4268-8c4d-67fb97f31054)

- ‘identity shape parameter {$\overline{T},\mathcal{S}$}’는 “CAESAR” body scan database의 3800개의 registerd heads를 통해 학습된다.
- “CAESAR” database는 다양한 shape variation을 capture한 static full-body scan을 갖고 있다.
- CAESAR scans는 논문의 ‘head template’과 결합된 ‘full-body SMPL model’로 register된다.
    - 2 step registration approach
        1. initial model로 ‘model-only registration’을 통해 global shape을 initialize
        2. 위 global shape에 ‘coupled refinement’를 적용
- ‘shape parameter’들은 위 registrations로 학습된다.

![Untitled 63](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/949c16a6-e0f8-46d5-9fb7-990201c574ae)

- ‘pose parameters {$\mathcal{P,W,J}$}’의 training에는 head motion(i.e. neck, jaw motions)이 발생할 수 있는 모든 범위의 표현이 포함된 training data가 필요하다.
- CAESAR dataset뿐만 아니라, 기존 3D face databases에도 충분한 ‘head pose articulation’을 제공하지 않기 때문에, 논문은 10 subjects에 대한 neck, jaw motions를 직접 capture하였다.(Fig 6)
- jaw, mouth sequences는 Section4에 언급된 것처럼 register된다.
- head rotation sequecnes는 ‘coupled alignment’를 사용하여 register된다.
⇒ neck region의 vetex들만 model space를 벗어날 수 있다.
(다른 vertext들은 model space안에 유지된다.)
    
    ⇒ head가 돌아갈 때, 불가피하게 발생하는 large facial occlusions에 robust하도록 한다.
    

![Untitled 64](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5758ccb9-f53d-43ff-9c39-3658d08c275d)

- ‘Expression model($\mathcal{E}$)’는 2개의 training data를 사용한다.
    1. D3DFACS의 registrations
    2. self-captured sequences
- 모든 motion sequences는 Section4에 언급된 것과 같이 ‘registration approach’를 통해 자동으로 register된다.
⇒ 69000개의 registered frames를 만든다.
- 이러한 3D sequences에서, 이웃한 frames는 매우 유사하다.
⇒따라서 21000개의 registered frames만을 training에 사용

---

***Figure 6***

![Untitled 65](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c4a52f1e-c27e-4cd8-b309-5b3353895e73)

- Sample registrations
- Top: CAESAR body data로에 extract된 ‘shape data’
- Middle: 머리의 rotation과 mouth articulation이 있는 self captured pose data의 sample registration
- Bottom: D3DFACS와 self captured sequences의 expression data의 sample registration

---

# 6. Model Training

![Untitled 66](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/fc87283a-98ec-426e-904f-a35d030e1ef9)

- Registered dataset이 주어졌을 때(Figure 6의 top: identity, middle: pose, bottom: expression), **“FLAME” training의 목표는 ‘shape’, ‘pose’, ‘expression’ variations를 분리**하는 것이다.
⇒ **parameter $\Phi$={$\overline{T},\mathcal{S},\mathcal{P},\mathcal{E},\mathcal{W},\mathcal{J}$}을 계산**
- 위의 decoupling을 위해서, ‘pose parameters {$\mathcal{P},\mathcal{W},\mathcal{J}$}’, ‘expression parameters $\mathcal{E}$’, ‘shape parameters {$\overline{T},\mathcal{E}$}’은 하나씩 optimize된다.
⇒ training data의 reconstruction error를 최소화하는 ‘iterative optimization approach’ 사용
- 논문에서는 ‘gender specific models’를 사용한다.
⇒ female models($\Phi_f$), male models($\Phi_m$)

## 6.1. Pose parameter training

![Untitled 67](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/403cbed9-c53d-400f-b37f-e5aeef53af39)

- FLAME 모델에는 2가지 종류의 ‘pose parameter’가 있다.
    1. 각 subject($i\in${$1,...P_{subj}$})에 specific한 parameters
        1. personalized rest-pose templates $T^P_i$
        2. person specific joints $J^P_i$
    2. subject들에 모두 걸치는 parameter
        1. blend weights $\mathcal{W}$
        2. pose blendshapes $\mathcal{P}$
        3. joint regressor $\mathcal{J}$
        ⇒ ‘personalized rest-pose templates($T^P_i$)’의 모든 subjects들의 ‘person specific joints($J^P_i$)’를 regression하여 학습된다.

![Untitled 68](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/0b01ffaf-f04a-4e9e-a6e2-836beb68ac79)

![Untitled 69](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/bc7fb54b-f1e3-498b-a84a-bc5e9a7abea3)

- 위 parameter들의 optimization은 아래 과정을 번갈아가며 수행된다.
    - **각 $j$번째 registration의 ‘pose parameter $\vec\theta_j$’를 찾는 것**
    - **‘subject specific parameters {$T_i^P, J_i^P$}’의 optimization**
    - **‘global parameters {$\mathcal{W},\mathcal{P},\mathcal{J}$}’의 optimization**
- optimization이 진행되는 objective function은 다음과 같은 요소들로 구성된다.
    - **data term $E_D$(식(7))**: training data의 ‘squared Euclidean reconstruction error’를 minimize
        - squared Euclidean reconstruction error: $E_i = \sum_{j=1}^{d} (x_{i,j} - \hat{x}_{i,j})^2$
    - **regularization term $E_{\mathcal{P}}$:** pose blendshapes의 ‘Frobenius norm’을 minimize
        - Frobenius norm: $|| A ||_F = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} |a_{i,j}|^2}$ ($a_{i,j}$는 matrix의 index)
    - **regularization term $E_{\mathcal{W}}$:** blendweights의 initialization에서 large deviations를 penalize
- ‘regularizers {$E_{\mathcal{P}},E_{\mathcal{W}}$}’의 weight는 “training data”와 유사하게 만드는 것”과 “parameters generalization을 유지하는 것” 사이의 균형을 의미한다.
⇒ 위 regularizaer들은 FLAME이 training data에 overfitting되는 것을 방지하고, general하게 만든다.
- ‘joint regressors’, ‘pose, shape parameters’를 optimization하는 것에 사용되는 method와 objective는 “SMPL” body model에 더 자세히 설명되어 있다.
⇒ FLAME에 그대로 적용

![Untitled 70](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/cba373b9-9695-4c20-835c-0dfe463e925a)

- ‘subject specific template $T_i^P$’가 없을 때, pose space를 학습하는 동안 ‘pose coefficients($\vec\theta$)’의 initial estimation은 ‘initial average template’을 통해 수행된다.
    
    ⇒ shape의 large variations에 대해 robust하기 위해, template과 각 registration 사이의 ‘edge difference’를 minimize하도록 training
    

![Untitled 71](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/725a3293-678e-4473-8736-2e5acbac9327)

- $T_i^P, J_i^P$가 강한 ‘facial expression’에 영향을 받는 것을 피하기 위해, $T_i^P,J_i^P$를 찾는 동안 ‘expression effects’를 제외한다.
    - 각 registration마다 ‘pose($\vec\theta$)’, ‘expression parameters($\vec\psi$)’를 동시에 solving
    - $B_E$(expression blendshapes)를 제외
    - 이 후 $T_i^P, J_i^P$를 solving

## 6.2. Expression parameter training

![Untitled 72](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/8a664219-0e26-461c-a9a1-8d5259b3c75c)

- ‘Expression space $\mathcal{E}$’를 training하는 것은 ‘pose variations’와 ‘shape variations’로부터 분리되 expression이 필요하다.
- 이는 먼저 각 registration마다 ‘pose parameters $\vec\theta_j$’를 구하고, $M(\vec0,\vec\theta,\vec0)$이 되는 inverse transformation을 적용하여 pose의 영향력을 없앰으로써 얻을 수 있다.
⇒ 논문은 이 단계를 “unposing”이라고 하고, registration을 unposing하여 나온 vertex들을 ‘$V_j^U$’라고 한다.
- neutral(중립의, 표정이 없는) expression으로부터 expression variation을 modeling하기 위해, neutral expression을 정의하는 registration이 각 subject마다 주어졌다고 가정한다.
- $V_i^{NE}$는 ‘subject $i$’의 unpose된 neutral expression의 vertex들이다.
- shape variations으로부터 expression variation을 분리하기 위해, 각 ‘registration $j$’마다 ‘expression residual $V_j^U-V_{s(j)}^{NE}$’를 계산한다. ($s(j)$는 index j의 subject)
- 이후 PCA를 위 ‘expression residuals’에 적용함으로써 ‘Expression space $\mathcal{E}$’를 계산한다.

## 6.3. Shape parameter training

![Untitled 73](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/67b2dcd3-ba11-465d-b343-e1634846a315)

- ‘shape parameter training’은 ‘template $\overline{T}$’와 ‘shape blendshapes $\mathcal{S}$’를 계산하는 방식으로 구성된다. ⇒ shape dataset의 registration을 위해
- 이전과 비슷하게, ‘pose, expression effects’는 training data에서 제거된다.
⇒ pose, expression, shape을 decoupling하기 위해서
- 그 후 ‘template $\overline{T}$’은 ‘expression-normalized registrations’와 ‘pose-normalized registrations’의 평균값으로 계산된다.
- ‘shape blendshapes $\mathcal{S}$’는 PCA로 계산된 첫번째 $|\vec\beta|$ principal components를 통해 구성된다.

![Untitled 74](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/15dac89f-0037-4c53-9196-f6dc3c84d752)

- FLAME model을 training하는 것은 반복적으로 ‘pose’, ‘expression’, ‘shape’ parameter들을 각각 단독 optimization하여 수행된다.
⇒ optimize되지 않는 다른 parameter들은 고정된 채로
- expression space formulation의 높은 capacity와 flexibility 때문에, ‘pose blendshapes’는 ‘expression parameters’ training이전에 training되어야 한다.
⇒ expression overfitting을 방지하기 위함

![Untitled 75](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/60345753-a227-4fdc-9070-b5a6b55ba216)