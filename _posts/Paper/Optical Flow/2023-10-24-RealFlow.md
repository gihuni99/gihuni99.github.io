---
title: RealFlow:EM-based Realistic Optical Flow Dataset Generation from Videos 논문 리뷰
date: 2023-10-24 00:00:00 +09:00
categories: [Paper, Optical Flow]
tags:
  [
    Paper,
    Optical Flow,
    Data Synthesis
  ]
pin: true
---

# Abstract

![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b0f87f05-1d9d-4570-800b-2372c8c0c34b)

- Video에서 Ground Truth label을 얻는 것은 challenge하다.(pixel-wise label을 수작업으로 annotation하는 것은 힘들기 때문)
- 게다가, synthetic dataset으로 학습된 모델을 real-world에 적용하는 기존 방법은 domain차이로 인해 필연적으로 성능이 떨어질 수 밖에 없다.
- 이를 해결하기 위한 방법을 **RealFlow**에서 제안한다.
→Expectation-Maximization based Model로 unlabeled realistic video에서 large-scale optical flow dataset을 만들 수 있다.
- 구체적으로, video frame쌍의 optical flow를 예측하고, 이렇게 예측된 optical flow를 통해 새로운 image를 synthesize한다.
→ 즉, 새로운 image쌍과 그들의 corresponding flows는 새로운 training set이 되는 것
- 논문은 **Realistic Image Pair Rendering(RIPR) module**을 제안한다.
→ **softmax splatting**과 **bi-directional hole filling**을 사용하여 합성 이미지의 artifacts를 완화
(**artifacts**는 노이즈, 왜곡 등의 오류를 의미)

![Untitled 1](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/791396b0-39ab-431f-82fa-b0ae13786ed0)

(위 softmax splatting의 식이다. 가중치에 지수함수를 적용하여 더 나은 성능을 보인다. 자세한 내용은 이후에 알아보자)

- E-step에서, RIPR은 많은 양의 training data를 만들기 위해 새로운 이미지들을 생성한다.
- M-step에서, 논문은 만들어진 training data로 optical flow network학습에 사용한다.
→다음 E-step에서 optical flow를 예측하는 것에 사용할 수 있도록
- learning step을 진행하면서 flow network의 성능은 점점 향상되고, synthesized dataset의 질과 flow의 정확도가 향상된다.

# 1. Introduction

- **deep optical flow method**는 large-scale datasets를 training에 사용한다.
→ 좋은 계산적 효율성과 public benchmarks에서 가장 좋은 결과를 얻을 수 있음
- optical flow method의 중요한 요소 중 하나는 training dataset이다.
- Deep learning algorithms 성공에 중요한 영향을 미치는 Flow dataset의 4가지 특성은 다음과 같다.
**[Label Criteria]**
1) The quantity of labeled pairs (labeled data의 양
2) The quality of flow labels (flow label의 질)
**[Realism Criteria]**
3) The image realism (image가 실제와 유사한가)
4) The motion realism (motion이 실제와 유사한가)
- 하지만, 현재 존재하는 flow dataset중에서 모든 요소를 만족하는 dataset은 없었다.
→ Ex 1) FlyingThing, Sintel, AutoFlow등은 정확한 flow label과 함께 많은 양의 training data를 만들어 label criteria는 만족하지만, realism criteria를 만족하지 못한다.
(위 dataset으로 학습한다면 domain gap으로 인해 성능이 저하됨)

→ Ex 2) realism을 위해 실제 video의 flow label을 직접 annotation하는 것을 제안하는 방법도 있지만 비용 소모가 크다. (충분한 양과 질의 데이터를 만들 수 없음)

→ Ex 3) Single image로 training pair를 만드는 방법이 제안되었다. random하게 transformation을 생성하여 flow label로 사용하고, 이를 기반으로 warping하여 다른 이미지를 생성하여 image data상을 만든다. 이 방법으로 label criteria의 조건은 만족하지만, realism criteria는 일부만 만족한다. (synthesized image는 artifact를 포함하고, 생성된 motion은 실제 물체의 motion과 비슷할 수 없기 때문)

---

- 이러한 문제를 해결하기 위해 **RealFlow**를 제안
→ Iterative Learning Framework를 통해 realistic video frame으로 부터 training pair를 생성하고, 동시에 생성된 flow data를 통해 향상된 flow network를 얻을 수 있다. (lablel, realism criteria를 모두 만족)
    
![Untitled 2](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/83abcfa5-5ff2-4cc0-bf17-2e3f28f436da)
    

위 Fig 1을 통해 간단하게 이해한 내용을 정리하자면, 원래 Background와 Foreground image pair에 synthetic motion을 통해 생성한 pair data를 사용하던 방법을 Video에서 2개의 frame을 통해 Flow data를 예측하여 생성하고, 이를 통해 새로운 New Image2를 만들어 학습시키면서 계속해서 Flow Estimation의 성능을 향상시킨다. 
(확실히 의자가 저렇게 motion을 갖는 경우가 거의 없으니 좋은 방법이지만 다소 아쉬운 method인 것 같은데, RealFlow에서 해당 부분을 상당히 좋은 방향으로 개선한 것 같음)

→ I1, I2 frame(실제 video의 frame) 사이의 optical flow F를 예측하고, I’2를 synthesize하는 것에 이용한다.(I2는 이용하지 않음) 따라서 (I1, I’2, F)의 flow labeled data를 사용하는 것이다. I’2는 F를 기반으로 warping되기 때문에 정확하다.

- 위와 같은 방법은 2가지의 challenge가 있다.

1) image synthesis가 artifact를 유발할수도 있다.(ex. diparity occlusions)
→ RIPR method를 사용하여 확실하게 I’2 image를 만들 수 있다. (image I1, I2를 통해 예측된 flow와 depth map을 기반으로 softmax splatting과 bi-directional hole filling을 사용하여 artifact 완화에 효과적임)

2) motion realism이 flow예측에 따라 영향을 받는다.
→ **Expectation-Maximization(EM) based learning framework**를 사용한다.

![Untitled 3](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7976a530-68b9-437c-8827-8869ab229ff9)

위는 EM based learning framework의 전체적인 과정을 보여준다.

E-step동안 RIPR은 training sample을 만들기 위한 새로운 image들을 생성하고, 
M-step동안 다음 E-step에서 optical flow생성을 위해 사용할 **optical flow network**를 학습시키기 위한 data를 생성한다.

⇒ 위의 iterative한 특성을 활용하여 network의 성능을 향상시키고, 많은 video를 이용하여 supervised optical flow networks를 어떤 장면에 대해서든 generalization할 수 있다.

### 논문의 기여 요약

![Untitled 4](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/2b357bbd-af44-497a-9819-6be663899de8)

# 2. Related Work

(Optical Flow에 대해 더 자세히 알아보고 싶어서 Related work도 하나씩 살펴보았다.)

![Untitled 5](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/cbb466dd-5db8-4d5c-ae0f-2075cba15541)

- FlowNet은 synthetic dataset으로 학습된 Convolutional network를 통해 optical flow를 예측하는 첫번째 연구이다. 해당 연구에 따르면, 이전의 연구들은 향상된 module과 architecture로 flow accuracy를 향상시켰지만, 최근 연구들은 recurrent framework에서 graph, attention-based global motion refinement을 제안한다. (supervised learning에서 큰 향상을 보임)
- synthetic dataset과 realistic dataset의 domain gap을 무시할 수 없고, 성능 저하가 불가피하다.

⇒ RealFlow는 realistic video를 통해 dataset을 만들어 이 문제를 해결했다.

![Untitled 6](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/30673abb-dd9e-4c44-9b99-a47c214413ae)

- Unsupervised method의 장점은 annotation이 필요없다는 점이다. 기존의 연구들은 경쟁력있는 결과를 얻기 위해, 여러개의 unsupervised loss들과 image alignment constraint를 사용한다.
- 하지만, unsupervised method는 occlusion, lack of texture, illumination variation 등 많은 한계가 있고, 이 문제들은 brightness constancy assumption원칙에 맞지 않는다.
→ supervised learning이 더 좋은 성능을 보임
- 

![Untitled 7](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7b29d688-3718-4b39-bf63-7178f525c937)

![Untitled 8](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c84429c7-55af-44fa-bb5e-484b3e673cc7)

- 여러 dataset들이 real-world dataset을 취득하기 위한 방법을 사용했지만, 충분한 양의 데이터를 얻기 쉽지 않다.

**[이를 해결하기 위한 시도들(synthetic)]**

⇒ Flyingchairs dataset은 처음으로 synthesized data를 사용

⇒ Flyingthings는 양을 더 증가

⇒ Virtual KITTI는 Unity game engine을 사용하여  large driving dataset을 생성

⇒ AutoFlow는 training data를 만들기 위한 hyperparameter를 찾는 방법을 learning 기법을 제시

**[이를 해결하기 위한 시도들(synthetic→real-world로 가기 위한 꾸준한 변화)]**

⇒SlowFlow는 high-speed video camera를 사용하여 large-scale dataset취득을 시도했지만, flow labels들에 대한 신뢰성이 없다.

- 위와 같은 문제를 해결하기 위한 노력이 **Depthstillation**이다. 
→single real image를 통해 새로운 image를 synthesis하는 것
- motion label은 fore, background를 위한 sampling된 parametric transformation이다.
→ 하지만 sampling된 motion은 실제가 아니며, 이러한 synthesis는 때때로 artifact를 발생시킨다.
- 반면, 논문은 real video에서 실뢰할 수 있는 flow label을 취득하고, 하나의 image 대신 2개의 frame을 통해 새로운 이미지를 synthesis한다.

# 3. Method

## 3.1 RealFlow Framework

![Untitled 9](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/e0de3670-7c78-449c-8c02-c5b5f36cf1a3)

- RealFlow에서 제안하는 pipline은 Fig2(EM based)와 같다. real-world video를 고려할 때, 논문의 목표는 **large-scale training dataset**을 만들고, 동시에 **optical flow estimation network**를 학습시키는 것이다.
→ 논문에서의 key idea는 **더 나은 training dataset이 optical flow network를 더 좋게 학습시키고, 반대로 더 좋은 optical flow network는 dataset 생성을 위한 더 나은 optical prediction을 제공한다는 것이다.**
- 따라서 RealFlow는 data generation과정과 optical flow network training과정을 통합시켜 generative model로 만들었다.
→ Expectation-Maximization(EM) algorithm을 통해 반복적으로 optimized될 수 있음

![Untitled 10](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5d08e603-1811-42d5-88f1-238b949ce95f)

![Untitled 11](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/4a6fcb6e-1af8-4bab-a0ff-d6fd83796217)

![Untitled 12](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b8eb5652-e010-4d3e-9144-5de13b6173d7)

- Figure 2에서 E-step과 M-step을 각각 볼 수 있다.
- iteration $t$에서, 먼저 training dataset $X^t=$ {$x^t$}를 생성하기 위해 E-step을 실행한다.
→ Input video로부터 sampling된 연속적인 image pair $(I_1, I_2)$를 고려하여, training data generation과정은 식(1)과 같이 나타낼 수 있다.
⇒ $Θ^{t-1}$는 learning된 optical flow network $Θ$의 이전 iteration t-1
⇒ $x^t$는 생성된 training sample
⇒ $R$은 RIPR module을 표현한 것이다.

![Untitled 13](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b2105d3d-38f3-436e-bc61-881c4be95114)

- M-step에서 새롭게 생성된 dataset $X^t$를  training에 사용하고, supervised learning방식으로 optical flow estimation network를 update한다. 해당 과정은 식(2)와 같다.
⇒ $L$은 optical flow network의 learning objective

- 위 과정을 통해, **EM iteration을 사용한 RealFlow를 통해 optical flow dataset과 높은 정확도의 optical flow network를 얻을 수 있다.**

![Untitled 14](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/baff2a15-d430-4273-8d8f-973f99573971)

fig3를 보고 RIPR module의 간단한 동작원리를 살펴보자. 

real-world videos의 2개의 frame $I_1, I_2$와 이를 통해 예측한 flow field $F_{1->2}, F_{2->1}$가 주어졌을 때, monocular depth network를 통해 depth $D_1, D_2$를 구한다. 

그 후 flow map을 $F'_{1->2}와 F'_{2->1}$으로 수정하고(수정하는 방법은 아래 method에 자세히 나와있을 것 같다), $F'_{1->2}$를 hole mask M을 check하는 것에 사용한다. 

최종적으로 splatting method는 새로운 view인 $I^s_1과 I^s_2$를 생성하는 것에 사용된다. ( $I^s_1과 I^s_2$은 새로운 이미지 $I'_2$를 만들기 위해 융합된다. $(I_1,I'_2,F)$가 새롭게 생성된 training pair가 된다.

(여기서 드는 의문은 monocular depth estimation이 과연 신뢰할만 할까?)

## 3.2 Realistic Image Pair Rendering

![Untitled 15](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/3345b315-9ccb-46e3-8971-1023c1a4f55c)

- RIPR method의 pipeline은 figure 3에서 확인할 수 있다. 연속적인 image pair $(I_1,I_2)$와 optical flow estimation network $Θ$를 고려할 때, 논문의 목표는 network training을 위한 flow labeled image pair를 생성하는 것이다.

**[Main Idea]**

- image pair $(I_1,I_2)$를 기반으로 새로운 image $I'_2$를 생성하고, $(I_1,I_2)$사이의 flow F를 예측한다.
→ F가 새로운 image pair $(I_1,I'_2)$의 training label로 사용될 수 있음
- 구체적으로, 
1) reference image $I_1$은 먼저 target view $I'_2$로 forward-warping된다. 
2) synthesized view $I'_2$의 realism을 보장하기 위해, dynamic moving objects와 depth disparities로 인해 생기는 occlusions와 holes를 제거한다.
(occlusions: object의 일부가 가려지거나 보이지 않는 것)

![Untitled 16](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/54b8cb17-7a64-43f0-8f0b-f0a9f2c8d280)

![Untitled 17](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/2ccca3c2-32e6-4a92-9612-ba783e83aacf)

- 논문은 monocular depth network를 통해 occlusion region을 위한 foreground와 background 구별을  **Splatting** method를 사용하여 한다.
- 또한 Backward flow와 $I_2$를 사용하여 hole region을 채우는 **Bi-directional Hole Filling(BHF)**를 고안했다.
- 마지막으로, target view가 생성되면, 새로운 training pair $(I_1,I'_2,F)$가 사용된다.
    
![Untitled 18](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/51d76d75-4b34-4554-9afa-dfefe301a981)
    

### **[Fig 3의 detail]**

![Untitled 19](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/09a04c59-59d4-4630-bc0f-6944f91615a6)

- 먼저 **forward flow**, **backward flow**, **depth of $I_1,I_2$**를 예측한다. 각 예측치를 구하는 수식은 식(3), (4)와 같다.
⇒$F_{1->2}$: Forward Flow
⇒$F_{2->1}$: Backward Flow
⇒ $D_1, D_2$: monocular depth network를 통해 예측된 depth ($D_1, D_2$는 inverse depth map이므로 camera에 가까울수록 pixel값이 크다→더 정확한 depth추정 가능)
⇒ $Ψ$: monocular depth network

![Untitled 20](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c3b4661b-f16e-4441-a866-1cf1e63f8bc7)

![Untitled 21](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d9758bed-9200-4e49-8f56-f1b834b562e8)

- 생성된 dataset의 다양성을 증가시키기 위해, 논문은 $α$ factor를 사용하였다.
→$α$ factor는 flow estimation을 방해하여, 생성된 view가 original $I_2$와 완전히 똑같지 않게 하고, view generation을 조절할 수 있다. 따라서 식(5)와 같은 flow filed($F'_{1->2},F'_{2->1}$)를 획득할 수 있다.
- flow field $F'_{1->2},F'_{2->1}$를 사용하여 splatting method를 통해 새로운 view를 생성하고, 그 수식은 식(6)과 같다. → 즉, original image, new flow field, depth를 통해 새로운 view를 생성
⇒ $S$: splatting method
⇒ $I^s_1,I^s_2$: 서로 다른 direction(fore,backward flow)으로 생성된 같은 view
(여기서 occlusion은 splatting operation 이후에 해결됨, 나중에 설명이 나온다)
- 마지막으로, 최종적인 view는 BHF method를 통해 만들어지며, 수식은 식(7)과 같다.
⇒ $B$: BHF method
⇒ $I'_2$: 새로운 image(view)
⇒ $(I_1,I'_2,F'_{1->2})$: RIPR module을 통해 생성된 training pair

### [Splatting]

(splatting은 3D scene data를 2D image로 변환할 때 사용)

![Untitled 22](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/99ee126f-38a2-4df1-b2dc-4578dc87e8ff)

- 주어진 flow field $F'_{1->2}$에 따르면, **Splatting**은 reference image $I_1$을 new view $I_s$로 Forward-Warping하는 것에 사용할 수 있다.
- Fig 4를 보면, conventional sum operation을 splatting에 사용하면 적절하지 않은 밝기값을 만들어 낸다.
- **softmax splatting method**는 이 문제를 완화시켜준다. operation은 식(8), (9), (10)과 같다.
⇒ $q$: $I_1$의 coordinate
⇒ $p$: target view($I'_2$)의 coordinate
- 위 식의 의미에 대해서 분석해보자면, $(q+F'_{1->2}(q))$는 flow field를 통해 이동된 좌표이므로 $u$는 target view와 예측된 flow field를 통해 이동된 좌표의 차이를 의미한다.
⇒$b(u)$: bilinear kernel
⇒$D_1$: $I_1$의 depth map
⇒$I_s$: Forward warping의 결과
- 식(10)을 적용함으로써, target view에서 occlusion이 발생한 background pixels가 compressed될 수 있다→ depth map과 softmax operation을 사용하여
- 하지만, 식(10)의 Softmax splatting은 여전히 occlution region의 unnatural을 야기할 수 있다.
→ 따라서 splatting method에서 대체 가능한 option인 max splatting을 제안한다.

![Untitled 23](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/e3c8e35a-87f6-4331-bc94-69696fbc93f4)

- $k$는 nearest kernel이다.
- 식(12)는 여러 pixel들이 $p$에 위치해있다면, 가장 큰 depth값을 갖는 pixel을 target view에 할당하는 과정이다.
→ 하지만 논문은 softmax splatting 방식이 더 좋은 성능을 보이는 것을 실험적으로 발견함

### [Bi-directional Hole Filling(BHF)]

![Untitled 24](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/bfcd87d9-12ba-47d5-bee0-907a80811d4d)

- occlusion외의 다른 문제는, original image에서 pixel이 project되지 않아 생기는 **hole**이 있다.
- 이전의 연구는 inpainting model을 사용하여 hole문제를 해결하고자 했지만, 이는 artifacts를 유발하여 생성된 dataset의 질을 떨어뜨릴 수 있다.
⇒ **BHF** method를 제안

![Untitled 25](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/e31244a0-b0b7-477e-9e2c-916d1024f4a1)

위 식(7)은 BHF method를 수식으로 나타낸 것이다.

- 먼저 **range map chek method**를 사용하여, $F'_{1->2}$로부터 hole mask $M$을 check한다. 해당 과정은 식(13)과 같다.

![Untitled 26](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/de7c4f24-d499-4830-a2e3-a22833fec9af)

- 위 과정에서 구한 hole mask $M$은 hole pixel은 0, hole이 아닌 pixel은 1로 labeling된다.
- 따라서 식(14)를 통해, $I^s_1$과 $I^s_2$를 융합하여 새롭게 생성된 image $I'_2$를 도출할 수 있다.
⇒ 식(14)의 과정은 $I^s_1$의 hole region이 $I^s_2$를 통해 채워진다는 것을 의미

# Result

![Untitled 27](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/786ab293-e52f-42d4-9cca-d5bb91cad478)

![Untitled 28](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b4fb9caa-c339-4fbc-8003-249227782136)