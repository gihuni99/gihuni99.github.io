---
title: LDF 논문 리뷰
date: 2023-03-13 00:00:00 +09:00
categories: [Paper, Salient Object Detection]
tags:
  [
    Computer Vision,
    Paper,
    Salient Object Detection
  ]
pin: true
---

Label Decoupling Framework for Salient Object Detection


![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/734f0c32-9b61-42a1-ab80-42a385a86e39)

- 더 정확한 saliency map을 얻기 위해서, 최근 방법들은 fully-convolutional network(FCN)을 통해 multi-level feature들을 종합하고, auxiliary(보조의) supervision을 위한 edge정보를 도입한다. 주목할만한 발전을 얻었지만, 논문의 저자들은 pixel이 edge에 가까우면 가까울수록, 예측되기 어렵다는 것을 발견했다.
⇒ edge pixel들이 매우 불균형한 분포를 갖고 있기 때문
- 이러한 문제를 해결하기 위해, 논문은 Label Decoupling Framework(LDF)를 제안한다.
⇒ label decoupling(LD) 단계와 feature interaction network(FIN)로 이루어짐.
- LD(label decoupling)는 original saliency map을 명확하게 body map과 detail map으로 분해한다.
⇒body map은 object의 중앙 지역에 집중하고, detail map은 edge주변의 지역에 집중한다.
- Detail map은 기존의 edge supervision보다 더 많은 pixel들을 포함하기 때문에 더 잘 작동한다.
- Saliency map과 다르게, body map은 edge pixel들을 버리고, center area에만 집중한다.
⇒이를 통해 training하는 동안 edge pixel로 인해 방해받는 것을 성공적으로 피할 수 있었다.
- 그러므로, 논문은 FIN(Feature Interaction Network)에서 body map과 detail map을 각각 다루기 위해서 2개의 branch를 사용했다.
- Feature Interaction(FI)는 saliency map을 예측하기 위해 2개의 상호 보완적인 branch를 융합하고, 다시 2개의 branch로 만드는 것에 사용하기 위해 디자인되었다.
- 이러한 반복적인 refinement(정제)는 더 나은 representation을 학습하고, 더 정확한 saliency map에 도움이 된다.
- 6개의 benchmark dataset에 대한 종합적인 실험들은 LDF의 state-of-the-art를 증명한다.

![Untitled 1](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/67f9a004-d19a-4792-81b1-d2843460462f)

- 먼저 label decounpling method에 대해 설명하고, saliency map을 body map과 detail map으로 분해하는 specific step을 보여줄 것이다.
- 그 후, Feature들 간의 상보성(상호 보완적 상태)의 이점을 가져오기 위해, 논문은 branch사이의 반복적인 정보 교환이 가능하게 하는 FIN(feature interaction network)을 소개
⇒ 아래가 대략적인 model을 나타내는 그림

![Untitled 2](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/0552a919-4a48-4c84-acc0-9283badc9b82)

![Untitled 3](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/0fc35fea-37b5-43fd-ba9f-60be3632cf83)

![Untitled 4](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/25b2734b-1006-4cab-ad03-1dcad89ecf4e)

- pixel 예측의 어려움은 pixel의 위치와 관련이 있다.
- 어수선한 background 때문에, edge 근처의 pixel들은 잘못 예측되기 더 쉽다.
- 이에 비해, central pixel은 더 높은 예측 정확도를 가지고 있다.
⇒ salient target의 internal consistency(내부 일관성)으로 인해
- 이 pixel들(central pixels, edge pixels)을 동일하게 처리하는 것보다, 그들 각각의 특성에 따라 다르게 다루는 것이 더 타당하다.
- 따라서 논문은 original label을 body label과 detail label로 decouple(분리시키다)하는 것을 제안한다.(Fig 3에서 볼 수 있다.)

![Untitled 5](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/cc1f2850-222f-4d2f-ba0b-dc1c14aceb6e)

- 이 목표를 달성하기 위해, 논문은 Distance Transformation(DT)를 도입
⇒DT는 전통적인 image processing algorithm(decouple the original label을 위해)
- DT는 binary image를 foreground(전경, 혹은 중요한 image)pixel이 background로부터 distance function에 의해 최소 distance를 가지는 새로운 image로 변환할 수 있다.
- 구체적으로, DT의 input은 binary image “I”이다.
⇒I는 Ifg(foreground)와 Ibg(backgroung)로 나뉠 수 있다.
- 각 pixel ‘p’에 대하여 I(p)는 그 픽셀에 해당하는 값이다.
    
![Untitled 6](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7abcba50-f07e-4835-ad70-107819f2b9b0)
    

⇒ p가  foreground의 pixel이면 I(p)는 1, background의 pixel이면 I(p)는 0.

- image I로부터 DT result를 얻기 위해, 논문은 metric function을 선언

![Untitled 7](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/730ab1f3-3333-419b-8a03-591f67ae3924)

⇒ pixel끼리의 distance를 측정하기 위한 식

- 만약 pixel p가 foreground에 속해 있다면, DT는 가장 먼저 background에 있는 가장 가까운 pixel q를 찾을 것이고, p와 q사이의 거리를 구하기 위해 f(p,q)를 사용할 것이다.
- 만약 pixel p가 background에 속해 있다면, 가장 작은 거리는 zero일 것이다.

![Untitled 8](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c4097837-ff86-4595-9c7b-4269532ce3d8)

⇒ 논문은 f(p,q)를 새롭게 생성되는 image의 pixel로 사용한다.
     (위 식은 distance transformation)

- distance transformation 이후, original image I는 I’으로 바뀐다.
⇒ I’(p)는 더이상 0 또는 1이 아니다.
- 논문은 I’의 pixel value를 linear function을 사용하여 normalization하였다.
    
![Untitled 9](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/569f6237-da2e-487d-9bd3-e6dbcdf15d3e)
    

⇒위 식을 통해 원래의 값을 [0,1]로 mapping

- 모든 pixel들을 동일하게 다루는 original image I와 비교해서, I’의 pixel value는 pixel이 foreground에 속해있는지, background에 속해있는지에 대해서만 결정되는 것이 아니라, pixel의 relative position에도 관련이 있다.
⇒ object의 중앙에 위치한 pixel들은 largest value를 가지고, 중앙으로부터 멀거나 background에 있는 pixel들은 smallest value를 갖는다.
- 따라서 I’은 original image의 body part를 의미한다.
⇒비교적 쉬운 central pixel들에 주로 집중한다.(논문은 이것을 body label로 사용)
- 그에 상응하여, original image I에서 body image I’을 지움으로써, detail image를 얻을 수 있다.
⇒이는 detail label로 취급되고, 주로 main region으로부터 떨어진 pixel에 집중한다.
- 추가적으로, 논문은 새롭게 만들어진 label들과 original binary image I를 곱한다.
⇒background interference(배경의 간섭)을 지우기 없애기 위해 (식은 아래와 같다.)
    
![Untitled 10](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/79647522-f8c0-408c-a3c1-3372807d7750)
    

(BL: body label, DL: detail label)

⇒이로써 original label은 서로 다른 종류의 supervision으로 분해되어, 각각 서로 다른 특성을 갖는 body feature와 detail feature 모두를 network가 학습하도록 한다.

![Untitled 11](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/075129cc-acc6-438a-8bd8-0b5f05eca066)

![Untitled 12](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5cf587f1-60bf-4b85-ae60-e0f186abf63d)

- Backbone으로 ResNet-50사용
- 구체적으로, 논문은 fully connected layer(Fc layer)를 없애고, 모든 convolutional block들을 유지했다.
- H x W의 input image가 주어지면, backbone은 downsampling을 위해 stride 2로 spatial resolution을 줄인 5가지 scale의 feature들을 만든다.
(논문은 이 feature들을 F={Fi|i=1,2,3,4,5}로 정의한다)
- i번째 feature의 size는 (W/2^i) x (H/2^i) x Ci이다.
(Ci는 i번째 feature의 channel)
- low-level feature는 computation cost가 증가하고, 하지만 제한된 performance 향상만을 가져온다.
- 따라서 논문은 Fi(i=2,3,4,5)의 feature들만 사용한다. (Fig 2에서 볼 수 있다)
- 2개의 convolution layers가 이 feature들에 적용된다.
⇒body prediction task와 detail prediction task에 각각 적용될 수 있도록
- 이 후 2개의 group의 feature를 얻게 된다.
B={Bi|i=2,3,4,5}, D={Di|i=2,3,4,5}
⇒ 64개의 channel로 squeezed(압축)되고 saliency map generation을 위해 decoder network로 보내진다.

![Untitled 13](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c995dfe2-e6eb-40c8-96cd-22fe1920d81a)

![Untitled 14](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/6df4db99-1607-4aaf-ab4a-03cf00add3aa)

- Featuer interaction network(FIN)은 label decoupling에 적응하기 위해 만들어졌다.(Fig 2)
- label decoupling에서, saliency label이 body map과 detail map으로 변환된다.
⇒ 둘 다 model learning을 위한 supervision으로 사용된다.
- FIN은 two-branch structure로 디자인되어있다. (각각 하나의 label을 책임지고 있다)
- body map과 detail map 모두 같은 saliency label에서 파생되었기 때문에, 두 branch들의 feature들 사이에 일정 수준의 유사성과 상호보완성이 있다.
⇒ 논문은 정보 교환을 위해 complementary branch사이의 feature interaction을 도입하였다.
- 전반적 보았을 때, 제안된 framework는 하나의 backbone encoder network, 하나의 interaction encoder network, 하나의 body decoder network, 그리고 하나의 detail decoder network로 구성되었다.
- ResNet-50은 multi-level feature B={Bi|i=2,3,4,5}와 D={Di|i=2,3,4,5}를 extract하기 위해 backbone network로 사용되었다.
- feature B에 대하여, body decoder network가 body map을 만들기 위해 사용되었고, 
feature D에 대하여 detail decoder network가 detail map을 만들기 위해 사용되었다.
- 이 두 branch의 output feature를 모두 얻은 후, 가장 간단한 방법은 이 features를 concat하고 final saliency map을 얻기 위해 convolutional layer를 적용한다.
- branches 사이의 정보 교환을 명확하게 촉진하기 위해서, interaction encoder network가 도입되었다.

![Untitled 15](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/1b304732-84d3-4dda-8b97-0962c73f9359)

- 더 구체적으로, intercation decoder는 body decoder와 detail decoder의 features를 concate한 것을 input으로 갖는다. interaction decoder는 multi-level feature들을 얻기 위해 multiple convolution의 stack으로 이루어져 있다.
- 이 multi-level feature들은 body decoder와 detail decoder에 각각 적절하도록 만들기 위해 3x3 convolution layer를 적용한다.
- dircet addition은 backbone encoder과 interaction feature를 융합하는 것에 이용
⇒더 정확한 saliency map을 만들 수 있다.
- 외견 상으로, 마지막 branch의 output이 이전 decoder에 사용되기 때문에 전체적인 network는 흔하지 않다.
- 하지만, 사실, feature interaction은 많은 iteration으로 이루어져 있다.
- 첫번째 iteration에서, 2개의 branches는 exchanging information없이 feature를 내보낸다.
- 두번째 iteration부터, 두 branch 사이의 interaction이 포함된다.

![Untitled 16](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/ed2625ad-b840-4cfe-8187-a10c658ed038)

![Untitled 17](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/e8da6a67-9a98-47c8-a1ca-df34e3abf434)

- 논문의 training loss는 모든 iteration의 output의 summation으로 정의된다. (아래 식과 같다)
    
![Untitled 18](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/9870f948-18ea-4351-8b1a-62da91032677)
    

(l(k)는 k번째 iteration의 loss, K는 iteration 총 횟수, ak는 각 iteration의 weight)

(⇒ 문제를 단순화하기 위해, ak=1로 설정→모든 iteration을 동일하게 취급)

- 각 iteration에 대해, 우리는 3개의 output을 얻을 것이다.
(body, detail, and segmentation)
⇒각각은 하나의 loss에 상응한다.
- 따라서 l(k)는 다음과 같이 3개의 loss 합으로 이루어 진다.
    
![Untitled 19](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/40ad52f3-fe18-470b-bcb1-4efde80b8538)
    
- 각 loss는 body lss, detail loss, segmentation loss로 부른다.
- 논문에서는 바로 body loss와 detail loss를 계산하는 것에 binary cross entropy(BCE)를 사용하였다. BCE는 binart classifiction과 segmentation에 널리 쓰이는 loss이다.
    
![Untitled 20](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/bdec1a02-2cab-42b2-8314-46a6a842e16b)
    

![Untitled 21](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/82c4750c-4963-4b00-9354-0d57e30111cf)

pixel (x,y)의 ground truth label

![Untitled 22](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/a725de57-b2a3-4111-8c7b-800d7b11744c)

salient object가 될 예측된 확률

- 하지만, BCE는 각 pixel마다의 독립적으로 loss를 계산하고, image의 global structure는 무시한다.
- 이러한 문제를 해결하기 위해, segmentation loss를 계산하는 것에 논문은 IoU loss를 사용한다.
⇒single pixel보다 전체적으로 2개의 image의 유사성을 측정한다. (식은 아래와 같다)
    
![Untitled 23](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/32754180-00b5-4434-bab3-998918edddf7)
    

⇒body loss와 detail loss에는 IoU loss를 적용하지 않는다.
