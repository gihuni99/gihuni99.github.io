---
title: EmoTalk:Speech-Driven Emotional Disentanglement for 3D Face Animation 논문 리뷰
date: 2023-12-17 00:00:00 +09:00
categories: [Paper, 3D Talking Head Generation]
use_math: true
tags:
  [
    Paper,
    3D Talking Head Generation
  ]
pin: true
---

# Abstract

![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/983c25ba-fbc3-40e6-87ad-d20a28880059)

- ‘Speech-driven 3D face animation’은 ‘speech content’와 ‘emotion’에 matching되는 사실적인 ‘facial expressions’ 만드는 것이 목적이다.
- 하지만, 기존 method들은 ‘emotional facial expressions’를 소홀히 하거나, ‘speech content’에서 ‘emotional facial expressions’를 구별하는 것에 실패하였다.
- 이러한 문제점을 고치기 위해, 해당 논문은 speech에서 서로 다른 emotion들을 구별해낼 수 있는 ‘end-to-end neural network’를 제안한다.
⇒ 풍부한 3D facial expressions를 생성할 수 있다.
- 구체적으로, 논문은 **‘Emotion Disentangling Encoder(EDE)’**를 소개한다.
⇒ 서로 다른 **‘emotion label’들이 존재하는 ‘cross-reconstructed speech signals’**를 통해 emotion과 content를 구별한다.
- 그 후, emotion이 풍부해진 **‘3D talking face’를 생성**하기 위해 **‘emotion-guided feature fusion decoder’**를 사용한다. 
⇒ decoder는 **통제 가능한 ‘personal, emotional styles’**를 생성하기 위해, 구별된 **‘identity’, ‘emotional’, ‘content embeddings’**를 입력으로 받는다.
- 최종적으로, ‘3D emotional talking face data’의 부족을 해결하기 위해,
    - ‘2D emotional data’로 그럴듯한 3D face reconstruction이 가능한 **‘facial blendshapes’를 통한 ‘supervision’**에 의존하고
    - **‘large-scale 3D emotional talking face dataset(3D-ETF)’**를 통해 network를 학습시킨다.

# 1. Introduction

![Untitled 1](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/90b8008f-88c0-47d9-b182-d3bf44b2a3f2)

![Untitled 2](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/195fbeeb-fe7e-450d-be6e-4ee35f163a20)

- ‘Dynamic, realistic speech-driven facial animation’은 ‘virtual reality’, ‘computer gaming’, ‘film production’에 대한 관심 증가로 생겨났다.
- 최근 상업적인 결과는, animator에 의해 수동으로 조작되는 ‘3D face blendshape’이 있다.
⇒이러한 과정은 상당한 시간과 노동력이 필요로 한다.
- deep learning technique이 다양한 상황에서 사용되기 때문에, ‘deep end-to-end speech-driven facial animation’은 널리 연구되고 있다.
- 머지 않아, ‘leaning-based 3D facial animations’는 ‘high-quality animation effects’를 만들 뿐만 아니라, ‘production의 비용 절감’ 또한 가능해질 것이다.

![Untitled 3](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/756a808e-27dc-454f-8ca3-533b331c7a61)

![Untitled 4](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/2a416cfd-349d-4fa7-b451-e77de5f65fcd)

- 하지만, 최근 method들은 주로 ‘lip movements’와 ‘speech’의 synchronization에만 주목하는 경향이 있다.⇒ ‘facial expressions’의 감정적인 다양성은 소홀히함
- 논문은 emotion이 의사소통과 표정에 중요한 요소이며, emotion이 배제된 3D facial animations는 ‘uncanny valley effect’를 유발할 수 있다고 주장한다.
⇒ ‘speech-driven 3D face animation problem’에서 ‘emotional expressions’를 표현하는 것은 중대한 사항이다.
- 사실, ‘emotional information’은 당연하게도 speech안에 존재한다.
⇒ emotion을 extract하는 것은 ‘speech understanding’에서 중요한 task이다.
- 그럼에도 불구하고, ‘audio content’와 ‘emotion’은 뒤엉켜있기 때문에, speech에서 명확한 content와 emtion을 동시에 extract하는 것은 어렵다.
- ‘rich emotional facial expressions’를 생성하기 위해, 이전의 2D facial animation method들은 수동으로 emotion들을 encoding하였고, speech의 content feature만을 학습하였다.
- ‘emotion code’를 조작함으로써, facial decoder는 적절한 ‘emotional modulation’을 얻을 수 있었다.
- 수동으로 조작하는 것은 변경 가능한 emotion을 생성할 수 있지만, speech와 모순되는 결과를 만들 수도 있다.
⇒ 예시로, angry speech를 input해도 happy expression나오는 등의 human intuition을 따르지 않는다.

---

***Figure 1***

![Untitled 5](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/0b5350e6-610a-475c-9346-5e8e0a5dc63c)

---

![Untitled 6](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/811d5449-dce2-42ee-acfa-e76d8cf97546)

- 위와 같은 문제를 해결하기 위해, 논문은 새로운 ‘speech-driven emotion-enhanced 3D facial animation method’를 제안한다.
⇒ **‘emotion disentangleing encoder’와 ‘emotion-guided feature fusion decoder’**가 주요한 기여
- **Emotion Disentangling Encoder(EDE)**
    - ‘content’와 ‘emotion’의 분리된 ‘latent space’를 extract하기 위해 2개의 구별된 ‘audio feature extractors’가 사용되었다.
    ⇒ ‘emotion’과 ‘content’를 명확하게 decoupling할 수 있다.
- **‘cross-reconstruction loss’**는 학습과정에서 speech로부터 ‘emotion’과 ‘content’를 더 잘 구별할 수 있게 해준다.
- **Emotion-guided Feature Fusion Decoder**
    - 여러개의 서로 다른 type의 feature들이 **‘preodic positional encoding’과 ‘emotion-guided multi-head attention’이 있는 ‘Transformer module’을 통해 decoding**된다.
    ⇒ 마지막 ‘human facial expressions’를 나타내기 위한 **‘52 emotion-enhanced blendshape coefficients’가 output**으로 나온다.

![Untitled 7](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/da175105-f63b-462f-ae86-52622f6873ea)

- 위에서 소개한 network를 training하기 위해, ‘3D facial expressions’와 대응되는 ‘emotional speech’가 필요하다.
- 하지만, 사용 가능한 ‘3D emotional talking face dataset’은 없다.⇒ 큰 문제이다.
- 위 문제를 해결하기 위해, “3D-ETF”라고 불리는 ‘large-scale pseudo-3D emotional talking face dataset’ 또한 해당 논문에서 소개된다.
- 위 dataset을 만들고, 더 활용 가능하게 만들기 위해, 논문은 ‘semantic meaningful’한 ‘52 FLAME head template’을 만들기 위해 몇몇의 animator들과 협업하였다.
- 그 후, ‘”pseudo” 3D blendshape labels’가 ‘large scale audio-visual dataset’의 image로부터 생성된다.
⇒ 잘 만들어진 ‘3D facial blendshape capture system’을 활용하여
- 최종적으로, 모든 ‘blendshape coefficients’가 있는 “3D-ETF” dataset과 mesh vertex들은 ‘Blend Linear Skining’을 통해 만들어진다.
⇒ 위의 ‘blendshape labels’는 ‘semantic meaningful’하기 때문에, “3D-ETF” dataset은 다양하게 사용될 수 있다.

![Untitled 8](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/3883e0bf-a793-44bb-9967-7b22a20c825e)

- 논문 기여도에 대한 요약
    - 논문은 ‘speech-driven emotion-enhanced 3D facial animation’을 위한 ‘end-to-end neural network’를 제안한다.
    ⇒ 다양한 ‘emotional expression’을 취득할 수 있고, SOTA를 달성
    - 논문은 speech에서 ‘emotion’과 ‘content’를 구별할 수 있는 ‘Emotion Disentangling Encoder’를 고안하였고, 확실한 ‘emotional information’을 인지하고 있는 facial animation을 만들었다.
    - 논문은 ‘large-scale 3D emotional talking face(3D-ETF) dataset’을 제안한다.
    ⇒ ‘blendshape coefficients’와 ‘mesh vertex’를 모두 포함
    - ‘blendshape coefficients’와 ‘FLAME’ model에 대해서 ‘parameterized transformations’를 수행한다.
    ⇒ 다양한 facial animations사이의 효과적인 conversion이 가능해짐

---

***Figure 2***

![Untitled 9](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/62ac91da-d803-4f81-88e0-e30e599ba95c)

- ‘speech input($A_{1:T}$)’, ‘emotional level($l$)’, ‘personal style($p$)’가 input으로 주어졌을 때, 논문의 모델은 2개의 ‘latent spaces’를 사용하여 speech에서 ‘emotion’과 ‘content’를 구별한다.
- 위 latent spaces를 통해 extract된 feature들은 combine되어 ‘Emotion-Guided Feature Fusion Decoder’의 input으로 들어간다.
⇒ output은 ‘emotion-enhanced blendshape coefficients’이다.
- 위 coefficients는 “FLAME” model을 animate하거나, image sequence로 render될 수 있다.

---

# 3. Method

![Untitled 10](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/1e6d0218-679b-481c-8905-2b91cb00db3c)

- 논문은 ‘3D facial animation model’을 제안한다.
⇒‘speech signals’로부터 풍부한 emotions이 있는 ‘facial expressions’를 reconstruction할 수 있다.
⇒ user가 **‘emotional level’**과 **‘personal style’**을 조절할 수 있다.
- $A_{1:T}=(a_1,...,a_T)$는 **‘speech snippets($a_t$)’의 sequence**이다.
    - $a_t \in \R^D$는 corresponding **‘visual frame($b_t$)’에 align된 $D$개의 sample**이 있다.
- $B_{1:T}=(b_1,...,b_T)$, $b_t \in \R^{52}$는 **‘$T$ length’의 ‘face blendshape coefficients’의 sequence**이다.
    - 각 frame은 52개의 값으로 표현된다.
- 어떤 임의의 ‘speech signal($A_{1:T}$)’로부터 emotional information을 분석함으로써, 논문의 method는 차별화된 ‘face coefficients($\hat{B}_{1:T}$)’를 도출할 수 있다.
- 또한, model은 **‘user-controllable emotional level($l \in \R^2$)’**을 input으로 받는다.
⇒ ‘facial animations’결과에 표현된 emotions의 강도를 user가 조절할 수 있다.
- **‘personal style($p \in \R^{24}$)’** input은 서로 다른 ‘speaking habits’에 따라 조절될 수 있다.
- 위 두 parameter $l$과 $p$는 모두 one-hot encoding이다.
- 그 후, **decoder는 ‘facial coefficients($\hat{B}_{1:T}=(\hat{b}_1,...,\hat{b}_T)$)’를 예측**한다.
⇒ **‘speech representations($A_{1:T}$)’, ‘emotional level($l$)’, ‘personal style($p$)’에 condition**된 decoder
- 공식으로 나타내면 다음과 같다.

![Untitled 11](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/237d90a0-c992-4211-ae8d-cfc0359478d2)

![Untitled 12](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f30047d6-78cf-476e-9abf-9a6c26d5a658)

- $\theta$는 ‘model parameters’를 나타낸다.
- detailed network components에 대한 설명의 편의를 위해, 다음과 같이 정의
    - $A_{ci,ej}$를 **‘audio sample’에서 ‘$i$번째 content’와 ‘$j$번째 emotion’에 관련된 ‘sample data’**
    - $B_{ci,cj}$를 **‘blendshape coefficients sample’에서  ‘$i$번째 content’와 ‘$j$번째 emotion’에 관련된 ‘sample data’**
- 위 두 representations는 논문 methode의 자세한 설명을 위해 사용될 것이다.

## 3.1. Emotion disentangling encoder

![Untitled 13](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/22118733-73fe-45bb-abdf-f92efe3e73ca)

- ‘speech’와 ‘facial expressions’의 복잡한 relationship은 ‘speech’에서 ‘emotional facial expressions’로 직접 mapping하도록 학습하는 것을 어렵게 한다.
- 이러한 문제를 해결하기 위해 논문은, ‘3D facial animation generation’을 위한 개선된 **‘Emotion Disentangling Encoder’**를 제안한다.
- ‘emotion disentanglement’를 ‘3D facial animation generation’ task에 적용한 것은 처음이다.
- 논문은 다양한 방법으로 원래의 ‘disentanglement module’을 간단하고 강화하였다.
- 먼저, 논문은 “MFCC feature extractor”를 ‘**pre-trained audio feature extractor wav2vec 2.0**’으로변경하였다.
⇒ “MFCC”는 풍부한 speech information을 capture할 수 없고, 복잡한 input process를 갖는다.
- 두번째로, 논문은 ‘disentanglement process’를 간소화하였다.
- 세번째로, module을 ‘facial animation’을 위해 필요한 **‘52개의 blendshape coefficients’를 직접적으로 출력**하는 ‘end-to-end’ 방식으로 바꾸었다.
⇒ training동안 model이 더 나은 constranints를 받을 수 있도록 함

---

***Figure 3***

![Untitled 14](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/1ba746c8-74f9-4611-bebf-2e024d3fccd6)

- **Emotion Disentangling Encoder**
    - speech의 다양한 inputs(서로 다른 contents와 emotions을 갖는)를 처리하여, ‘facial expressions’의 뚜렷한 조합을 나타내는 **‘cross-reconstructed blendshape coefficients’**를 생성한다.

---

### Reorganization and disentanglement

![Untitled 15](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b8954650-f76b-4379-a8bc-bd287197d0a8)

- Fig 3에 나와있는 것처럼, ‘Emotion Disentangling Encoder(EDE)’는 speech의 ‘long-term emotion features’에서 ‘short-term content features’를 구별할 수 있도록 설계되었다.

![Untitled 16](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/67943d20-10a1-45fe-9edc-a0ba3eeb94e7)

- 그럼에도 불구하고, **EDE는 ‘content’와 ‘emotion’의 disentanglement를 보장할 수 없다.**
- 위를 해결하기 위해서, 논문은 **다양한 emotions와 contents를 결합한 ‘pseudo-training pairs’를 input**으로 사용하고, network가 **‘corresponding ground truth samples’를 reconstruction하여 output**으로 내보내도록 한다.
- 이러한 방식은 network가 구별된 ‘content represenations’와 ‘emotion representations’를 얻을 수 있게 한다.
⇒ speech관점에서 더 잘 capture하고, model의 전체적인 성능을 높일 수 있다.

![Untitled 17](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/351cd037-47d2-46b4-aa3c-f48028692200)

![Untitled 18](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/6a1a2104-52d2-4a97-bad4-5e233eb863a1)

- speech에서 ‘content features’와 ‘emotion features’를 분리하기 위해서, 2개의 ‘pre-trained audio models’가 ‘feature extractors $E_c, E_e \in \R^{1024}$’로 사용된다.
⇒ content와 emotion으로 각각 fine-tunning된다.
- ‘pre-trained models’의 ‘temporal convolutional network(TCN) layer’는 fine-tuning동안 fix된다.
⇒많은 audio data로 pre-trained되었기 때문
- input으로 2개의 audio ‘$A_{c1,e2}$’, ‘$A_{c2,e1}$’을 사용하였다.
    - $c$는 text content
    - $e$는 audio emotion
- ‘content features $c_1,c_2$’는 각각 ‘$E_c(A_{c_1,e_2}),E_c(A_{c_2,e_1})$’를 사용하여 extract되었다.
- ‘emotion features $e_1,e_2$’는 각각 ‘$E_e(A_{c_1,e_2}),E_e(A_{c_2,e_1})$’를 사용하여 extract되었다.
- ‘content features’와 ‘emotion features’는 concat되고, reconstruction을 위한 ‘face blendshape coefficients’를 출력하는 decoder module의 input으로 들어간다.
- 서로 다른  content와 emotion의 결합을 포함하는 ‘pseudo-training pairs’는 input으로 사용되고, network는 ‘corresponding ground truth samples($\hat{B}_{c_1,e_1},\hat{B}_{c2,e2}$)’를 output으로 출력한다.
⇒ ‘real samples($B_{c1,e1}, B_{c2,e2}$)’이 되도록 학습된다.
- 이러한 방식은 ‘content features’와 ‘emotion features’이 결합되어 ‘speech’의 두가지 측면을 모두 재현할 수 있도록 요구함으로써, disentanglement를 강제한다.

## 3.2. Emotion-guided feature fusion decoder

![Untitled 19](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5afe1fa5-9b5a-4ff1-afb3-517c5f0eb66f)

- 논문은 ‘emotion-guided feature fusion decoder’를 제안한다.
⇒ audio의 ‘emotional information’을 사용하여 ‘audio’를 ‘3D facial animation coefficients’로 mapping한다.
- 이러한 방식은 더 expressive한 ‘facial animation’을 생성하는 것에 초점을 둔다.
- ‘emotion-guided feature fusion decoder’는 4개의 요소로 구성된다.
    - **‘emotion features($F_e \in \R^{256}$)’**
        - latent space로부터 extract됨
    - **‘content features($F_c \in \R^{512}$)’**
        - latent space로부터 extract됨
    - **‘personal style feature($F_p \in \R^{32}$)’**
        - ‘facial expressions’의 individual 특징을 조절
    - **‘emotion level features($F_l \in \R^{32}$)’**
        - ‘emotional expression’을 조절
- 위 feature들은 같은 dimension을 따라 concat되고, 이후에 ‘emotion-guided feature fusion decoder’에 input으로 들어간다.

![Untitled 20](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/2e391411-7882-479b-97d9-6403aef5afc6)

- 합쳐진 feature로부터 ‘3D blendshape coefficients’를 생성하기 위해, ‘Transformer decoder’와 유사한 module을 사용하였다.
- ‘input feature($F$)’는 ‘periodic positional encoding’을 통해 먼저 encoding된다.
⇒ speech동안 ‘lip movements’의 안정된 open, close times를 capture함
- 그 후, ‘biased multi-head self-attention layer’는 $f'_t$를 생성한다.
    - biased multi-head self-attention layer
        - “ALiBi”의 영감을 받아 ‘multi-head attention layers’에 ‘positional encoding’을 결합한 것
    - $f'_t$
        - ‘mask layer’에 더 가까운 information에 높은 weight를 주고, 인접된 동작 사이의 변화에 초점을 둔다.
- 그 후에, ‘emotion-guided multi-head attention’는 ‘$f'_t$’와 ‘emotion latent space’의 ‘output $E_e(A_{ci,ej})$’를 결합한다.
⇒ ‘3D animated faces’의 ‘emotional expressiveness’를 높인다.
- 최종적으로, $f''_t$는 ‘feed-forward layer’에 input으로 들어가고 output $f'''_t$가 출력된다.
- $f'''_t$는 ‘52개 blendshape coefficients’를 출력하는 ‘FC layer’를 사용하는 ‘audio-blendshape decoder’를 통과한다.

## 3.3. Loss function

![Untitled 21](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d1d449d1-8682-47e9-839c-d5f3c7d74fab)

- neural network를 training하기 위해서, 논문은 4개의 요소로 구성된 loss function을 사용했다.
    - **cross-reconstruction loss**
    - **self-reconstruction loss**
    - **velocity loss**
    - **classification loss**
- 식(2)와 같이 나타낼 수 있다.

### Cross-reconstruction loss

![Untitled 22](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/a2ee7171-fa1f-443a-bffc-9692dcdb7152)

- ‘speech signals’에서 ‘emotional content를 disentangling하기 위해, 다양한 ‘cross combinations’를 reconstruction하고, 새로운 ‘blendshape coefficients’를 생성하도록 training하였다.
- **‘input audio$A_{c1,e2},A_{c2,e1}$’이 주어졌을 때, ‘encoder’는 audio를 분리하고, 새로운 combinations을 reconstruction한다.**
⇒ ground truth blendshape coefficients $B_{c1,e1}, B_{c2,e2}$와 비교한다.
- 식은 위 식(3)과 같고, $D$는 ‘cross combinations’를 reconstruction하기 위한 **‘emotion-guided feature fusion decoder’**이다.

### Self-reconstruction loss

![Untitled 23](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/e6200c49-9413-47f6-b4bd-ff47bb330fef)

- ‘cross-reconstruction’을 사용하여 reconstruction된 output의 quality를 보존하면서, 그 자체의 ‘ground truth blendshape coefficients’를 reconstruction할 수 있도록 한다.
- ‘self-reconstruction loss’는 위 식(4)와 같다.

### Velocity loss

![Untitled 24](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/98e00fa7-48fe-4d55-9f77-a220a102606a)

- reconstruction loss만을 사용했을 때, ‘jittery(떨리는) output’문제를 해결하기 위해, 논문은 ‘temporal stability’를 유도하는 ‘velocity loss’를 사용한다.
⇒ prediction의 smoothness와 sequence context안의 ground truth를 고려한다
- 해당 loss를 사용함으로써, 논문의 model은 더 smooth하고 realistic한 ‘facial expressions’를 만들게 된다.
- 위 식(5)와 같다.

### Classification loss

![Untitled 25](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/20bc7298-d165-4664-b765-e9ea8b712d89)

![Untitled 26](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/16f9c04c-adac-4b0d-848d-0bb90f40019f)

- disentangling process동안 ‘emotional latent space’의 분리 가능성을 명확하게 알아내는 것의 근본적 어려움 때문에, 논문은 ‘classification loss’를 소개한다.
⇒ ‘emotion extractor $E_e$’의 output에 대해 supervision을 사용하고, 서로 다른 emotions을 구별하는 능력을 강화한다.( 식(6)과 같이 정의된다)
- $M$은 ‘emotion categroy’의 수를 나타낸다.
- $y_{ic}$는 ‘sample $i$’가 내보내는 ‘emotion label $c$’를 나타낸다.
- $p_{ic}$는 ‘sample $i$’가 ‘class $c$’에 속할 확률을 나타낸다.

## 3.4. Datasets construction

![Untitled 27](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d473dfb6-ebb3-4040-9228-b7833667afee)

- emotion이 존재하는 3D talking face data의 부족 때문에, 공공으로 사용 가능한 데이터는 없다.
- 이러한 data을 취득하기 위해, 전문적인 장비와 다양한 감정이 들어간 똑같은 문장을 말해줄 행위자가 필요하다. ⇒ 비용이 매우 크게 발생한다.
- 하지만, 많은 ‘2D emotional audio-visual datasets’는 존재한다.
- 논문은 groung tructh(supervison)로 ‘facial blendshapes’를 사용하였다.
⇒ 2D image로부터 그럴듯한 3D face를 reconstruction할 수 있다.
- 그 후, 정교한 ‘blendshape capture method’를 사용하여 2개의 dataset으로부터 ‘blendshape coefficients’를 extract한다.
⇒ 정확한 emotional expressions’를 capture
- ‘blendshape coefficients’의 700,000 frames로 구성되어 있는 large ‘3D emotional talking face(3D-ETF)’ dataset은 위 methode를 통해 만들어진다.
- ‘Blend Linear Skinning’을 통해, ‘blendshape coefficients’와 ‘mesh vertex’ 모두 3D-ETF dataset을 위해 만들어져, ‘3D facial animation datasets’의 공백(특히 ‘emotional expression data’에 관련된)을 채우고, 생생하고 실제와 같은 ‘human facial expressions’를 제공한다.

---

Figure 4

![Untitled 28](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d2cbb99d-4487-48a9-a1a8-2598ec43a06a)

- Input은 서로 다른 expression을 같은 ‘video stream’, output은 ‘blendshape coefficients’

---

![Untitled 29](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/db3369c3-6497-4588-ae27-509a5a641b1f)