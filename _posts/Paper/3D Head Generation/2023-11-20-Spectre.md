---
title: Visual Speech-Aware Perceptual 3D Facial Expression Reconstruction from Videos 논문 리뷰
date: 2023-11-20 00:00:00 +09:00
categories: [Paper, 3D Head Generation]
use_math: true
tags:
  [
    Paper,
    3D Head Generation
  ]
pin: true
---


# Abstract

![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d10af4e0-dd37-435a-8938-4cfeef6bcf85)

![Untitled 1](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f06eabde-f992-44aa-9b24-1f99bd73003e)

- Deep Learning 덕분에, 최근 image data에서의 monocular 3D face reconstruction분야의 SOTA 모델은 많은 발전을 이루고 있다.
- 하지만, 대부분 single RGB image를 input으로 사용하는 것에 초점을 두고 있다.
- 다음 중요한 요소들은 간과하는 경향이 있다.
a) 관심있는 facial image data의 대다수가 single image로부터 온 것이 아니라, video에서부터 왔다.(video는 많은 dynamic information을 포함)
b) 게다가, 이 video들은 일반적으로 언어로 소통하는 형태의 individuals를 담고있다.(ex. public talks, teleconferences(화상회의), audiovisual human-computer interactions, interviews, monologues/dialogues in movies 등)
- 현재 존재하는 3D face reconstruction method가 이러한 video에 적용된다면, 입 영역의 shape과 motion reconstruction에서의 artifacts는 심해질 것이다.
⇒ speech audio와 잘 match되지 않기 때문

![Untitled 2](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/e7f2bf7d-723f-466b-b708-4b4e02d23c23)

![Untitled 3](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/e4bd1839-ed74-4487-8e36-4275f4a71ecc)

- 앞서 언급한 한계점을 극복하기 위해서, 논문은 3D moutho expression의 visual speech-aware perceptual reconstruction을 위한 첫번째 방법을 제시한다.
- 논문은 **“lipread” loss**를 통해 이를 해결하고자 하였다.
⇒ fitting process를 guide하여, 3D reconstructed talking head로부터 이끌어낸 perception이 원래의 video 장면과 유사할 수 있게 해준다.
- 흥미롭게도, 논문은 **lipread loss가 traditional landmark loss들과 비교하여 mouth movements의 3D reconstruction에 더 알맞다는 것을 입증**하였고, 심지어 3D supervision에 비교될만 하다.
- 게다가, 논문이 고안한 방법은 text transcription 또는 corresponding audio에 의존하지 않는다.
⇒ unlabeled dataset에서 학습시켜 이상적으로 rendering하기 위함
- 논문은 3개의 large-scale dataset에서 철저하고 실증적인 평가를 통해 논문 방식의 효과성을 입증하고, 거기에 더하여 web-based studies(웹기반 사용자 연구, 온라인 설문조사, 사용자 행동 분석 등)로 주관적인 평가 또한 입증하였다.

# 1. Introduction

![Untitled 4](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5c0ca57d-3d1a-4226-9123-c4b0658abc74)

- 지난 몇년간, Deep Learning frameworks는 monocular 3D face reconstruction의 정확도를 유의미하게 상승시켰다. (어떠한 image data에서도)
- 최근 SOTA model은 3D facial geometry의 fine detail을 robust하게 reconstruction할 수 있고, 게다가 captured subject의 facial anatomy(몸,구조)의 신뢰할 수 있을 만한 estimation도 해낸다.
⇒ 이는 augmented reality, performance capture, visual effects, photo-realistic video synthesis, human computer interaction, personalized avatars 등 많은 분야에 이득이 된다.

![Untitled 5](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c15d80f5-283a-406e-95ce-01d90815cfd4)

- 반면, 대부분의 현존하는 method들은 single RGB image로부터의 3D face reconstruction에 주목한다. (사람의 얼굴에 내제되어 있는 많은 dynamic information(ex. speech를 하는 동안)을 활용하지 않는)
- 하지만, facial video를 reconstruction하는 dynamic modelling을 포함하는 몇몇의 method들도, mouth motion과 articulated(표현되는) speech 사이의 강한 correlation을 명확하게 modeling하지 않는다.
- 동시에 대부분의 흥미로운 facial video들은 verbal communication(언어적 소통)에 참여하는 individuals를 capture한다.
- 현존하는 3D face reconstruction method들이 이러한 video에 적용된다면, mouth area의 shape과 motion의 reconstruction에 존재하는 artifacts는 더 심해지고, human perception(대인지각)의 관점에서 상당한 문제가 생기고, speech와 관련 있는 입의 perceptual movements가 잘 capture되지 않는다.

![Untitled 6](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d9a10448-f4e2-41cc-8432-9bc3046d6e7f)

![Untitled 7](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/76c402dc-eb11-4d17-a76c-fe5b7f672e37)

- 틀림없이, 기존 method들의 한계의 중대한 요소는 대부분의 method들이 guidance의 형태로 face alignment method에 의해 예측된 landmark들을 통해 weak 2D supervision을 사용한다는 것이다.
- 이러한 landmark들은 facial shape의 coarse etimation을 할 수는 있지만, 크게 변화되는 mouth reion의 expression detail에 대한 정확한 representation을 제공하지 못한다.
- human mouth의 shapes가 인지적으로 speech와 연관되어있고, 3D talking head의 realism이 말해지는 문장과 완전하게 연관되어있다는 것에 주목하는 것 또한 매우 중요하다.
- 결과적으로, bi-labial consonants(양순음, 입술이 붙는 소리)를 말할 때 입술을 붙이거나 rounded-vowel(원순모음, 입술을 동그랗게 하여 내는 소리)을 말할 때 입술을 동그랗게 하지 않고 말하는 3D model은 인지적인 자연스러움이 없다.
- “EMOCA”에서는 3D reconstructed head의 표현력 향상에 사용되는 중요한 과정이 있지만, **perceptual emotional consistency loss**는 emotions와 일치하는 순간에만 영향을 미친다.
⇒ 게다가, 위 method는 jaw parameter를 잘 예측하지 못해, articualtion(표현)이 좋지 못하다.

![Untitled 8](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/44a5e3d2-aeb5-4ced-9fcb-76a0d62990af)

- 논문은 다음과 같이 결론을 내렸다.
⇒ reconstructed 3D face로부터의 speech perception(음성 인식)이 다양한 분야(augmented and virtual reality, gaming, affective avatars)들에 중요하지만, 기존 연구들에서는 보통 간과되었다.
- 기존 method 대부분에서 사용되었던 주된 evaluation metric이 model이 예측한 vertex들과 ground truth의 distance라는 것은 언급할 가치가 있다.
- 하지만 facial/mouth expression의 geometric error는 human perception 필수적으로 연관된 것은 아니다.

![Untitled 9](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/ac793465-c361-49b4-9ec7-4138636e1df4)

- 기존 연구들의 한계점을 극복하기 위해서, 논문은 video를 통한 monocular 3D face reconstruction의 문제를 해결하였다.
⇒ **mouth area와 speech articulation(음성 표현)과 관련된 mouth area의 expression과 movements에 중점을 둠**
- 논문은 video에서 human talking의 정확한 3D reconstruction에는 사람들이 speech와 일치한다고 여기는 mouth expression과 movements가 포함되어야 한다는 사실을 강조하고 언급한다.
- 논문의 method는 lip reading의 SOTA model을 활용한다.
⇒ **rendered video와 original input video 사이의 “speech perceptual” distance를 최소화하기 위함**
- 논문의 기여도는 아래와 같다.

![Untitled 10](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/320f78d7-f814-4fb8-b25b-00386a7b9b63)

- 논문은 **audio에 해당하는 text transcriptions이 필요하지 않은**, speech에 초점을 둔 human faces의 perceptual 3D reconstruction을 위한 첫번째 method를 design, implement하였다.
- 논문은 “lipread” loss를 고안하였다.
⇒ fitting process를 guid하여, reconstructed face와 특히 mouth area가, 관련 audio와 함께 결합될 때, viewer에게 유사한 perception을 이끌어내고 현실감을 느낄 수 있게 한다.
- 논문은 광범위한 객관적/주관적 evaluation을 진행하였다.
⇒reconstructed talking head의 perception이 상당히 증가했다는 것을 입증함.
- 또한 논문은 reconstructed 3D heads에서 human speech의 perception을 객관적으로 평가하는 것에 다양한 lip-read metric의 사용을 제안한다.

# 2. Related Work

#### 3D Models

![Untitled 11](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/bde7474a-ccc2-4807-9b0c-fccc79ed849e)

![Untitled 12](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b87b8bc9-c48d-4854-b695-e4e0202f08bd)

- 다양한 input source(RGB, Depth)을 통한 3D face reconstruction model을 위해 computer vision, graphic분야에서 많은 연구들이 있다.
- “3D Morphable Models”는 훨씬 가장 널리 사용되는 선택지이다.
⇒ 빈틈없는 representation을 제공할 뿐만 아니라, expression과 identity variation을 편리하게 분리한다.→ 더 잘 manipulation(조작)할 수 있다.
- 전통적인 3DMM(3D Morphable Models)는 linear하고 3D shape variation의 PCA-based model이었지만, 몇몇의 non-linear, deep learning-based 방법이 최근 몇년동안 제안되었다.

**PCA (Principal Component Analysis): 다양한 분야에서 데이터의 차원을 축소하거나 핵심적인 정보를 추출하는 데 사용되는 통계적인 기법**

- 가장 유명한 3D face model들은 “Basel Face Model”, “FaceWare-house model”, “FLAME”이 있고, 더 최근에는 “FaceScape”, “FaceVerse” model이 있다.
- 일반적으로, 이 모델들은 human face의 large 3D scan dataset을 통해 만들어진다.

#### Monocular 3D Face Reconstruction

![Untitled 13](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c1c7f942-eeca-45ac-b5c9-d587ebc0d7f9)

- 3DMMs의 일반적인 적용 방법은 RGB image에 가장 잘 맞는 model parameters의 예측을 포함한다.
- 이것은 analysis-by-synthesis framework에서 직접적인 optimization 절차를 통해 일어날 수 있다.
- 하지만, 매번 새로운 image에서 실행하기에 계산적인 비용이 큰 절차이다.
Ex) “FaceVerse” method는 detailed refinement를 위해 10분정도 소요된다.
- 위와 같은 이유로, 해당 문제를 image data를 통한 regression으로 표현하는 다양한 방법들이 제시되었다.
⇒ Deep Learning을 사용
- 신뢰할 수 있는 facial landmarker와 결합되어, 3D supervision이 없이도 정확한 결과를 도출할 수 있다.

![Untitled 14](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c66e4946-6434-4ffc-9035-17cde6b71408)

- 예를 들어, “RingNet”은 FLAME model을 사용하여 3D reconstruction을 수행했다.
⇒ identity와 expression을 구분하기 위해 shape subject의 image들 사이에서 shape consistency loss를 사용함으로써
- “DECA”는 RingNet을 기반으로 발전시켰고, 3D ground truth의 부족함을 해결하는 다양한 loss coefficients를 사용하여 CNN을 통해 FLAME model의 parameter를 jointly(jointly loss)하게 예측한다.
- “EMOCA는 reconstructed model의 expressiveness에 중점을 둔다.
⇒ emotional perceptual loss를 추가하고, 3DMM의 expression parameter를 예측하는 특정 CNN을 large emotional dataset을 통해 학습시킴으로써
- 반면, “ExpNet”은 SOTA method를 사용하여 주어진 image의 정확한 3D reconstruction을 통해 optimization 문제를 해결함으로써, pseudo-3DMM parameter들을 생성했고, landmark없이 CNN이 해당 parameter들을 예측하도록 하였다.
- 3DDFA에서는, Cascaded CNNs을 사용하여 face alignment와 3D reconstruction을 동시에 수행한다.

**3DDFA(3D Dense Face Alignment): face image의 특징점을 감지하고, 이를 통해 얼굴의 3D model을 alignment하는 기술**

**Cascaded CNN: 여러 단계들을 나누어, 각 단계에서 특정한 task를 수행하는 CNN**

- 최신 기술인 “MICA” method는 3DMM의 identity parameter를 정확하게 예측하는 것에 중점을 두었다.
⇒ large-scale 2D raw image data와 pair된 medium-scale 3D annotated dataset을 사용
- 마지막으로, “DAD3DHeads”는 3D reconstruction의 direct supervision에 사용될 수 있는 large-scale 3D head dataset을 제공한다.

![Untitled 15](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f522d102-ab59-44bd-9f64-78b206bbd9fc)

- 대부분의 methode들이 single face image를 reconstruct하거나 video의 frame-by-frame방식으로 동작한다고 하더라도, subject의 facial shape를 강제하거나 face reconstruction의 시간적 일관성을 도입하기 위해 monocular face video의 dynamic information을 이용하는 method는 거의 없다.

![Untitled 16](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/fad0495d-f53a-45bb-b6d5-038368e5dcc8)

![Untitled 17](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/086f783b-62f4-4229-bd69-78f6f62dec41)

- 논문은 “EMOCA”의 연구와 거의 유사하다
⇒perceptual reconstruction과 연관있다는 점에서
- 하지만 비교해보자면, EMOCA는 image로부터 affective(정서적인) information을 유지하는 것에 중점을 둔 반면, 논문은 speech production과 관련된 입과 입술 위치의 정확한 reconstruction에 중점을 둔다.
- 게다가, EMOCA는 입의 벌어진 정도와 rotation을 포함하는 jaw pose parameter를 정확하게 예측하는 것에 실패했다.(jaw pose가 고정됨)
⇒ convergence(융합)에 어려움을 겪음

#### Mouth/Lip Reconstruction

![Untitled 18](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/02c175f7-5354-48e3-8a7e-0e01a664dce7)

- 3D reconstruction을 위한 입과 입술의 dynamics에 중점을 둔 이전의 몇몇 연구들은 다음과 같다.
⇒ “Basu”(combined-statistical model을 사용, “Gregor”(lip motion을 따라가기 위해 markers를 사용), “Cheng”(2D images를 통한 mouth tracking을 Adaboost와 Kalman filter를 사용하여 수행)
- 가장 최근 연구는 video로부터 lip tracking을 하는 “Garrido”의 연구이다. (3D lips reconstruction에서 주목할만한 결과를 얻음)
⇒ lip tracking을 위해 high quality 3D stereo database를 사용하고, 2D images의 reconstruction 결과를 수정하기 위해 ground truth shapes와 함께 radial basis function(RBF)를 사용

---

##### **”Radial Basis Function”**

![Untitled 19](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/4f0b5a80-dc12-4d8e-b1ef-c7ef1e1cdbdc)

- RBF는 다음 2단계로 학습을 진행
1. 각 Basis function의 Center 와 Width 를 추정(input data만을 사용하여 unsupervised)
2. input data와 target data를 모두 사용하여 Weight를 구함(supervised)

---

# 3. Method

## 3.1. Preliminaries

![Untitled 20](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/3e99c9d0-ec0f-4519-9e57-0fd22256c981)

- 논문의 연구는 static RGB image를 통해 monocular 3D reconstruction을 하는 SOTA모델 “DECA”를 기반으로 한다.
⇒ 따라서 논문은 DECA 논문의 표기법을 채택
- original DECA에서, input image($I$)가 주어지면, coarse encoder(ResNet50 CNN)가 다음 parameter들을 jointly하게 예측한다.
⇒ **identity parameters($β ∈ R^{100}$), neck pose와 jaw($θ ∈ R^6$
), expression parameters($ψ ∈ R^{50}$), albedo($α ∈ R^{50}$), lighting($l ∈ R^{27}$), camera(scale and translation)($c ∈ R^3$)**
- 위 parameter들은 FLAME 3D face model의 parameters중 일부임을 주의
- 그 후, 위 parameter들은 predicted 3D face의 rendering에 사용한다.
- 또한 DECA는 **UV-displacement map과 연관된 latent vector를 예측하는 detail encoder**를 포함한다.
⇒ **high-frequency person-specific details(ex. wrinkles)를 modeling**
- 더 최근에는, “EMOCA”가 DECA를 기반으로 더 발전시켰다.
⇒ expression vector($ψ$)를 예측하기 위해 extra expression encoder(ResNet50)을 추가⇒ reconstructed face의 perceived emotion을 original image와 유사하게 만든다.
- 논문은 위 2개의 연구를 출발점으로 삼고, input video의 perceived expression을 증가시키는 architecture를 deisgn하는 것에 중점을 둔다.
⇒ mouth area에 집중하고, realistic articulation(발화) movements를 이끌어냄

![Untitled 21](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/493167d8-296b-4bc5-98cb-d294789abda9)

**<Figure 2에 대한 설명>**

- Figure 2는 perceptual 3D reconstruction을 위한 논문의 architecture를 전체적으로 보여준다.
- input video는 먼저 **3D reconstruction block**으로 들어간다.
⇒ **fixed encoder가 scene parameters(camera, lighting), identity parameters(albedo/identity), jaw parameter와 expression parameter의 initial estimation을 detect한다.**
- 그 후, **Mouth/Expression encoder가 refined facial expression parameters와 jaw pose를 예측**하고, **differentiable renderer가 prediced 3D shape을 rendering**한다.
- 마지막으로, **mouth area가 구별 가능하도록 input과 rendered image에서 모두 crop**된다. 그리고 **lip reader가 그들(input과 rendered image)사이의 perceptual lip reading loss를 측정**한다.
- **perceptual expression loss를 측정하기 위해 facial expression recognize에도 위와 같은 과정을 진행**한다.

## 3.2. Architecture

![Untitled 22](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/81ec1c11-90bd-4396-b477-ce6718114ea0)

![Untitled 23](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/1789ea5b-1dfe-4b9a-ab08-8fd4a6e3b5e0)

- architecture의 high-level overview는 Figure 2에 있다.
- **input video($V$)로부터 sampling된 K개의 연속적인 RGB frames이 주어졌을 때**, 논문의 method는 **FLAME topology에서 각 frame $I$마다 얼굴의 3D mesh를 reconstruct**한다.
⇒ **mouth movements와 general facial expressions가 perceptual하게 보존**된다.
- FLAME 3D face model 명명법에 따라, 논문은 estimated parameters들을 2개씩 구별된 sets로 분리한다.

#### Rigid & Identity parameters

![Untitled 24](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/fcc4ed76-3b25-41bc-88e5-9dc2d62d3037)

- 논문은 **input sequence의 각 image $I$에서 identity($β$), neck pose($θ_{neck}$), albedo($α ∈ R^{50}$), lighting($l ∈ R^{27}$), camera($c$)를 독립적으로 예측하기 위해 DECA의 coarse encoder를 사용**했다.
- “EMOCA”와 같이, 논문은 **해당 network(coarse encoder)를 training시에 fix**했다.

#### Expression & Jaw parameters

![Untitled 25](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/4c96b256-912a-43d3-8b16-15c64cb09e00)

- **input sequence와 연관되어 있는 expression parameters($ψ$)와 jaw pose parameters($θ_{jaw}$)는 추가적인 “perceptual” CNN encoder에 의해 예측**된다.
- 위 parameter들은 FLAME framework 아래에서 mouth expression과 movement를 control한다.
⇒ 따라서 논문의 방식에 의해 적절하게 예측되어야 한다.
- 논문은 **“lightweight MobileNet v2” architecture를 선택**하였고, **output에 “temporal convolution kernel”을 사용**한다.(**input sequence의 mouth movements와 facial expressions의 temporal dynamics를 modeling하기 위해**서)
- 논문은 system상의 연산적인 간접비를 줄이기 위해, 앞서 언급한 MobileNet의 lightweight option을 선택하였다.(이미 DECA backbone이 rescource가 필요한 ResNet50모델을 사용하기 때문)

![Untitled 26](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/29589737-6964-4e58-96c5-1dee0653ba64)

- 요약하자면, 논문은 “EMOCA”에서 제시된 것과 유사한 architecture를 계승한다.
⇒앞서 언급한 parameter들의 2개의 parallel path를 가지고 있음
- 그럼에도 불구하고, 논문의 중점은 전혀 다른 문제이고, 따라서 적절한 “directions”와 “constraints”가 제시된 Loss들의 사용을 통해 학습되어야 한다.

### 3.2.1 Training Loss

![Untitled 27](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/67afc31c-1df2-4ec2-a99e-df6ce567f145)

- **perceptual encoder를 training하기 위해**서, 논문은 **reconstruction guiding을 위한 2개의 perceptual loss function을 사용**한다.(geometric constraints와 함께)

#### Perceptual Expression Loss

![Untitled 28](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7f45a365-55d5-4e32-ba5b-4de267c7da87)

![Untitled 29](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d9b68e8a-eb6d-4028-9069-582e7843f368)

- **“perceptual encoder”의 output은 identity, albedo, camer, lighting의 predictions와 함께 사용**된다.
⇒**original input video와 관련있는 textured 3D mesh sequence를 구별 가능하도록 rendering**하기 위함
- 그 후, **input video와 reconstructed 3D mesh는 “emotion recognition network”(EMOCA에서 가져온)에 feed되고, 2개의 feature vector sequence를 얻게 된다.**
- 그 후, **2개의 feature vector sequence 사이의 distance를 minimize하는 시도를 함으로써 “perceptual expression loss($L_{em}$)을 적용**한다.
- 흥미롭게도, **“emotion recognition network”가 emotion을 예측하기 위해 학습되지만, 도움이 되는 facial characteristics를 잘 유지**한다.
- 따라서, 위와 같은 loss는 일**반적인 facial expression을 학습할 수 있게 하고, reconstruction의 현실성을 증대시킬 수 있는 emotion을 만들어낼 수 있다.**
- 특히, **perceptual expression loss($L_{em}$)은 눈 부분에 긍정적인 영향**을 미친다.
⇒ **눈을 감거나 얼굴을 찌푸리는 등의 동작의 estimation을 더 사실적으로 표현**한다.

#### Perceptual Lip Movements Loss

![Untitled 30](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/8c5dd9eb-9f88-41d4-87d0-188b5c876153)

- **“perceptual expression loss”는 mouth에 대한 detailed information을 충분히 유지시키지 못한다.**
⇒ 따라서, **추가적인 mouth-related loss가 필요**하다.
- **2D landmarks를 사용한 weak supervision의 geometric loss에만 의존하는 것이 아니라, 논문은 추가적인 perceptual loss를 사용**한다.
⇒**mout movements의 복잡함을 capture하는 “output jaw, expression coefficients”를 guide**함
- 이러한 perceptual mouth-oriented loss의 필요성은 extracted 2D landmarks를 통한 부정확성에 의해 더욱 주목받는다. (이러한 현상의 예시는 “Suppl. Material”을 참고)

![Untitled 31](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/30092ccd-e6f4-48e8-adf2-7acf3aa0947f)

![Untitled 32](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b72fc0d0-21d5-4d53-b222-46f5c2fc7237)

![Untitled 33](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/76c390a9-5403-468c-a9c9-e20cd0cb0cc8)

- 이러한 목적으로, 논문은 LRS3(Lip Reading in the Wild3) dataset으로 학습된 network를 사용한다.
- “rip-reading network”는 “Ma”에 의해 제공되는 pretrained model이다.
⇒input으로 mouth주변이 crop된 grayscale image의 sequence를 사용하고, predicted character sequence를 출력
- 위 network는 attention과 결합된 Connectionist Temporal Classification(CTC) loss를 통해 학습되었다.

**Connectionist Temporal Classification(CTC): audio와 transcription pair가 주어졌을 때, alignment를 위한 확률을 계산하는 방법(자세한 내용은 추후에 알아보기)**

- **model architecture는 3D convolutional kernel, 2D ResNet18, 12-layer conformer, transformer decoder layer의 순서로 구성**되어 있다.
⇒**predicted sequence를 출력**
- 여기에서의 논문의 목표는 **original sequence와 output image sequence사이의 speech-aware movements의 perceptual distance를 최소화**하는 것이다.
- 마지막까지, 논문은 **구별 가능하게 rendering된 image sequence를 얻고, 그 후에 landmarks prediction에 사용되는 mouth area 부분을 crop**한다.
- 마지막으로 **“lip-reading network”의 2D ResNet-18의 output으로부터 corresponding feature vector($ε_I$와 $ε_R$)를 계산**한다.
- 논문은 CNN output으로부터 나온 features가 mouth의 spatial structure를 더 잘 modeling하지만, “conformer”의 output을 통한 feature들은 sequence context에 의해 크게 영향을 받고, 반드시 필요한 spatial structure는 보존하지 못한다는 것을 실증적으로 밝혀냈다.
- feature vector를 산출한 뒤, 논문은 **input image sequence와 output rendered sequence($L_{lr} =(1/K)Σ^kd(ε_I,ε_R)$)사이의 “perceptual lip reading loss”를 최소화**한다.
(cosine distance($d$), input sequence length($K$))
- 추가적으로, 초기의 실험들은 문장의 original transcription을 고려하여, 기존 lip reading network의 예측된 output에 대한 CTC loss를 기반으로 두는 명시적인 lip reading loss를 포함한다.
- 이것의 간단한 직관에도 불구하고, 위와 같은 접근법은 video transcription이 필요하다는 점 외에도 주된 단점들이 있다.
- 먼저, 전체 문장이 한번에 처리되어야 하기 때문에, 상당한 연산적 간접비가 발생한다.
- 반면에, 제안된 접근법은 단순히 consecutive(연속적인) frame의 부분집합을 sampling하고, extract된 mouth-related features를 minimize하는 것이다.
- 게다가, 해당 approach는 “conformer”의 output을 통해 산출된 feature와 동일한 동작을 하여 실제로 효과적이지 않다는 것이 밝혀졌다.

#### Geometric Constraints

![Untitled 34](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/ebf4b7c2-6ec5-403c-9c9d-831d57c4171b)

![Untitled 35](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/6fc4e927-bca2-48e9-8925-d2c53b502ec8)

- **rendered images와 original images의 domain mismatch로 인해, “perceptual losses”가 perception의 high level information을 유지하는 것에 도움이 된다고 할지라도, 몇몇 상황에서 artifacts를 유발**하는 경향이 있다.
- 이것은 예상된 결과이다; **perceptual losses가, input manifold(데이터의 집합)가 realistic images에 일치한다는 것을 보장하지 않는 pretrained task-specific CNNs에 의존**한다.
- 예를 들어, **lip reading 성능이 좋은 왜곡된 facial reconstruction의 unrealistic images를 만들 수도 있다**.(adversarial model에서 전형적으로 나타나는 문제)
- 따라서 논문은, **training process가 geometric constraints를 따를 수 있도록 강제**함으로써 guide한다.
⇒ **initial predicted DECA parameters에 관한 L2 normalization($||ψ − ψ^{DECA}||^2$, $||θ_{jaw} − θ^{DECA}_{jaw}||^2$)을 규제하여 expression, jaw parameters를 regularization**한다.
- 앞서 언급된 regularization terms는 **DECA의 estimation을 “good” starting point로 사용**한다.
⇒ 이는 **논문의 method가, artifact가 없는 결과를 만들어내는 것이 입증된 DECA parameters로부터 크게 벗어나면 안된다는 것을 의미**
- 다시 말해서, 위와 같은 regularization 전략을 사용하여, 논문은 **DECA와 DECA의 training과정에서 hardcoding(직접 설정)된 일부 제약조건들을 간접적으로 사용**하는 것이다.
- 또한 논문은 **“3D model의 nose,face outline, eyes의 landmarks”와 “face alighnment method를 통한 predicted landmarks” 사이의 $L_1$ loss도 적용**하였다.
- **“mouth area”를 위해서는, “mouth landmarks의 intra-distances”사이의 더 완화된 $L_2$ relative loss를 사용**하였다.
- 앞서 언급된 **landmark losses는 “reconstructed face의 predicted 2D landmarks”와 “original image의 2D landmarks”사이의 distance를 기반으로 둔 geometric loss를 명확한 적용을 위한 대안으로써 구성**한다.
- 간단한 loss는 잘못된 reconstruction을 유발한다.
⇒ “perceptual losses”와 “2D landmark loss”는 주로 서로 상충되기 때문
- 제안된 relative landmark losses를 사용하는 것은 perceptual losses를 제한하는 지나치게 엄격한 제한 없이, 필수적인 face geometric strucuture를 유지 가능하게 한다.

![Untitled 36](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/a89d653d-777c-45dc-b0a0-689f12c4d414)

- 최종적으로, training에 사용되는 전체 Loss는 다음과 같다.
⇒ $L = λ_{lr}L_{lr} + λ_{em}L_{em} + L_c$ ($L_c$는 이전에 정의된 geometric constraints를 포함)

## 3.3. Training Details

![Untitled 37](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/439f8bee-f1b4-4c51-b7d3-86e3aa97b943)

- 논문은 Lip Reading Sentences3(LRS3) dataset으로 network를 학습시켰다.(lip reading in the wild를 위한 large public dataset)
- 나머지는 parameter들의 조건