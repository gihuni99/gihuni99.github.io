---
title: TCD
date: 2023-01-02 00:00:00 +09:00
categories: [Paper, RGB-D Segmentation]
tags:
  [
    Computer Vision,
    Paper,
    RGB-D,
    Segmentation
  ]
pin: true
---
(처음 인공지능 공부를 시작할 때, notion에 정리했던 논문 내용을 업로드하는 것이다. 부족한 부분이 많다.)

[Two-Stage Cascaded Decoder for Semantic Segmentation of RGB-D Images]

- Cascaded Decoder는 계단식, 즉 단계적인 Decoder를 의미하는 것이라고 생각했다

![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/092ab258-6363-4236-a0ad-9d3e50342aed)

[구글해석]

- RGB 및 깊이 정보를 활용하면 시맨틱 분할의 성능을 높일 수 있습니다. 그러나 RGB 영상과 해당 깊이 맵의 차이로 인해 이러한 다중 모드 정보를 효과적으로 사용하고 결합해야 합니다. 대부분의 기존 방법은 동일한 융합 전략을 사용하여 다양한 수준에서 다단계 보완 정보를 탐색하며 세분화를 위해 다양한 수준에서 다양한 기능 기여도를 무시할 수 있습니다. 이 문제를 해결하기 위해 세부 폴리싱 모듈을 포함하는 2단계 캐스케이드 디코더(TCD)를 사용하여 높은 수준과 낮은 수준의 기능을 효과적으로 통합하고 낮은 수준의 세부 사항에서 노이즈를 억제하는 네트워크를 제안합니다. 또한 RGB 이미지의 안내에 따라 깊이 단서에서 정보 영역을 추출하는 깊이 필터 및 융합 모듈을 소개합니다. 제안된 TCD 네트워크는 벤치마크 NYUDv2 및 SUN RGB-D 데이터 세트에서 최첨단 RGB-D 시맨틱 분할 방법에 필적하는 성능을 달성합니다.

[주요 문장]

Most existing methods use the same fusion strategy to explore multilevel complementary information at various levels, likely ignoring different feature contributions at various levels for segmentation. To address this problem, we propose a network using a two-stage cascaded decoder (TCD), embedding a detail polishing module, to effectively integrate high- and low-level features and suppress noise from low-level details. Additionally, we introduce a depth filter and fusion module to extract informative regions from depth cues with the guidance of RGB images.

[요약]

기존의 방식들은 대부분 multilevel 정보(여기에서는 RGB와 depth information)를 결합하지만 각 Feature들의 중요도는 무시하는 경향이 있다. 이 문제를 해결하기 위해 해당 논문은 detail polishing module(polishing은 다듬기를 의미)을 포함하는  two-stage cascaded decoder(TCD)를 사용한다. 

이를 통해 high-level과 low-level feature들을 효과적으로 통합하고, low-level detail에서의 noise를 줄인다. 추가적으로 RGB image에서의 depth cues(깊이 단서)로부터 informative region을 추출하는 depth filter와 fusion model을 소개한다.

# 1. Introduction

**[논문에서 말하는 문제상황]**

Although existing methods have achieved remarkable performance, two problems remain to be addressed: 

1) RGB and depth features are fused using a common attention mechanism or its variants via a single decoder 

2) level-specific characteristics of high- and low-level features are neglected, possibly resulting in an unsatisfactory performance for complex scenes.

- RGB와 Depth feature들이 단일 Decoder를 통하는 ‘일반적인 Attention Mechanism이나 그것의 변형들’을 사용하여 융합된다.
- high-, low-level feature들의 level(수준)별 특징들이 무시되어, 복잡한 장면에서 performance가 떨어질 수 있다.

**[논문이 말하는 TCD의 기여도]**

The contributions of this study can be summarized as follows:

1. We introduce a general network using TCD for RGB-D semantic segmentation. The network consists of a TCD embedded in a top-down process to gradually refine segmentation. A DPM for the second decoder suppresses noise from low-level details.
2. To fully suppress noise in depth maps and improve the compatibility between RGB and depth features, we propose a DFFM to adaptively integrate useful RGB cues and remove redundancy in depth features under the guidance of the corresponding RGB image.
3. The proposed TCD network for semantic segmentation outperforms existing methods and achieves state-of-theart (SOTA) performance on both the NYUDv2 [21] and SUN RGB-D [22] datasets.
- RGB-D semantic segmentation을 위한 TCD를 사용하는 일반적인 network를 소개한다. 이 network는 segmentation은 점차 개선하기 위해 top-down process(하향식)로 TCD가 구성된다.
- Depth map에서의 noise를 완벽하게 없애고, RGB와 Depth Feature들의 호환성을 높이기 위해, ‘DFFM’을 제안

⇒DFFM은 유용한 RGB cues(단서들)를 적응적으로 통합하고, 해당하는 RGB image의 guidance에 따라 Depth Feature에서 불필요한 중복을 제거한다.

# Main

## **[요약]**

![Untitled 1](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/47f53cf8-6888-4147-a683-f2cf9c8ede5a)

![Untitled 2](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/dcebf805-aa50-45b0-890e-805d2695994d)

- **DFFM(Depth Filter and Fusion Module)**
각 level(총 5 level)에서의 RGB와 Depth information을 incorporate(통합하는 역할)
⇒RGB 이미지에서 유용한 정보를 추출하고 깊이 맵의 중복 단서를 제거하여 RGB 및 깊이 양식에 대한 통합된 표현을 가능하게 한다.
- **MPDM(Modified Pyramid Dilated Module)**
High-level Feature들의 receptive field를 확장
    
![Untitled 3](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/3fa67fe7-5d2c-4e5c-86a0-b437d481b31b)
    

⇒MPDM에 의해 가공된 top 3 level의 Feature들은 첫번째 Decoder(Cascaded, 계단식)에 의해 연결되고 첫 Segmentation map(Hp)를 만든다

- Hp(Initial Segmentation Map)과 DFFM으로 가공된 나머지 2 level의 Feature들이 low-level detail의 noise를 제거하기 위해 DPM에 feed된다.
⇒ DPM(Detail Polished Module)

![Untitled 4](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/0ca900cd-4bf0-44be-a44a-df32cc12bb03)

⇒ DPM연산의 결과와 Hp는 Second Decoder에 의해 다시 연결되고, 최종 Segmentation Map인 Fp를 얻게 된다.

## [본문]

- High-level Feature는 obeject들을 찾기 위한 많은 의미의 정보(rich semantic information)을 담고 있다.

![Untitled 5](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/9b399ef6-9755-4c21-8410-c69a4b3d2df4)

- Low-level Feature에는 Segmentation 향상에 기여할 수 있는 풍부한 세부정보(affluent detailed information)들을 전달한다.

![Untitled 6](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c90c1130-4232-454c-a75f-4a5b06376b74)

⇒Therefore, we explore the rich semantic information in high-level cross-modal features to refine low-level details using a TCD.

**[따라서 low-level details를 개선하기 위해 high-level cross-modal features에서 rich semantic information을 TCD를 이용하여 찾는 방식**

![Untitled 7](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/6f2bd85a-7f82-48a7-a9da-96f1996814a7)

![Untitled 8](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/abd25b47-5d42-404c-88be-16ae1ae5e439)

- 위는 High-level Feature에서 Hp(first segmentation map)을 만드는 과정의 식

![Untitled 9](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7ef9737e-bfe9-4b54-963c-044546cd2d08)

![Untitled 10](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d13fff84-77e7-4ae6-8e5b-5a0474310f5b)

(Cat은 concatation 연산)

![Untitled 11](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f3e7269f-bed4-4c47-93df-26505649f38c)

- 위는 Low-level Feature와 Hp를 사용하여 최종 Segmentation map을 만드는 과정의 식

## [Depth Filter and Fusion Module(DFFM)]

![Untitled 12](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/051b4ccd-f5a1-45e8-a6e9-b361027e3ec7)

**[RGB와 Depth Feature를 통합할 때 두 가지 주요 문제]**

- 본질적으로 Feature의 양식이 달라서 비호환적
- low-quality depth map에서의 noise와 중복성
- RGB와 Depth Feature 간의 호환성을 개선, Depth Feature에서 유익한 단서를 추출하기 위해 DFFM 도입

⇒Discard–accept–integrate mechanism [24] is applied to the fusion strategy of RGB and depth features.

⇒An activation function is shared in the accept and discard stages to preserve useful information in RGB cues and discard redundant information in depth cues separately.

[번역]

- 폐기-수락-통합 메커니즘[24]은 RGB와 깊이 특징의 융합 전략에 적용됨.
- 활성화 기능은 승인 및 폐기 단계에서 공유 되어 RGB 큐의 유용한 정보를 보존하고 깊이 큐의 중복 정보를 별도로 폐기합니다.

![Untitled 13](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/4182dba0-2410-4c41-a6ab-bd39455f4d2c)

[식에 대한 요약은 생략]

# Conclusion

![Untitled 14](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7725d9b4-aa91-4914-8b6f-ce9fbc60e80c)

**[번역]**

우리는 RGB-D 시맨틱 분할을 위해 멀티모달 및 다단계 기능을 효과적으로 활용하기 위해 TCD를 사용하는 네트워크를 제안한다. 2단계 디코더와 DPM은 높은 수준의 기능을 활용하여 낮은 수준의 기능에서 세부 사항을 세분화하고 노이즈를 제거하여 다양한 수준에서 특정 의미적 특성을 추출한다. 또한 간단하지만 효과적인 DFFM은 RGB 이미지에서 유용한 정보를 추출하고 깊이 맵의 중복 단서를 제거하여 RGB 및 깊이 양식에 대한 통합된 표현을 가능하게 한다. 실험 결과는 벤치마크 NYUDv2 및 SUN RGB-D 데이터 세트에서 제안된 TCD 네트워크의 효과와 SOTA 성능을 확인한다.
