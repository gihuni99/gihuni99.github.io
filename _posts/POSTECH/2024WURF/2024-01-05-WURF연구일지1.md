---
title: Postech 24-WURF 연구 노트(12/29~1/5)
date: 2024-01-05 00:00:00 +09:00
categories: [POSTECH,2024WURF]
use_math: true
tags:
  [
    2024WURF,
    3D Talking Head Generation
  ]
pin: true
---

### "연구를 진행하며 정리한 공부 내용과 생각들"

# To do List

(연구참여 오기전까지) 3D talking head 쪽 연구 찾아보기

⇒ google schalor에서 인용된 논문들 찾아보기(2, 3개정도 더)

Sad Talker, Shouting Talker 등 이미 있는 기술들이 있을 수 있다.

**만약 존재하는 논문이라면, 여기서 가져가야 할 차별점은 무엇인지 고민해보기**

### 1주차(1월8일-14일):

- EMOCA, **SPECTRE** 등 코드 한번씩 돌려보면서 코드 파악하기
    - 모델이 어떻게 구성되어 있고,
    - FLAME 파라미터가 어떤식으로 추출되는지 (dimension 등등)
    - 직접 파라미터 값 바꿔보기 (shape, expression, pose 등)
- 2D video dataset에서 non-verbal attribute들 확인하기
    - “talking”과 동시에 발생, 혹은 소리가 동반되어 있어야 함
        - e.g., 눈 깜빡임은 talking과 연관이 없음
        - e.g., laughing, weeping은 talking 혹은 소리와 관계 있음
        - e.g, weep, shout, whisper, sigh, sing, cry (from CELEBV-Text)
    - 위의 조건에 맞는 2D 비디오 데이터 임의로 취득
    - EMOCA, SPECTRE를 활용하여 위에서 취득한 데이터를 3D로 reconstruction 해보기
    - 꼭 non-verbal이 아니어도 되어서, 관련해서 생각해보기
- 위에서 선택한 비디오들을 걸러낼수 있는 pipeline 생각해보기

# 12/29 - 1/1: GPU 가상환경 설정(Ubuntu)

[[Setup] 딥러닝 개발 환경 구축 한방에 끝내기](https://theorydb.github.io/dev/2020/02/14/dev-dl-setting-local-python/)

[윈도우10에서 리눅스(Linux) 설치하기 (Ubuntu on WSL2)](https://ingu627.github.io/tips/install_ubuntu/)

구글링을 해가면서 환경을 구축했다. 이전에 이미 구축된 연구실 서버를 사용하는 것은 해보았지만, 직접 내 데스크탑을 사용하여 딥러닝 환경을 구축하는 것은 처음이었기 때문에 많이 헤맸다. 결국 3-4일의 시간동안 온갖 방법들을 쓰면서 해결했다.

처음에는 윈도우 환경에서 구축할 수 있을 것이라고 생각하여, GPU driver 등을 윈도우 환경에 맞게 설치하였다. 서로 호환되는 버전끼리 충돌하지 않고 동작하도록 하려고, 열심히 설치했는데, github의 코드를 사용하려면 (ex. bash) 리눅스가 필요하다는 것을 알게 되었다. 그래서 이틀을 날렸다.

다시 마음을 다잡고, 리눅스 환경에 딥러닝 환경을 구축하는 방법들을 찾아보았다. 결과적으로 Ubuntu를 사용하여, VScode에서 코드를 만지고 실행시킬 수 있도록 하였다. 위 사이트들을 많이 참고했지만, 잘 되지 않는 부분이 나올 때마다 구글링을 해서, 어떤 사이트 하나만을 참고했다고 말하기 어렵다. 다만, 이제 우분투 환경을 설치해야 할 때, 조금 더 익숙하게 할 수 있게 된 것 같다. GPU가 인식되지 않는 문제로 정말 많은 시간을 쏟았는데, 결론적으로 드라이버를 수동으로 설치하여 해결하였다. 이렇게 환경 설정을 하여, “SPECTRE” github코드가 정상적으로 동작하는 것을 확인한 것이 총 4일 소요되었다.

![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/3cc6d680-161e-4b6f-9192-cbb0cbf17630)

![Untitled 1](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/2be54690-fb63-4180-96a1-cf8edc68cc44)

# 1/2: Spectre 코드 분석(1)

모델 코드를 따라가면서 공부하고, 주석을 달며 공부했다. 모델 구성이 어떻게 되어 있는지 어느 정도 파악한 이후에, 각 parameter값들이 어떻게 출력되는지 확인해보았다.

![Untitled 2](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/65dd8050-da0f-427d-8438-bad15e7e393f)

## spectre모델 구성

### E_flame(Coarse Encoder)

![Untitled 3](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/a5da836d-d936-47c4-8988-14275cc121b1)

- 우선 E_flame은 “Spectre” 모델을 구성하는 2개의 Encoder 중에서 ‘Identity/Scene(Coarse) Encoder’를 의미한다.
- ResNet50을 사용한다.

```python
cfg.model.n_shape = 100 #shape(identity) parameter
cfg.model.n_tex = 50 #texture(albedo) parameter
cfg.model.n_exp = 50 #expression parameter
cfg.model.n_cam = 3 #camera(scale, translation) parameter
cfg.model.n_pose = 6 #neck pose, jaw parameter
cfg.model.n_light = 27 #lighting parameter
```

![Untitled 4](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d5ab0acc-b375-4a88-98f7-c3f14391e92b)

- 출력은 identity(shape), neck pose, albedo(texture), lighting, camera의 parameter들이고, 총 128-dimension을 갖는다.(총 파라미터 수가 236개인 것)
    - 위 config를 보면 pose parameter가 neck, jaw 2개로 구성되어 있는데, 논문에서 언급된 ‘coarse encoder’를 통해 나오는 parameter는 ‘neck pose’ 밖에 없다. 그런데, 그대로 236-d ouput이 나온다. 여기서 233개만을 사용하는 것인지, 아니면 다른 의미가 있는 것인지 확인해볼 필요가 있다.(그대로 출력하고, 사용하지 않는 것이 맞다)

### E_expression(Perceptual Encoder)

![Untitled 5](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5839ea9e-7ef5-4527-b0bb-f7d05a0efaa4)

- MovileNetV2(perceptual CNN encoder)를 사용한다.

![Untitled 6](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/993d86e9-9ca4-4b23-bb21-d440baab8e93)

- output은 expression과 jaw pose parameter 2가지이다.
- expression parameter는 50개, jaw pose parameter는 3개이므로, 총 53개의 parameter들이 estimation된다.

### FLAME, Differentiable Renderer

![Untitled 7](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/435f7538-d242-42ae-a2f9-ad46a57a3b5d)
- differentiable renderer는 3D모델을 2D image로 변환해주는 과정

# 1/3: Spectre모델 분석(2)

오늘은 시간이 많이 있지 않아서 조금만 분석할 수 있었다.

## Model Parameter(codedict)

![Untitled 8](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/6d27d664-c8e8-41e3-9d8d-d52fe5a2cbaa)

위 코드를 보면 “spectre”의 encoder를 통해 나온 결과가 있는데, `initial_deca_exp`와 `initial_deca_jaw`는 expression과 jaw pose만을 extract하는 encoder를 통해 나온 결과이다. 여기서 주의할 점은 ‘Coarse Encoder’에서 expression과 jaw pose를 extract하지 않는 것이 아니라는 점이다. 위 코드에 주석으로도 달아놓았지만, ‘Coarse Encoder’를 통해 미처 추출하지 못한 부분까지 추출하기 위함이기 때문에 더해준다.

- 결과적으로 모든 parameter들은 ‘codedict’ dictionary에 저장된다.

codedict변수에 저장된 값들을 모두 확인해보기 위해 print를 찍어보았다. 그 결과는 아래와 같다.

```
-----------------------------------------
shape torch.Size([39, 100])
-----------------------------------------
tex torch.Size([39, 50])
-----------------------------------------
exp torch.Size([39, 50])
-----------------------------------------
pose torch.Size([39, 6])
-----------------------------------------
cam torch.Size([39, 3])
-----------------------------------------
light torch.Size([39, 9, 3])
-----------------------------------------
images torch.Size([39, 3, 224, 224])
```

frame의 수는 padding까지 총 39개이다.

- shape(identity) parameter:  100개
- texture(albedo) parameter: 50개
- expression parameter: 50개
- pose(jaw, neck) parameter: 6개
- camera parameter: 3개
- lighting parameter: 27개

### Expression Parameter의 효과 확인

1) expression parameter를 모두 0으로 바꾸어 Inference를 진행

![Untitled 9](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f8865de0-5bce-40d7-8dee-9646f95532dc)

![Untitled 10](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/3731230a-1a9a-425a-97c5-d84656f48754)

2) expression parameter를 유지하여 Inference를 진행

![Untitled 11](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/0834505f-f190-49dd-97b2-73cb220572b8)

- 위 결과를 보면, 확실히 expression이 있어야 자연스러워 보이는 것을 볼 수 있다.

# 1/4: Spectre 모델 분석 마무리, 직접 취득한 데이터로 reconstruction, 데이터 취득 pipeline 조금 다뤄보기

### Shape Parameter의 효과 확인

![Untitled 12](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/bbf67f03-35aa-456f-b1cd-69c1ca96f65e)

1) Shape parameter가 0인 model reconstruction

![Untitled 13](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5b414ffa-63ca-4a2c-88a5-3d6664c436fa)

2) 원래의 model reconstruction

![Untitled 11](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/0834505f-f190-49dd-97b2-73cb220572b8)

## 직접 취득한 Data로 face reconstruction

CelebV-HQ 데이터 중 하나로 직접 Inference해보았다. 생각보다 결과가 좋았고, 확실히 데이터셋으로 사용할 수 있을 것이라는 생각이 들었다. 또한 몰랐던 것들을 알게 되는 계기가 되었다.

![Untitled 14](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/99ffc0d1-2049-48d9-9028-e8220a643894)

### 1) face region이 자동으로 crop된다.

![Untitled 15](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/edf390f6-000c-4732-bae0-f5999dd3d35e)

위는 원래의 video data이다. 하지만 결과를 보면 face부분만이 crop된 것을 볼 수 있었다.

![Untitled 16](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/2f5fa655-06f5-4280-bbfc-c90c438d142f)

위 코드를 볼 때, crop face를 해도 달라지는 부분이 없는데, 왜 사용되는지 의문을 품었었다. 하지만, 새로운 데이터로 직접 Inference를 하여 해당 부분의 필요성을 알 수 있었다. Sample data에서는 이미 face region이 extract된 video를 활용하여 Inference되었다. 따라서 변화가 없었던 것처럼 느껴진 것이다.

### 2) Video에 audio가 포함되지 않은 경우 새롭게 경로를 지정해주어야 한다.

![Untitled 17](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d0e8e54b-a546-40df-9321-395519060627)

기존 sample data에는 audio가 있는 video였다. 하지만 CelebV-HQ data에는 video에 audio가 포함되지 않고, 따로 존재한다. 따라서 ‘--audio’ 옵션을 주었는데, audio파일이 존재하지 않아 오류가 발생했다.

![Untitled 18](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/059daf57-d791-431e-9496-f423587fae5b)

위 오류를 통해 video의 audio를 따로 처리하는 코드가 있다는 것을 확인할 수 있었다. 따라서 해당 video에 audio를 포함시키려면, audio파일의 경로를 따로 지정해주어야 한다는 것을 알 수 있었다.

### 3) 사람의 얼굴이 detection되어 face reconstruction된다.

Inference에 사용되는 video는 1분 40초 길이의 영상으로, 2명의 말하는 사람이 있고 카메라가 전환된다. 이때, 각 사람에 대한 parameter들이 제대로 취득될지 궁금했다. 위 영상은 한 사람만 존재하도록 8초의 영상으로 잘라서 Inference하였는데, 이번에는 자르지 않고 그대로 Inference해보았다.

![Untitled 19](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c5767795-3f2a-4450-934d-a107bde7751f)

그 결과, 사람이 바뀔 때마다 parameter가 정상적으로 취득되어 reconstruction되었다.

## Data Collection Pipeline 다뤄보기

CelebV-HQ 데이터셋을 활용하여 데이터를 취득할 pipeline에 대해 익히기 위해, 간단하게 구현해보았다. CelebV-HQ데이터셋은 json파일 형태로, 각 video에 대한 emotion, action등의 정보가 있는데, 이를 바탕으로 ‘shout’ action이 있는 video만을 다운받을 수 있게 코드를 추가해보았다. 매우 간단한 코드의 변형이지만, 앞으로 데이터 취득의 전체 pipeline을 구성해야 하기 때문에, 어떤 방식으로 취득할지 느낌을 익힐 겸 짜보았다. 코드는 아래와 같다.

### ‘load_data’ function

![Untitled 20](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/56e46ea5-ca26-4824-ad85-bf6fde2a261b)

### ‘ext_action’ function

![Untitled 21](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/82f86d41-82c3-4d80-98a4-b145cd269bfe)

### main process

![Untitled 22](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/878a8793-99bc-466c-98ab-f9eaa688b474)

![Untitled 23](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/79f18d27-687e-44d9-abe1-1ebb1b1b250d)

## ‘shout’ annotation을 통해 collecting된 video로 Inference

![Untitled 24](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/be0fbfe4-182c-412b-bf8f-ffd3323489c4)

위 동영상은 ‘shout’ annotation이 되어 있는 동영상들 중 하나인데, ‘shout’ action이 있는 상황에서, 특히 화를 내며 shouting하는 상황에서는 head motion 또한 dynamic해야 조금 더 자연스러워 보인다는 것을 알 수 있었다.

# 1/5: 3D talking head 연구에 어떤 것들이 있는지 살펴보기(간단하게)

### Singing Head generation

[](https://arxiv.org/pdf/2312.04369.pdf)

### Sad Talker

[](https://arxiv.org/pdf/2211.12194.pdf)

[[논문리뷰] SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation](https://kimjy99.github.io/논문리뷰/sadtalker/)

### LaughTalk

[](https://arxiv.org/pdf/2311.00994.pdf)

[LaughTalk:Expressive 3D Talking Head Generation with Laughter 논문 리뷰](https://gihuni99.github.io/posts/LaughTalk/)

음성(audio signal)로 확실하게 알 수 있는 non-verbal signal이 무엇이 있을까 생각해보았다. 우선 CelebV-HQ의 annotation을 기준으로 생각해보았을 때, “shouting”, “whispering”, “crying”

생각보다, audio-driven 3D talking head generation 분야에서, ‘non-verbal’ signal을 expression하는 연구가 많이 없다.

### 같은 영상에서 whispering과 shouting이 어떻게 표현되는지 살펴보았다.

**1) Whispering**

![Untitled 25](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/0d147347-83a6-440f-bed5-e2d786ab27a4)

**2) Shouting**

![Untitled 26](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b6d24ffc-386c-4b5b-ae2e-b8e403044299)

위 data가 밝기가 낮아서 제대로 표현되지 않는 부분도 있는 것 같다. 다만, shouting을 할 때에는 expression이 안면 전체에 잘 표현되어야 자연스럽다고 느껴진다. 생각하지 못했던 부분은, **Whispering도 매우 dynamic한 expression이 필요**하다는 점이다. 소리를 크게 내지 못하는 대신 표정으로 감정을 표현하기 때문! 그래서 만약 non-verbal로 ‘whispering’을 선택한다면 whispering audio에서 expression을 정확한 extract하는 것이 필요할 것 같다.

**3) Crying**

![Untitled 27](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/230e203c-16d0-4fc6-9855-24f00bc27d89)

crying video도 다소 어두워서 잘 구현 안된 부분이 있는 것 같지만, 입술, 눈 떨림 등의 표현을 더 살린다면 더 자연스럽게 보일 수 있을 것 같다. 

### +) VSR model을 통해서 조금 더 성능 향상이 가능하지 않을까? 라는 생각을 했다.

- generation된 3D model을 VSR model에 통과시켜 speech content를 파악하고, 이를 Ground Truth Audio와 비교하여 Loss function을 설정한다면, lip movement뿐만 아니라, non-verbal signal의 정확도도 향상시킬 수 있지 않을까 라는 생각을 해보았다.