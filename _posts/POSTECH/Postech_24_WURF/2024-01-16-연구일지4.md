---
title: Postech 24-WURF 연구 노트(1/15~1/16)
date: 2024-01-16 00:00:00 +09:00
categories: [POSTECH,2024WURF]
use_math: true
tags:
  [
    Postech_24_WURF,
    Paper
  ]
pin: true
---

### "연구를 진행하며 정리한 공부 내용과 생각들"

# 1/15:

### 1. Neutral Talk에 대한 Training

### 2. ‘Dyadic conversation에서 발생하는’ sound가 동반된 non-verbal signals에 대해서 training(Data취득이 다소 어려울 것으로 판단)

### 또는 Rich Emotion Video에 대해 training(emtion level annotation)

- Dyadic conversation에서 expression이 어색하다고 느껴지는 이유는, 이전 대화의 분위기를 expression하지 못하는 것도 있지만, ‘dyadic conversation’의 특성 상 speech에서 드러나는 감정에 비해 조금 더 과장된 expression을 하는 경우가 많은 것 같다.

(motion단위로 emotion recognition model을 사용하여 loss 설정하는 것도 괜찮을 수 있을 것 같다.)

### 3. Dyadic conversation자체를 Condition으로 training

- 한마디씩의 대화가 주어졌을 때, 앞부분의 speech가 condition, 뒷부분의 speech가 input audio
- condition을 어떻게 부여해야 할지 감이 잘 오지는 않는 것 같다.

### 어려움을 느끼는 점

가설들이 맞는지 확인해보려면, dataset을 통해서 training을 시키고 유의미한 결과가 나오는지 확인해보아야 할 것 같은데, 가설마다 training결과를 확인하는 것은 쉽지 않은 것 같다.


### CREMA-D

[https://github.com/CheyneyComputerScience/CREMA-D](https://github.com/CheyneyComputerScience/CREMA-D)

- CREMA-D dataset은 audio와 video가 각각 emotion: ‘anger’, ‘disgust’, ‘fear’, ‘happy’, ‘neutral’, ‘sad’와 emotion level: ‘high’, ‘medium’, ‘low’로 annotation되어있다.
- ‘Dyadic conversation’에서는 공감을 위한 충분한 expression이 필요하다고 생각이 든다. 공감을 충분하게 하기 위해서는 expression의 풍부함이 중요하다. 따라서 ‘emotion level’이 ‘high’인 dataset을 학습하여 이용하면 조금 더 expressive한 talking head를 만들 수 있을 것이라는 생각을 했다.
- 결국 앞에서 ‘umm’, ‘yes’ 등 speech가 아닌 ‘non-verbal signals’에 대한 학습을 따로 진행하는 것은 필요하다고 판단된다.
- 앞의 speech를 condition에 대한 reactive speech를 학습하는 것은 따로 진행해야 할 것 같다는 생각이 든다.
    - previous speech에 대한 적절한 반응은, ‘previous speech, video’를 condition으로 부여하고, 그에 대한 적절한 speech expression을 따로 학습해야 할 것이라고 생각했다.

결론적으로,

1. **Dyadic conversation에서 자주 등장하는 ‘non-verbal signals’에 대한 facial expression, pose등을 따로 학습**(LaughTalk에서 Laugh dataset에 대해서만 따로 학습했던 것 처럼)
2. **Dyadic conversation에서 ‘condition(previous speech)’에 대한 ‘input audio(responsor’s speech)’의 facial expression, pose 등을 따로 학습**
    - 여기서 한가지 더 생각이 든 것은 ‘previous speech’가 진행되는 동안의 ‘listener’s motion’또한 ‘responsor(=listener)의 speech’에 연관이 있을 것이라고 생각이 들었다.
    - 따라서 learning2listener의 **‘listener’s motion’정보를 함께 사용한다면 더 좋은 expression을 할 수 있지 않을까?** 라는 생각을 했다.
    - learning2listener에서 ‘speaker’s motion, audio’를 condition으로 사용하였는데, 이 방법을 활용해보는 것도 좋은 결과를 낼 수 있을 것 같다는 생각을 했다.

## CodeTalker

[GitHub - Doubiiu/CodeTalker: [CVPR 2023] CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior](https://github.com/Doubiiu/CodeTalker?tab=readme-ov-file)

- learning2listening과 같이 Codebook을 사용하여서 함께 사용할 수 있을 것 같다.

### CodeTalker 가상환경 설정

![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/efd8893e-ded5-4b79-b2de-9086b2b3438c)

위와 같은 오류가 발생하면,

‘anaconda3/envs/CodeTalker/lib/python3.8/site-packages/OpenGL/platform/ctypesloader.py’에서 수정이 필요하다.

![Untitled 1](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/8ab6bfc4-1398-4d9b-affb-cecbe1c5a5a1)

- 위와 같이 “fullName =util.find_library( name )”를 “fullName = '/System/Library/Frameworks/OpenGL.framework/OpenGL'”로 수정해주어야 한다.

## 새로운 연구주제(Multilingual 3D Talking Head Generation)

### Motivation

- Recent progress in 3D talking head ⇒ mainly focus on the English data
- Would they work on other languages?
    - They might work, but are they good enough?

### Reference check!!

- **[1]** **3D talking head**
    - multilingual 관련 연구 확인 필요
    
    [https://arxiv.org/pdf/2006.11610.pdf](https://arxiv.org/pdf/2006.11610.pdf)
    
    - 기존에 모델이 다른 언어에도 generalize된다고 데모를 보여주긴 함
    - Explicit하게 multilingual을 타케팅한 연구가 있는가?
    - 기존 모델의 정확도 측정 필요 (qualitative results)
        - 한가지 언어에 대해서도 다양한 샘플을 넣어서 확인 해봐야함
            - Faceformer, Emotalk, Codetalker, …, EMOTE 등등 기존 연구에 “한국어”, “영어”, “일어” 등 다양한 언어를 입력하여 동작을 잘하는 지 확인 필요
                - ~~Emotalk: 세팅완료~~
                - ~~Faceformer: 세팅완료(pth파일 찾았음)~~
                - ~~CodeTalker: 세팅완료~~
                - ~~VOCA: 세팅완료~~
            
            FaceFormer등 audio가 포함되지 않고 Inference되는 모델들은 emotalk(구현되어 있음) 참고하여 코드 짜기
            
- **[2]** **2D talking head**
    - multilingual 관련 연구 확인 필요
        - 만약 존재한다면, 어떤식으로 문제를 define했고, 해결했는지 참고해야함
        - AV2AV 논문에서 데이터셋들 확인 필요
            - [https://arxiv.org/pdf/2312.02512.pdf](https://arxiv.org/pdf/2312.02512.pdf)

### [3] Collecting dataset

- 2D video dataset source를 찾아야 함
- 문장단위로 끊던가, 시간 단위로 끊던가
- 언어가 아주 다양하지 않아도 되고 적당히 span 할수 있으면 될것 같음 (약 3-5개?!)
    - 5개의 언어라고 가정했을 때, 대략 1000개의 dataset
    - 데이터가 있는거 위주로 해야함
    - 후보: 영어(기본으로 들어감), 한국어, 프랑스어, 스페인어, 일어, 중국어 등등

~수요일: [1] 수행 **오후 10:00**

~금요일: [2], [3] 수행 **오후 10:00**

---

# 1/15: Speech-driven 3D talking head model가상환경 설정-1

오늘은 세미나가 4시간이라서 FaceFormer에 대한 가상환경 세팅만 완료했다..

## FaceFormer

- OpenGL import문제 발생 해결책은 아래에 있다.

![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7fa039b9-cb60-4767-83fc-d77d69b6660f)

아래 방법대로 하면 해결되었다.

- `$CONDA_PREFIX/include`는 ‘해당 conda env경로’

![Untitled 1](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7b9f5aab-de42-42f4-8e85-f297ebc4dd23)

- pyglet, python버전 호환 문제

![Untitled 2](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/1080ebb5-501e-45b3-915a-a73482401fb9)

pyglet==1.4.10이 3.7에 호환된다.

![Untitled 3](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/2eff898b-d299-4035-b8c6-618a958c94bd)

- psbody module문제

![Untitled 4](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f1a4b3bc-d143-4315-a68f-1b1cdb84989c)

아래와 같이 설치를 해주면 된다.

`pip install git+https://github.com/MPI-IS/mesh.git`

- pyrender문제

![Untitled 5](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/31d39653-9e7c-4f5f-87d6-f555a11d2139)

pyrender가 pyopengl==3.1.0에 맞다고 오류가 뜨더라도, ‘`pyopengl==3.1.4`’를 설치해서 돌리면 해결된다…(이거 때문에 2시간 날렸다.)

![Untitled 6](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/0b435a08-706a-453d-b0e3-c9112a54a9bf)

![Untitled 7](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/cdbe12ef-0dd9-470d-b88c-277cc372569f)

- VOCASET의 download files위치

![Untitled 8](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/0d75fc06-9927-4307-8500-ff2352975f82)

아래 VOCA의 “Training Data”에 있다.

![Untitled 9](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c0cf3862-208e-4f1b-aad5-ec30a1790584)

# 1/16:

# (1) Speech-driven 3D talking head model가상환경 설정

## CodeTalker

- CodeTalker에서는 github를 그대로 따라해서 되었는데, 오류가 발생한 부분은 FaceFormer 가상환경을 설정할 때와 같아서 수월하게 해결했다.

## VOCA

# (2) Dataset조사 및 Inference결과 비교

- 아래 ‘AV2AV’논문에서 사용된 dataset들을 살펴보고, 언어 5개에서 각각 200개 정도의 data를 뽑을 수 있는지 살펴보아야 한다. 영어는 매우 많기 때문에, 다른 언어들(한국어, 일본어, 중국어, 불어 등)에서 데이터셋 확보가 되는지 살펴보아야겠다.

[](https://arxiv.org/pdf/2312.02512.pdf)

## Dataset

### 1) VoxCeleb

[VoxCeleb](https://mm.kaist.ac.kr/datasets/voxceleb/)

### 2) AVSpeech

[AVSpeech: Audio Visual Speech Dataset](https://looking-to-listen.github.io/avspeech/)

[https://github.com/naba89/AVSpeechDownloader](https://github.com/naba89/AVSpeechDownloader)

데이터 취득시에 muavic으로 speech2text한 후, text로 language를 추정?(language annotation이 안되어 있다.)

---

### speech-to-text model

[https://github.com/facebookresearch/muavic](https://github.com/facebookresearch/muavic)

- Loss 설정할 때 사용할 수 있지 않을까?
    - 만약 사용한다면, muavic에 학습된 language를 사용해서 평가하면 성능이 좋을 것 같다는 예상을 한다.

[https://github.com/facebookresearch/av_hubert](https://github.com/facebookresearch/av_hubert)

---

# Youtube영상으로 취득할 경우

- Data Filtering이 매우 중요하다. 그 외에도, Korean 또는 Japanese는 따로 구해야 한다.

[Some languages]

⇒ 다양한 언어가 있지만(korean, japanese), 언어별 유튜브 채널이 있는 언어들보다는 data가 적다.

Korean의 경우에는 약 1분 당 5문장, 1개의 영상 당 5분이므로 25개의 video clip이 나올 것이고, 50개 정도의 영상이 있으므로, 3-5초 정도의 video clip이 500개 정도 나올 것으로 예상된다. 낮게 잡아도 300개 정도는 나올 것 같다.

[https://www.youtube.com/@easylanguages](https://www.youtube.com/@easylanguages)

아래 유튜브 채널들 외에도 언어가 있으며, 인터뷰 형식으로 다양한 사람들이 등장한다. 

[Spanish] (Script o)

[https://www.youtube.com/@EasySpanish](https://www.youtube.com/@EasySpanish)

[French] (Script o)

[https://www.youtube.com/@EasyFrench](https://www.youtube.com/@EasyFrench)

[German] (Script o)

[https://www.youtube.com/@EasyGerman](https://www.youtube.com/@EasyGerman)

[Chinese(Mandarin)] (Script x)

[https://www.youtube.com/@EasyMandarin](https://www.youtube.com/@EasyMandarin)

[Italian] (Script o)

[https://www.youtube.com/@EasyItalian](https://www.youtube.com/@EasyItalian)

### Script정보가 sentence별로 있다.

![Untitled 10](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/3d285c89-df1d-41e2-b03d-cde785871b91)

[https://github.com/jdepoix/youtube-transcript-api](https://github.com/jdepoix/youtube-transcript-api)

---

- Data collection pipeline생각할 때 참고

[https://arxiv.org/pdf/1809.00496.pdf](https://arxiv.org/pdf/1809.00496.pdf)

# Data Pipeline

1. Video Script extraction

![Untitled 11](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/bc0f1ca3-afc3-4bd4-b572-e66959e3d722)

1. video clip extraction(per sentence)

![Untitled 12](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/6ae11620-33b8-47fc-9dba-98b420917414)

1. Data filtering

## Data Collection / Filtering이 필요한 부분(해당 기술(github)들 모두 찾아보고 실제로 적용해보기)

1. 영상에서 말하는 사람이 아닌, 외부에서 말소리가 들리는 경우
    
    https://github.com/TaoRuijie/TalkNet-ASD
    
2. 주변 noise가 심한 경우(+배경 음악 소리가 너무 큰 경우)
    
    audio separation
    
3. video clip 하나에 1-2개의 sentence가 들어가도록
    
    https://github.com/jdepoix/youtube-transcript-api
    
4. scene이 바뀔 때
    
    [https://github.com/Breakthrough/PySceneDetect?tab=readme-ov-file](https://github.com/Breakthrough/PySceneDetect?tab=readme-ov-file)
    

# Dataset에서 취득

### Chinese

[https://www.vipazoo.cn/CMLR.html](https://www.vipazoo.cn/CMLR.html)

### Spanish, Portuguese, German, and French

[http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/](http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/)

## Korean (AI hurb)

[https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=538](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=538)

### Annotation

"Sentence_info": [
{
"ID": 1,
"topic": "health/diet",
"sentence_text": "믿어도 될지 의심스럽고 불안해.",
"start_time": 3.041814,
"end_time": 5.921088435374149
},

---

### English, Portugese, Russian …(AV Speech, VoxCeleb2)

- unlabeled data

[https://looking-to-listen.github.io/avspeech/](https://looking-to-listen.github.io/avspeech/)

![Untitled 13](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/90cceb71-dc14-48c3-b0d4-a2c1f389785a)

[https://mmai.io/datasets/lip_reading/](https://mmai.io/datasets/lip_reading/)

[GitHub - Curated-Awesome-Lists/awesome-ai-talking-heads: A curated list of 'Talking Head Generation' resources. Features influential papers, groundbreaking algorithms, crucial GitHub repositories, insightful videos, and more. Ideal for AI enthusiasts, researchers, and graphics professionals](https://github.com/Curated-Awesome-Lists/awesome-ai-talking-heads?tab=readme-ov-file)

## +) 논외

speech-to-text model을 사용해서, text-condition을 같이 주면 더 정확한 발음이 가능하지 않을까? 라는 생각..