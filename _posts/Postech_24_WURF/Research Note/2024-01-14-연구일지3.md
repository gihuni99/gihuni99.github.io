---
title: Postech 24-WURF 연구 노트(1/11~1/14)
date: 2024-01-14 00:00:00 +09:00
categories: [Postech_24_WURF, Research Note]
use_math: true
tags:
  [
    Postech_24_WURF,
    Paper
  ]
pin: true
---

### "연구를 진행하며 정리한 공부 내용과 생각들"

# 1/11, 12: 연구 방향성 바뀜(1/12 교수님과의 미팅준비)

기본적으로 ‘audio-driven 3D talking head generation’인 것은 동일하다. 이 때 연구의 방향성은 이전 상황에 대한 정보가 주어졌을 때, 이후의 speech에 대한 3D talking head에 변화가 유의미한지 알아보는 것이다.

- 우선, 이전 상황의 audio 또는 text 또는 video등을 condition으로 부여한다. 그 후, 그 다음 상황에 대한 speech audio를 통해 3D talking head generation을 진행한다. 핵심은 condition의 유무에 따라 generation되는 head expression이 유의미하게 바뀌는지 여부이다.

condition을 줄 때 어떤 모듈을 사용할까?..

Emotion Recognition module에서 classifier를 뺀 feature만을 사용하면 조금 더 도움이 될 수 있을까?

![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/87eb787a-c501-4765-b157-8f16e36ef0ba)

[](https://arxiv.org/pdf/2204.08451.pdf)

위 논문은 motion과 audio가 input으로 사용되고 리액션하는 3D mesh가 output으로 나오지만, 우리가 추구하는 것은 speaker의 audio는 condition, 다른 원래의 audio input을 통해 audio를 풍부하게 표현하는 3D mesh를 reconstruction하는 것이 목표이다.

- 아래는 위 논문에서 사용되는 Code이다.

[https://github.com/LizhenWangT/FaceVerse](https://github.com/LizhenWangT/FaceVerse)

[https://github.com/RenYurui/PIRender](https://github.com/RenYurui/PIRender)

# 1/13: Learning2listening 논문 리뷰
+ Learning2listen Code Inference 및 Module 확인

[](https://arxiv.org/pdf/2204.08451.pdf)

[https://github.com/evonneng/learning2listen](https://github.com/evonneng/learning2listen)

- Listening을 자연스럽게 나타내기 위해, speaker의 speech feature를 extract하여 사용할 것으로 예상한다. 우리는 speech feature를 사용하여 listening을 하는 것이 아니라, speech를 하는 것이기 때문에 다른 방향성을 갖지만, 그래도 speech feature를 어떻게 효과적으로 이용하는지 파악할 수 있다면 연구에 도움이 될 것이라고 판단한다.
- +) SER(Speech Emotion Recognition)을 사용하여, previous speaker의 emotion을 continuous하게 파악할 수 있다면 도움이 될 수 있을 것 같다는 생각을 했다.

---

learning2listening에서 사용하는 sound source separation이다. speaker-only audio를 취득하기 위해 사용되었다.

[https://github.com/andrewowens/multisensory](https://github.com/andrewowens/multisensory)

---

## 논문을 읽으며 생각나는 아이디어 정리

1. 논문에서는 rotation을 ‘facial front’를 기준으로 한 방향만을 생각했지만, 우리는 eyes, jaw, neck의 모든 pose parameter를 학습하도록 설정하면 더욱 자연스러운 speech를 만들 수 있지 않을까?

# 1/14

# 연구 목표: 
Dyadic conversation에서 더 자연스러운 non-verbal signals를 표현할 수 있도록 하는 것

- Training이 정상적으로 될지 안될지는 아직 아무도 모른다. 현재 해볼 수 있는 것들을 먼저 하자. 지금은 dyadic conversation의 dataset을 어떻게 취득할지 고민해보려고 한다.

## 1. Dyadic conversation에서 ‘non-verbal signals’에 초점을 두지 말고, 앞의 맥락에 따른 expression을 나타내는 것이 중요할 때

### 1-1. Dyadic conversation video를 모두 취득

### 1-2. Previous speaker와 Responsor의 video, audio dataset을 취득한다.

- Dyadic coversation의 특징은, Responsor의 대답이 Previous speak가 될 수 있다는 것이다. (대화가 오고 가기 때문에 질문이 대답이 될 수 있고, 대답이 질문이 될 수 있는 것)
- 따라서 아래 ‘Audio-separation’모델을 통해 dyadic conversation에서 각각의 사람이 말하는 video, audio data를 얻을 수 있을 것이라고 예상한다.
    
    [GitHub - andrewowens/multisensory: Code for the paper: Audio-Visual Scene Analysis with Self-Supervised Multisensory Features](https://github.com/andrewowens/multisensory?tab=readme-ov-file)
    
    ![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/887f6f84-3f99-4b50-8ca9-b704634b7022)
    
- 예시로, video clip의 길이가 3-5초 이상 나오는 data들만을 사용하여, ‘dyadic conversation pair dataset’을 collection할 수 있을 것이라고 예상된다.
- video clip의 길이를 3-5초로 생각하는 이유는 ‘dyadic conversation’ video들을 보았을 때, 이전 대화 맥락의 감정이 들어가는 시간이 대략 3-5초 정도 된다고 생각이 들었기 때문(이 후 시간에는 새로운 주제로 넘어가거나 감정이 다르게 바뀌는 경우가 많은 것 같다.)
- 위 pair dataset 중 ‘previous speaker’s video’는 condition으로 사용되고, ‘responsor’s audio’는 input으로 사용되도록 설정할 것 같다.

### 1-3. 의문점(확인이 필요한 부분)

- 위 dataset의 취득 방법은, dyadic conversation에서의 ‘non-verbal’ signals를 얻는다기보다, 대화의 이전 상황을 의미하는 ‘previous speaker’s video’와 그에 대해 대답하는 ‘responsor’s video’로 데이터를 구성하는데, 이때 ‘responsor’s video’가 다른 human talk video에 비해 큰 차이가 있을지 확인이 필요하다.
(우리가 원하는 비언어적인 표현(nodding, smile 등)을 표현하는 것은 다소 어려울 수도 있을 것 같다.)(대답하는 5초동안 항상 active한 reaction이 있는 것이 아니고, 있다고 하더라도 dynamic하지 않은 경우도 많다.)
- 만약 위 dataset을 사용한다면 “LaughTalk”의 방식처럼, neutral talk에 대해서 먼저 학습을 한 후, ‘condition(previous speaker’s video)’이 있는 ‘responsor’s video’에 대해 학습하는 방식으로 하는 것이 기본적인 방식이 될 수 있을 것 같다.
(responsor’s data에도 neutral talk의 비중이 상당히 많은 것 또한 생각을 해보긴 해야 한다.)

## 2. Dyadic conversation에서 audio가 동반된 ‘non-verbal signals’에 초점을 둘 때
아니면 dyadic conversation이 아니더라도, 의성어 또는 침묵 등의 ‘audio-driven talking head’에서 제대로 표현되지 않는 부분들을 잘 나타내도록 연구하는 것도 괜찮은 방향인 것 같다.

## Ex) Reactive talking head

### 2-1. Dyadic conversation video를 모두 취득

### 2-2. Sound가 있는 ‘non-verbal signals’에 대해서만 data filtering

- Filtering과정이 다소 복잡할 것 같긴 하다. 그래도 간단하게 여러가지 생각해보자면
- Audio, Video speech recognition을 사용해서, text가 정상적으로 나오지 않는 부분(인식이 제대로 된다면 text가 나올 것이지만, 의성어 또는 침묵 등은 나오지 않을 것이라고 생각이 든다)의 video clip을 가져온다.

[https://github.com/smeetrs/deep_avsr](https://github.com/smeetrs/deep_avsr)

- 만약 head movement와 관련된 non-verbal signals를 찾고 싶다면 head-pose estimation model을 통해, movement의 dynamic이 일정 이상 있는 data들만을 취득할 수도 있을 것 같다.

[https://github.com/natanielruiz/deep-head-pose](https://github.com/natanielruiz/deep-head-pose)

사용할 수 있는 data filtering방법은 더 있겠지만, 조금 더 생각해보아야 할 것 같다.

### 2-3. 의문점

- 위처럼 데이터 취득을 한다면, ‘dyadic conversation’보다는 reaction에 특화된 3D talking head generation으로 주제가 바뀌게 되는 것 같다.(’condition(이전 상황)’이 필요하지 않다..)
- 그렇다면 위 방식대로 데이터를 취득했을 때, ‘speech’가 아닌 의성어, 침묵, 등을 더 잘 표현하는 ‘reactive 3D talking head generation’으로 바꾸어야 하는데 이는 연구 주제로 적절할 것인가를 생각해보아야 함.
- 생각보다 소리가 존재하는 non-verbal signals가 없기 때문에 데이터 취득이 어려울 수 있다.

### +) pretrained ‘Video, audio emotion recognition model’을 classifier없이 사용하면 ‘previous speaker’s data’를 condition으로 사용할 수도 있을 것이라는 생각을 했다.

reaction이 있는 3D talking generation이 왜 필요한가?

보통 의성어나 head pose에 대한 표현을 제대로 못하기 때문

reaction을 좋게 만드는 방법에는 어떤 것들이 있을까?

1. 이전 대화 내용을 condition으로 부여하여, 그 정보를 통해 이후의 speech를 더욱 풍부하게 하려는 방법
2. 의성어를 쓰거나, ‘non-verbal signals’가 많은 speech data를 학습시켜서, ‘speech’속에서 verbal signals 외에도 풍부한 non-verbal signals를 표현하기 위함. 특히, ‘dyadic conversation’에만 특화된 motion, expression이 있는데, 이를 잘 학습하여 표현할 수 있도록 하기 위함이다.

# learning2listen의 예측에서 listener의 마지막 frame parameter를 ‘talking head generation’에 사용하면 어떨까?

아래는 l2l dataset

[ViCo - Mohan's Projects](https://project.mhzhou.com/vico/)

## 결국 Responsor의 speech는 listening부터 시작하기 때문에, listening의 motion 또한 매우 중요한 정보가 될 수 있을 것 같다는 생각을 했다.

- 예측된 listener의 가장 마지막 motion이, 그 다음으로 나올 speech의 motion에 가장 연관이 높다는 생각이 든다.
- FaceFormer는 이전 frame의 parameter를 입력 받지만, 가장 처음에는 이전 frame이 없기 때문에 입력받지 않는다.
- 그렇다면 가장 처음에 웃는 얼굴의 parameter를 넣어준다면? 어떻게 될까?

![Untitled 2](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/cb6bc874-97e9-46b5-9acd-6e73fe4c3fd1)

- 직접 확인해보고 싶었는데, 현재 faceformer에 사용되는 pretrained model들에 대해 접근이 되지 않아 방법을 찾아보아야겠다.

### +)의문점

- 왜 speech-driven talking head model들은 pose(neck, eyes)에 대한 estimation을 잘 시도하지 않지..(거의 없다)