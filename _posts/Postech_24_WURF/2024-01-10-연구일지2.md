---
title: Postech 24-WURF 연구 노트(1/8~1/10)
date: 2024-01-10 00:00:00 +09:00
categories: [Postech_24_WURF]
use_math: true
tags:
  [
    Postech_24_WURF,
    Paper
  ]
pin: true
---

### "연구를 진행하며 정리한 공부 내용과 생각들"

### 1주차(1월8일-14일):

- EMOCA, **SPECTRE** 등 코드 한번씩 돌려보면서 코드 파악하기
    - 모델이 어떻게 구성되어 있고,
    - FLAME 파라미터가 어떤식으로 추출되는지 (dimension 등등)
    - 직접 파라미터 값 바꿔보기 (shape, expression, pose 등)
- 2D video dataset에서 non-verbal attribute들 확인하기
    - “talking”과 동시에 발생, 혹은 소리가 동반되어 있어야 함
        - e.g., 눈 깜빡임은 talking과 연관이 없음
        - e.g., laughing, weeping은 talking 혹은 소리와 관계 있음
        - e.g, weep, shout, whisper, sigh, sing, cry (from CELEBV-Text)
    - 위의 조건에 맞는 2D 비디오 데이터 임의로 취득
    - EMOCA, SPECTRE를 활용하여 위에서 취득한 데이터를 3D로 reconstruction 해보기
    - 꼭 non-verbal이 아니어도 되어서, 관련해서 생각해보기
- 위에서 선택한 비디오들을 걸러낼수 있는 pipeline 생각해보기

Did:

12/29-1/1: 딥러닝 환경 구축

1/2-3: spectre코드 파악(모델 코드 분석(주석), parameter값 바꾸기, 새로운 데이터로 Inference 등)

1/4: CelebV-HQ데이터에서 ‘shout’ annotation이 된 video collection

# 1/9: EMOCA 코드 분석

어제까지 EMOCA를 직접 돌려보려고, 집에 있는 데스크톱 환경으로 세팅을 하였는데, 결국 안되었다. 오늘 서버를 할당받았기 때문에, 다시 서버에 EMOCA를 실행시킬 수 있는 환경을 구축해보려고 한다. 

- 버전끼리의 충돌 문제로 가상환경을 4번 정도 삭제했다. github를 참고해보았을 때,

1. EMOCA was developed with Pytorch 1.12.1 and Pytorch3d 0.6.2 running on CUDA toolkit 11.1.1 with cuDNN 8.0.5. If for some reason installation of these failed on your machine (which can happen), feel free to install these dependencies another way. The most important thing is that version of Pytorch and Pytorch3D match. The version of CUDA is probably less important.

- pytorch==1.12.1, pytorch3d==0.6.2를 default로 놓고, 충돌이 일어나지 않도록 설치를 해보아야 겠다. CUDA문제일 것이라고 생각을 했는데, 언급된 바로는 그렇게 중요하지는 않다고 한다. 만약 위처럼 해보아도 안되면, 그 때 다시 cuda버전까지 맞춰서 환경설정을 해보아야겠다.

## 멘토분께 받은 해결책

- 가상환경 만들고 pytorch 다운로드

```
conda create --prefix conda_env python=3.8

conda activate ./conda_env

#https://pytorch.org/get-started/previous-versions/
# CUDA 11.3
#pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113

#CUDA_HOME을 설정해줘야함!!!!!
export CUDA_HOME=/usr/local/cuda-11.3

conda install pytorch==1.10.0 torchvision==0.11.0 torchaudio==0.10.0 cudatoolkit=11.3 -c pytorch -c conda-forge
```

- pytorch3d 를 위해 필요한 것들 다운로드 및 pytorch3d 다운로드

[https://github.com/facebookresearch/pytorch3d/blob/main/INSTALL.md](https://github.com/facebookresearch/pytorch3d/blob/main/INSTALL.md)

```
conda install -c fvcore -c iopath -c conda-forge fvcore iopath
conda install pytorch3d -c pytorch3d
```

- “/home/sung/genesis_lab/emoca”로 이동

```
pip install -r requirements.txt
```

- 만약 아래와 같은 에러가 뜬다면?
    - ImportError: /home/sung/krafton/spectre/conda_new/lib/python3.8/site-packages/pytorch3d/_C.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZTIN3c1021AutogradMetaInterfaceE
    - **아래 stable version으로 pytorch3d 다운로드하기**
    - (다행히 나는 emoca github에 있는 코드 그대로 다운로드되었다)

```
pip install "git+https://github.com/facebookresearch/pytorch3d.git@stable"
```

- 만약 numpy error가 뜬다면?
    - ImportError: cannot import name 'bool' from 'numpy' (/home/sung/krafton/spectre/conda_new/lib/python3.8/sitepackages/numpy/init.py)

```
pip uninstall numpy
pip install numpy==1.23.1
```

### 아래는 필요시에!!

- 만약 아래와 같은 에러가 뜬다면?
- Exception: You need either charset_normalizer or chardet installed

```
pip uninstall charset-normalizer
conda install -c conda-forge charset-normalizer
```

- pytorch lightning download

```
conda install -c conda-forge pytorch-lightning==1.7.7
```

- downgrade torchmetrics

```
torchmetrics==0.11.4
```

- 위 가이드는 cuda 11.3버전을 기준으로 하였지만, ‘/usr/local’에 깔려있는 cuda버전이 11.3은 없고, 11.6이 있어서, 11.6에 맞는 cudatoolkit을 다운로드하였다.
- ‘CUDA_HOME’에 경로를 설정해주어야 한다는 것을 처음 알았다. 유용하니 잘 기억해두자.

### +)_2D→ TWO_D

![Untitled](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/1c7fef3e-8b2e-429a-9988-9703a97667dd)

- 위와 같은 _2D AttributeError가 발생하였을 때는 아래와 같이 수정이 필요하다.

![Untitled 1](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/a56aaef5-131c-4eda-94e2-e6d33d43c170)

### +) ffmpeg에서 문제가 발생

![Untitled 2](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/90b73843-c021-4df0-b23c-97403e23a4c4)

- image에서는 정상적으로 동작하는데, video에서 동작하지 않아서 원인을 찾아보았을 때, ffmpeg.prob()에서 계속 except가 발생하였다. 따라서 ffmpeg를 지우고 ffmpeg-python으로 설치하였고, 이에 따라 호환되는 버전들로 모두 바꿔주었다..

![Untitled 3](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/2978214a-a92b-4c4c-bc22-13380b03f66f)

- 충돌 오류가 날 때마다, 올바른 버전으로 바꿔주어서 정확하게 어떤 것들을 바꾸었는지는 기억이 나지 않는다. 다만, 위와 같은 오류가 발생하면 메시지에 적힌 그대로 다시 설치해주었다.

![Untitled 4](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f34becb9-dc36-4ae9-b712-df9a59604aa8)

- 위와 같이 해결되었다.

### +) EMOCA data경로 코드

![Untitled 5](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d57150ed-7af8-470b-af7c-5e6fa123bb39)

- demo파일을 실행할 떄, test_emoca_on_images.py에는 정상적으로 들어가 있는데, 위와 같이 ‘test_emoca_on_video.py’에는 "/assets/data/EMOCA_test_example_data/videos/82-25-854x480_affwild2.mp4”와 같이 절대경로로 되어있다. "assets/data/EMOCA_test_example_data/videos/82-25-854x480_affwild2.mp4”로 수정해주어야 한다.(직접 --input video로 경로 설정을 해주면 문제 없음)
- 또한 sample data가 assets파일 바깥에 다운로드되는데, assets파일 안으로 다시 넣어주어야 한다.(마찬가지로 경로 설정을 해주면 문제는 없다)

---

## 환경 설정하는 동안 생각해본 Model Architecture

### EMOTE

[](https://dl.acm.org/doi/pdf/10.1145/3610548.3618183)

![Untitled 6](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/26b2d533-877e-459a-bbe2-60a6298cd3ee)

![Untitled 7](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/681baf23-4194-4b84-9b05-0e6be29b4930)

- 위에서 볼 수 있듯이, EMOTE는 video로부터 emotion feature를 extraction하여 사용한다. “MEAD” dataset은 emotion과 intensity가 annotation되어있기 때문에 emotion feature를 classifier에 넣어 emotion과 intensity에 대한 classification을 training하고, 실제 모델에 적용할 때는 classifier를 사용하지 않고 ‘emotion feature’만을 사용하여 loss function에 사용한다.
- 위에서 영감을 얻어 생각한 방법은 **“audio emotion recognition”을 사용하여, 그 feature를 training에 사용한다면 더 좋은 표현을 얻을 수 있지 않을까?** 라는 생각을 했다.
- “laughter”같은 경우, 거의 모든 경우 emotion이 ‘happy’라고 생각되어진다.
- 반면에, ‘shouting’ 또는 ‘whispering’같은 “action”의 경우는 emotion이 확실하지 않다. ‘happy’인 경우에도 ‘shouting’할 수 있고, ‘sad’인 경우에도 ‘shouting’할 수 있다. 따라서 audio에서 어떤 감정인지 알아내는 것이 매우 중요할 것이라는 생각을 했다.

[https://github.com/MiteshPuthran/Speech-Emotion-Analyzer](https://github.com/MiteshPuthran/Speech-Emotion-Analyzer)

[https://github.com/Data-Science-kosta/Speech-Emotion-Classification-with-PyTorch](https://github.com/Data-Science-kosta/Speech-Emotion-Classification-with-PyTorch)

![Untitled 8](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/e80d4782-f4af-4a68-beab-3b35fbc0dba1)

- pre-train model을 사용하여, classifer를 제외하고, emotion feature를 사용하여 3D reconstruction을 한다면 emotion을 더 잘 나타낼 수 있지 않을까?
- EMOTE와 유사한 Architecture를 따르고, ‘input condition’ 대신, ‘audio emotion feature’를 사용하는 것이 가능할지는 잘 모르겠다.

---

## 환경 설정하는 동안 생각해본 Dataset Filtering

[GitHub - jtkim-kaist/VAD: Voice activity detection (VAD) toolkit including DNN, bDNN, LSTM and ACAM based VAD. We also provide our directly recorded dataset.](https://github.com/jtkim-kaist/VAD?tab=readme-ov-file)

만약 ‘shouting’ 또는 ‘whispering’ 등의 non-verbal signal을 사용한다면

Voice Activity Detection을 사용하여 ‘speech’가 있는 video의 ‘audio’를 취득하고, ‘speech가 있는 audio’와 ‘speech가 없는 audio’의 데시벨을 비교하면 상대적인 데시벨(ex, ‘speech가 있는 audio’가 ‘speech가 없는 audio’보다 일정 threshold이상 높으면 ‘shouting’이 있는 data를 filtering할 수 있지 않을까?)(’whispering’은 반대)을 알 수 있고, 이를 data filtering에 사용할 수도 있겠다는 생각을 하였다.

- Ex) 배경소리가 없이 목소리만 존재하는 audio파일의 평균 데시벨을 구하고, 평균보다 특정 threshold이상 데시벨이 높은 clip을 데이터로 사용
(whispering 또는 shouting에 이용 가능할 것 같다.)

---

오늘 Emoca code를 분석하고 싶었는데, Inference를 할 수 있도록 가상환경 설정하는 것에 너무 많은 시간을 쏟아버렸다.. 내일은 EMOCA code의 흐름을 파악하고, 각 parameter에 대해서도 분석해보아야 겠다.

# 1/10: EMOCA코드 분석
+ EMOCA, EmoTalk Inference결과 비교

![Untitled 9](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7cf9bb94-4840-4c28-9ba1-b9ea19d0fe69)

![Untitled 10](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b1657f10-68b7-49b1-b4d3-1b74877ba607)

- 위에서 help를 읽어서 각 변수의 역할을 알 수 있었다.

모델이 Spectre보다는 다소 복잡해서, 코드를 따라가면서 분석해보려고 한다.

## Main()

![Untitled 11](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f588317b-bd0e-4a68-af0f-efc312c9cc67)

## reconstruct_video()

![Untitled 12](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/29a0e295-3bc4-457d-943f-ef48003a3551)

- TestFaceVideoDM클래스의 object생성

![Untitled 13](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c3498a66-13a0-4cad-bc1c-c698d2e27dc4)

- 우선 prepared_data와 setup 함수의 동작 과정을 살펴보았다.

### 1. prepare_data()

![Untitled 14](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c0668e22-d183-4447-a1bb-b8b07d48118f)

### 1.1. gather_data()

![Untitled 15](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f9757963-eeac-4b06-a9e0-7b2b8f9979f4)

### 1.1.1. gather_video_metadata()

![Untitled 16](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/c1aa7f63-bfba-4fc4-a200-2ebb992f7535)

- ffmpeg.prob를 통해 video의 세부정보들을 가져올 수 있다. ‘vid’ variable에 저장되어 있고, print를 찍어보았을 때 아래와 같은 결과가 나왔다.

![Untitled 17](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/493b7e2a-6c43-4b38-951a-0adaaf65e2de)

'streams': [{'index': 0, 'codec_name': 'h264', 'codec_long_name': 'H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10', 'profile': 'High', 'codec_type': 'video', 'codec_time_base': '1/60', 'codec_tag_string': 'avc1', 'codec_tag': '0x31637661', 'width': 854, 'height': 480, 'coded_width': 854, 'coded_height': 480, 'has_b_frames': 2, 'sample_aspect_ratio': '1280:1281', 'display_aspect_ratio': '16:9', 'pix_fmt': 'yuv420p', 'level': 31, 'chroma_location': 'left', 'refs': 1, 'is_avc': 'true', 'nal_length_size': '4', 'r_frame_rate': '30/1', 'avg_frame_rate': '30/1', 'time_base': '1/15360', 'start_pts': 0, 'start_time': '0.000000', 'duration_ts': 2765829, 'duration': '180.066992', 'bit_rate': '511308', 'bits_per_raw_sample': '8', 'nb_frames': '5402', 'disposition': {'default': 1, 'dub': 0, 'original': 0, 'comment': 0, 'lyrics': 0, 'karaoke': 0, 'forced': 0, 'hearing_impaired': 0, 'visual_impaired': 0, 'clean_effects': 0, 'attached_pic': 0, 'timed_thumbnails': 0}, 'tags': {'language': 'eng', 'handler_name': 'VideoHandler'}}, {'index': 1, 'codec_name': 'aac', 'codec_long_name': 'AAC (Advanced Audio Coding)', 'profile': 'LC', 'codec_type': 'audio', 'codec_time_base': '1/44100', 'codec_tag_string': 'mp4a', 'codec_tag': '0x6134706d', 'sample_fmt': 'fltp', 'sample_rate': '44100', 'channels': 2, 'channel_layout': 'stereo', 'bits_per_sample': 0, 'r_frame_rate': '0/0', 'avg_frame_rate': '0/0', 'time_base': '1/44100', 'start_pts': 0, 'start_time': '0.000000', 'duration_ts': 7938265, 'duration': '180.006009', 'bit_rate': '128387', 'max_bit_rate': '128387', 'nb_frames': '7754', 'disposition': {'default': 1, 'dub': 0, 'original': 0, 'comment': 0, 'lyrics': 0, 'karaoke': 0, 'forced': 0, 'hearing_impaired': 0, 'visual_impaired': 0, 'clean_effects': 0, 'attached_pic': 0, 'timed_thumbnails': 0}, 'tags': {'language': 'und', 'handler_name': 'SoundHandler'}}]

- 위에서 볼 수 있듯이 video, audio 2가지 stream이 존재한다.

![Untitled 18](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/990cced4-c036-40fb-a947-65402c988b69)

- 위 코드는 stream에 존재하는 video가 없거나, 2개 이상이 있을 때 예외처리를 하는 코드이다.

![Untitled 19](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/a557ea8f-e36d-444d-8448-1ea5d07d4ef0)

- 결과적으로 ‘self.video_metas’에 video의 여러 정보들을 저장하는 코드이다.

![Untitled 20](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/21fc83a2-bc10-4ecb-9bc9-1aaddbc89099)

- “self.video_metas”에 저장되는 정보들은 위와 같다.
[{'fps': '30/1', 'width': 854, 'height': 480, 'num_frames': 5402, 'bit_rate': '511308', 'bits_per_raw_sample': '8'}]

![Untitled 21](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/1ec819d3-d91a-453d-90cd-c40e6a6afb2e)

- audio를 처리하는 과정도 동일하고, ‘self.audio_metas’에 저장되는 정보는 다음과 같다.

![Untitled 22](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/47da9174-d7b5-4bf8-9f9d-d8119e3c7f53)

- [{'sample_rate': '44100', 'sample_fmt': 'fltp', 'num_frames': 7754}]

### 1.2 unpack_videos()-1

![Untitled 23](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/27d65def-ae52-4ce0-abef-945674a98b3f)

![Untitled 24](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/8146a687-a6f9-46c7-b4b2-5a15d310a407)

### 1.2.1 get_ path_ to_sequence _frames

![Untitled 25](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5b5fb96d-0951-4d31-8a28-c3eb82963307)

![Untitled 26](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/247c7469-2fbb-47ea-9645-86a2cc638c53)

- out_folder를 print해보았을 때, 각 frame마다 video, detection, landmarks결과를 저장할 경로를 지정해주는 역할을 한다.

### 1.2 unpack_videos()-2

![Untitled 27](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/a08557d0-0e8d-4163-8be7-a805d9700f60)

- self.frame_lists를 print했을때,

![Untitled 28](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5f734f5e-8acc-479e-92bb-5d3eb30eeb9e)

- 위와 같이 video의 frame이 png파일 형태로 video파일에 저장된다.

![Untitled 29](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/4af8f4ae-1653-4ea7-b5f9-1de3c4e1e841)

### 1.3 detect_faces()

![Untitled 30](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/2dc8b3eb-324b-4b7e-84c6-73e6d0811058)

![Untitled 31](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/2c03b3ad-8e1b-4e41-bc77-7f8b3494e905)

- detection과 landmark가 저장될 경로를 설정해준다.

![Untitled 32](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/3bb18dca-c8d7-401a-a1e8-a57ce039a511)

![Untitled 33](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/6a365c17-0d33-4dd8-a384-1e08132c024c)

![Untitled 34](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/7d84fa94-d9ce-4e1e-95ec-cce1cbd9848d)

- 위 코드를 통해, 각 frame마다 detection과 landmark를 취득한다. 취득이 완료되면 “Done detecting faces in sequence”문구가 출력된다.

### 1.4 _saveMeta()

![Untitled 35](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/146595d0-4ee2-4389-a74e-e285da51478d)

![Untitled 36](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/16f38649-bda5-423a-9e46-ae8f13934e4e)

각 정보들을 metadata.pkl형태로 저장하는데, 아직 어디에 사용되는지는 파악하지 못했다. code를 읽어나가면서 알 수 있을 것 같다.

### 2. setup()

![Untitled 37](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/3092236d-76f9-4ac5-bb45-72b27c4a8178)

![Untitled 38](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/57d09fb2-1caf-4b24-a5b7-ba4b63c432c7)

detection파일의 image들을 list로 만들어 ‘TestData’ class에 전달

- TestData는 torch.Dataset이다.

![Untitled 39](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/e2555980-709f-4354-8055-fb22934e77d8)

![Untitled 40](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5f659901-35ca-457a-9c7b-811a3c0f86b2)

- 위 코드는 __ getitem__()의 일부인데, 이전에 crop하는 과정이 있지만, 이미 crop되어있기 때문에 거치지 않는다.
- image, image_name,image_path를 dictionary형태로 전달한다.

## reconstruct_video()

![Untitled 41](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/f9209d88-001f-4ab9-bf2a-9bf6d4c1203e)

- 이제 모델을 불러오고, cuda()에 올린다.

![Untitled 42](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/89f41e75-29dd-4708-bbc0-db89f40e9114)

![Untitled 43](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/065cc8ba-c636-458c-8a89-586420664a03)

### 3. test_dataloader()

![Untitled 44](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/212731ec-6229-4f50-9387-a351111593e8)

- test_dataloader는 torch.DataLoader이다.

## reconstruct_video()

![Untitled 45](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d74a7795-5d37-4a45-b1f8-da0299c08648)

- 위 코드는 data를 통해 직접 model을 돌리는 단계이다.

### 4. test()

![Untitled 46](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/0d436983-53f6-4e19-9074-1b37c21fc7fc)

- image를 gpu로 옮기고, 1x3x224x224형태로 image가 되어 있지 않으면 처리하는 코드

![Untitled 47](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5d60bef6-46ea-4c32-91b8-42d95aeac894)

- batch_size가 4로 설정되어 있기 때문에, 4x3x224x224씩 data가 처리된다.

다음으로 model을 통해 생성되는 parameter들이 무엇이 있는지 알아보았다.

![Untitled 48](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/91fa77c6-1147-439a-949d-01998fd77c92)

![Untitled 49](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/3f2be43a-5b8f-44ab-8fba-04711fc1deeb)

- 위 parameter들은 image를 encoder에 통과시킨 후 생성된 parameter들이다. 위 parameter들은 예상한대로 나왔다.

```
====================encoded parameters=======================
shapecode torch.Size([4, 100])
texcode torch.Size([4, 50])
expcode torch.Size([4, 50])
posecode torch.Size([4, 6])
cam torch.Size([4, 3])
lightcode torch.Size([4, 9, 3])
detailcode torch.Size([4, 128])
detailemocode torch.Size([4, 0])
images torch.Size([4, 3, 224, 224])

-----------------encoded original_code parameters--------------------
shape torch.Size([4, 100])
tex torch.Size([4, 50])
exp torch.Size([4, 50])
pose torch.Size([4, 6])
cam torch.Size([4, 3])
light torch.Size([4, 9, 3])
```

![Untitled 50](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/5140724a-3d96-4947-8153-d170ce3864f8)

```
========================decoded parameters======================
+++++++++++++++++++++++vals++++++++++++++++++++++++
shapecode torch.Size([4, 100])
texcode torch.Size([4, 50])
expcode torch.Size([4, 50])
posecode torch.Size([4, 6])
cam torch.Size([4, 3])
lightcode torch.Size([4, 9, 3])
detailcode torch.Size([4, 128])
detailemocode torch.Size([4, 0])
images torch.Size([4, 3, 224, 224])
-------------------decoded original_code parameters---------------------
shape torch.Size([4, 100])
tex torch.Size([4, 50])
exp torch.Size([4, 50])
pose torch.Size([4, 6])
cam torch.Size([4, 3])
light torch.Size([4, 9, 3])
predicted_images torch.Size([4, 3, 224, 224])
predicted_detailed_image torch.Size([4, 3, 224, 224])
predicted_translated_image  None
dict_keys(['images', 'albedo_images', 'alpha_images', 'pos_mask', 'shading_images', 'grid', 'normals', 'normal_images', 'transformed_normals'])
normals torch.Size([4, 5023, 3])
mask_face_eye torch.Size([4, 1, 224, 224])
verts torch.Size([4, 5023, 3])
albedo torch.Size([4, 3, 256, 256])
landmarks2d torch.Size([4, 68, 3])
landmarks3d torch.Size([4, 68, 3])
predicted_landmarks torch.Size([4, 68, 2])
predicted_landmarks_mediapipe torch.Size([4, 105, 2])
trans_verts torch.Size([4, 5023, 3])
masks torch.Size([4, 1, 224, 224])
predicted_detailed_translated_image  None
translated_uv_texture  None
uv_texture_gt torch.Size([4, 3, 256, 256])
uv_texture torch.Size([4, 3, 256, 256])
uv_detail_normals torch.Size([4, 3, 256, 256])
uv_shading torch.Size([4, 3, 256, 256])
uv_vis_mask torch.Size([4, 1, 256, 256])
uv_mask torch.Size([4, 1, 256, 256])
uv_z torch.Size([4, 1, 256, 256])
displacement_map torch.Size([4, 1, 256, 256])
```

![Untitled 51](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/95f40a5c-3fb3-46c3-91c4-0c105ac6ba4a)

- decoder를 통과한 후의 parameter들이 위와 같은 parameter들로 나왔는데, 아직 정확하게 파악하지 못했다. 또한 vals와 visdict로 나뉘어진 이유는 조금 더 찾아보아야 할 것 같다.

```
++++++++++++++++++++visdict++++++++++++++++++++++++
inputs torch.Size([4, 3, 224, 224])
landmarks_predicted torch.Size([4, 3, 224, 224])
output_images_coarse torch.Size([4, 3, 224, 224])
geometry_coarse torch.Size([4, 3, 224, 224])
geometry_detail torch.Size([4, 3, 224, 224])
mask torch.Size([4, 3, 224, 224])
albedo torch.Size([4, 3, 256, 256])
output_images_detail torch.Size([4, 3, 224, 224])
uv_detail_normals torch.Size([4, 3, 256, 256])
uv_texture_gt torch.Size([4, 3, 256, 256])
```

## reconstruct_video()

![Untitled 52](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/b9c9cf70-95e0-429e-a2ec-903018923c9e)

- 마지막으로 rendering을 진행한다. rendering하는 자세한 코드는 살펴보지 않았다.

---

## Frown?

얼굴을 언제 찌푸릴까? 얼굴을 찌푸리면서 하는 소리는 대부분 뭘까?

- 화가 많이 날 때, 보기 싫은 것을 보았을 때,
- 소리는 대부분 의성어를 많이 쓸 것 같다(dataset에서 확인해보아야 할 것 같다)

## EMOCA vs EMOTalk(동일한 crying video, audio로 비교)

1. EMOCA

![Untitled 53](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/a81fda49-9d1a-41f7-9bf1-66fdc7e847c0)

1. EmoTalk

![Untitled 54](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/92a2352b-8e18-4c44-8142-2542bb720c7d)

- 확실히 EMOCA가 더 생동감있게 느껴지지만, shape이나 movement때문에 그렇게 느껴지는 것 같긴하다. crying dataset들을 살펴보았을 때, 크게 소리내어 우는 video는 없었다. 그렇다면 audio만으로 ‘crying’을 생동감있게 나타낼 수 있을지 의문이다.(data취득이 어렵고, 취득하더라도 다른 모델에 비해서 더 생동감 있게 나타낼 수 있을지 모르겠다.)

- shouting이나 whispering에 대한 결과도 비교해보고 싶지만, dataset을 다운받을 수 없기 때문에 우선을 나중에 비교해 보아야겠다.
- laughter만큼 확실하게 표출되고, 데이터가 많은 non-verbal signal을 생각해보아야 하는데, 정말 어려운 것 같다.

# 1/10: 연구 주제 생각

## Pose Parameter Estimation

## Shouting action

sound를 통해서 head movement를 예측하는 것은 어떨까?

expression만을 예측하는 모델들의 결과를 보면, 어색하게 느껴지는데 그 이유가 expression보다는 head movement에 있는 것 같다.

[GitHub - yiranran/Audio-driven-TalkingFace-HeadPose: Code for "Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose" (Arxiv 2020) and "Predicting Personalized Head Movement From Short Video and Speech Signal" (TMM 2022)](https://github.com/yiranran/Audio-driven-TalkingFace-HeadPose?tab=readme-ov-file)

pose parameter(6개)만을 따로 학습하는 모듈을 만들면 talking에 맞는 자연스러운 pose를 학습할 수 있을까? LaughTalk의 연장선으로, Laughter에 자연스러운 pose까지 예측할 수 있도록 한다면 좋지 않을까?(가능할지는 모르겠다..)

- 현재 존재하는 Pose estimation model들은 대부분 video 또는 image를 input으로 사용한다. head pose estimation을 할 수 있는 새로운 방법을 찾지 않는 이상 적용하기 힘들 것 같다.

[DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models](https://raineggplant.github.io/DiffPoseTalk/)

- 거의 유사한 연구가 있고, 아직 코드 공개가 되지 않았음

## Shouting

1. EmoTalk

![Untitled 55](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/8054ddf7-2b4d-412e-ba1f-fcfa01929028)

1. EMOCA

![Untitled 56](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/d9f76971-463f-408f-9e5f-4d997f952567)

생각보다 확연한 차이가 있었다. emoca가 훨씬 자연스러워 보이는 것은 shape이 들어가서 일 수도 있지만, crying과는 다르게 ‘shape’뿐만 아니라 ‘expression’또한 차이가 많이 나는 것으로 보인다.

shouting dataset을 확보할 수 있고, shouting expression을 잘 표현할 수 있는 방법을 찾는다면 좋은 주제가 될 수 있을 것 같다.

[GitHub - face-analysis/emonet: Official implementation of the paper "Estimation of continuous valence and arousal levels from faces in naturalistic conditions", Antoine Toisoul, Jean Kossaifi, Adrian Bulat, Georgios Tzimiropoulos and Maja Pantic, Nature Machine Intelligence, 2021](https://github.com/face-analysis/emonet?tab=readme-ov-file)

- shouting의 emotion이 대부분 angry이긴 하지만, 다양할 수 있다. 따라서 ‘arousal’을 기준으로 data를 filtering할 수도 있을 것 같다. (shouting은 모든 감정에서 격양된 상황이기 때문)
    
    [GitHub - audeering/w2v2-how-to: How to use our public wav2vec2 dimensional emotion model](https://github.com/audeering/w2v2-how-to/tree/main)
    

[https://github.com/aeldesoky/valence-arousal-detection-using-elm](https://github.com/aeldesoky/valence-arousal-detection-using-elm)

- 위 모델 한번 돌려보기

### +) EMOTE모델에서 Input Condition 대신 SER(sound emotion recoginition) model의 결과를 사용하면 어떨까..

![Untitled 57](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/9630aaf6-2b58-4c67-9f53-125ba73a2993)

![Untitled 58](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/2183d405-89c3-406f-a035-bab6800473c1)

‘shouting’에서 emotion intensity를 예측할 수 있다면? Intensity에 따라 다양하게 표현할 수 있지 않을까?

Shouting의 기준은 다르지만, 3D mesh로 reconstruction했을 때 shouting이 자연스럽게 보이려면 expression이 잘 나타나야 한다.(약간 과장되어 보일 정도로)

소리를 지르더라도, expression이 다양하지 않은 data를 사용한다면 제대로 학습되지 않을 수도 있다.(어차피 사용하지 못하는 데이터일 것이라는 생각) 따라서 video의 emotion intensity를 extract할 수 있는 모델을 사용하여, 일정 threshold를 넘었을 때 데이터를 취득한다면 가능할 것 같다.

⇒ 그렇다면 crying도 마찬가지로 눈에 띄게(흐느껴) 우는 data만을 사용한다면 더 자연스러운 3D talking head를 만들 수 있지 않을까?

## 앞으로의 주제와 비슷한 연구논문

[Responsive Listening Head Synthesis with 3DMM and Dual-Stream Prediction Network | Proceedings of the 1st International Workshop on Multimedia Content Generation and Evaluation: New Methods and Practice](https://dl.acm.org/doi/pdf/10.1145/3607541.3616820)

대화 도중에, 각각의 상황마다 말하는 사람의 감정이 바뀐다.

말하는 상황에서의 감정을 continuous하게 extract하여 expression에 반영할 수 있다면, dyadic conversation에서 더욱 자연스러운 expression들이 나올 수 있지 않을까?

+) head pose estimation

![Untitled 57](https://github.com/gihuni99/gihuni99.github.io/assets/90080065/9630aaf6-2b58-4c67-9f53-125ba73a2993)
